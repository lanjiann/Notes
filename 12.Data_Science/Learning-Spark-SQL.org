#+TITLE: Learning Spark SQL
#+SUBTITLE: Architect streaming analytics and machine learning solutions
#+VERSION: 2017
#+AUTHOR: Aurobindo Sarkar
#+STARTUP: entitiespretty

* Table of Contents                                      :TOC_4_org:noexport:
- [[Preface][Preface]]
  - [[What this book covers][What this book covers]]
  - [[What you need for this book][What you need for this book]]
  - [[Who this book is for][Who this book is for]]
  - [[Conventions][Conventions]]
  - [[Reader feedback][Reader feedback]]
  - [[Customer support][Customer support]]
    - [[Downloading the example code][Downloading the example code]]
    - [[Downloading the color images of this book][Downloading the color images of this book]]
    - [[Errata][Errata]]
    - [[Piracy][Piracy]]
    - [[Questions][Questions]]
- [[1. Getting Started with Spark SQL][1. Getting Started with Spark SQL]]
  - [[What is Spark SQL?][What is Spark SQL?]]
  - [[Introducing SparkSession][Introducing SparkSession]]
  - [[Understanding Spark SQL concepts][Understanding Spark SQL concepts]]
    - [[Understanding Resilient Distributed Datasets (RDDs)][Understanding Resilient Distributed Datasets (RDDs)]]
    - [[Understanding DataFrames and Datasets][Understanding DataFrames and Datasets]]
    - [[Understanding the Catalyst optimizer][Understanding the Catalyst optimizer]]
      - [[Understanding Catalyst optimizations][Understanding Catalyst optimizations]]
      - [[Understanding Catalyst transformations][Understanding Catalyst transformations]]
    - [[Introducing Project Tungsten][Introducing Project Tungsten]]
  - [[Using Spark SQL in streaming applications][Using Spark SQL in streaming applications]]
    - [[Understanding Structured Streaming internals][Understanding Structured Streaming internals]]
  - [[Summary][Summary]]
- [[2. Using Spark SQL for Processing Structured and Semistructured Data][2. Using Spark SQL for Processing Structured and Semistructured Data]]
  - [[Understanding data sources in Spark applications][Understanding data sources in Spark applications]]
    - [[Selecting Spark data sources][Selecting Spark data sources]]
  - [[Using Spark with relational databases][Using Spark with relational databases]]
  - [[Using Spark with MongoDB (NoSQL database)][Using Spark with MongoDB (NoSQL database)]]
  - [[Using Spark with JSON data][Using Spark with JSON data]]
  - [[Using Spark with Avro files][Using Spark with Avro files]]
  - [[Using Spark with Parquet files][Using Spark with Parquet files]]
  - [[Defining and using custom data sources in Spark][Defining and using custom data sources in Spark]]
  - [[Summary][Summary]]
- [[3. Using Spark SQL for Data Exploration][3. Using Spark SQL for Data Exploration]]
  - [[Introducing Exploratory Data Analysis (EDA)][Introducing Exploratory Data Analysis (EDA)]]
  - [[Using Spark SQL for basic data analysis][Using Spark SQL for basic data analysis]]
    - [[Identifying missing data][Identifying missing data]]
    - [[Computing basic statistics][Computing basic statistics]]
    - [[Identifying data outliers][Identifying data outliers]]
  - [[Visualizing data with Apache Zeppelin][Visualizing data with Apache Zeppelin]]
  - [[Sampling data with Spark SQL APIs][Sampling data with Spark SQL APIs]]
    - [[Sampling with the DataFrame/Dataset API][Sampling with the DataFrame/Dataset API]]
    - [[Sampling with the RDD API][Sampling with the RDD API]]
  - [[Using Spark SQL for creating pivot tables][Using Spark SQL for creating pivot tables]]
  - [[Summary][Summary]]
- [[4. Using Spark SQL for Data Munging][4. Using Spark SQL for Data Munging]]
  - [[Introducing data munging][Introducing data munging]]
  - [[Exploring data munging techniques][Exploring data munging techniques]]
    - [[Pre-processing of the household electric consumption Dataset][Pre-processing of the household electric consumption Dataset]]
    - [[Computing basic statistics and aggregations][Computing basic statistics and aggregations]]
    - [[Augmenting the Dataset][Augmenting the Dataset]]
    - [[Executing other miscellaneous processing steps][Executing other miscellaneous processing steps]]
    - [[Pre-processing of the weather Dataset][Pre-processing of the weather Dataset]]
    - [[Analyzing missing data][Analyzing missing data]]
    - [[Combining data using a JOIN operation][Combining data using a JOIN operation]]
  - [[Munging textual data][Munging textual data]]
    - [[Processing multiple input data files][Processing multiple input data files]]
    - [[Removing stop words][Removing stop words]]
  - [[Munging time series data][Munging time series data]]
    - [[Pre-processing of the time-series Dataset][Pre-processing of the time-series Dataset]]
    - [[Processing date fields][Processing date fields]]
    - [[Persisting and loading data][Persisting and loading data]]
    - [[Defining a date-time index][Defining a date-time index]]
    - [[Using the  TimeSeriesRDD object][Using the  TimeSeriesRDD object]]
    - [[Handling missing time-series data][Handling missing time-series data]]
    - [[Computing basic statistics][Computing basic statistics]]
  - [[Dealing with variable length records][Dealing with variable length records]]
    - [[Converting variable-length records to fixed-length records][Converting variable-length records to fixed-length records]]
    - [[Extracting data from "messy" columns][Extracting data from "messy" columns]]
  - [[Preparing data for machine learning][Preparing data for machine learning]]
    - [[Pre-processing data for machine learning][Pre-processing data for machine learning]]
    - [[Creating and running a machine learning pipeline][Creating and running a machine learning pipeline]]
  - [[Summary][Summary]]
- [[5. Using Spark SQL in Streaming Applications][5. Using Spark SQL in Streaming Applications]]
  - [[Introducing streaming data applications][Introducing streaming data applications]]
  - [[Building Spark streaming applications][Building Spark streaming applications]]
    - [[Implementing sliding window-based functionality][Implementing sliding window-based functionality]]
    - [[Joining a streaming Dataset with a static Dataset][Joining a streaming Dataset with a static Dataset]]
    - [[Using the Dataset API in Structured Streaming][Using the Dataset API in Structured Streaming]]
    - [[Using output sinks][Using output sinks]]
      - [[Using the Foreach Sink for arbitrary computations on output][Using the Foreach Sink for arbitrary computations on output]]
      - [[Using the Memory Sink to save output to a table][Using the Memory Sink to save output to a table]]
      - [[Using the File Sink to save output to a partitioned table][Using the File Sink to save output to a partitioned table]]
    - [[Monitoring streaming queries][Monitoring streaming queries]]
  - [[Using Kafka with Spark Structured Streaming][Using Kafka with Spark Structured Streaming]]
    - [[Introducing Kafka concepts][Introducing Kafka concepts]]
    - [[Introducing ZooKeeper concepts][Introducing ZooKeeper concepts]]
    - [[Introducing Kafka-Spark integration][Introducing Kafka-Spark integration]]
    - [[Introducing Kafka-Spark Structured Streaming][Introducing Kafka-Spark Structured Streaming]]
  - [[Writing a receiver for a custom data source][Writing a receiver for a custom data source]]
  - [[Summary][Summary]]
- [[6. Using Spark SQL in Machine Learning Applications][6. Using Spark SQL in Machine Learning Applications]]
  - [[Introducing machine learning applications][Introducing machine learning applications]]
    - [[Understanding Spark ML pipelines and their components][Understanding Spark ML pipelines and their components]]
    - [[Understanding the steps in a pipeline application development process][Understanding the steps in a pipeline application development process]]
  - [[Introducing feature engineering][Introducing feature engineering]]
    - [[Creating new features from raw data][Creating new features from raw data]]
    - [[Estimating the importance of a feature][Estimating the importance of a feature]]
    - [[Understanding dimensionality reduction][Understanding dimensionality reduction]]
    - [[Deriving good features][Deriving good features]]
  - [[Implementing a Spark ML classification model][Implementing a Spark ML classification model]]
    - [[Exploring the diabetes Dataset][Exploring the diabetes Dataset]]
    - [[Pre-processing the data][Pre-processing the data]]
    - [[Building the Spark ML pipeline][Building the Spark ML pipeline]]
      - [[Using StringIndexer for indexing categorical features and labels][Using StringIndexer for indexing categorical features and labels]]
      - [[Using VectorAssembler for assembling features into one column][Using VectorAssembler for assembling features into one column]]
      - [[Using a Spark ML classifier][Using a Spark ML classifier]]
      - [[Creating a Spark ML pipeline][Creating a Spark ML pipeline]]
      - [[Creating the training and test Datasets][Creating the training and test Datasets]]
      - [[Making predictions using the PipelineModel][Making predictions using the PipelineModel]]
      - [[Selecting the best model][Selecting the best model]]
    - [[Changing the ML algorithm in the pipeline][Changing the ML algorithm in the pipeline]]
  - [[Introducing Spark ML tools and utilities][Introducing Spark ML tools and utilities]]
    - [[Using Principal Component Analysis to select features][Using Principal Component Analysis to select features]]
    - [[Using encoders][Using encoders]]
    - [[Using Bucketizer][Using Bucketizer]]
    - [[Using VectorSlicer][Using VectorSlicer]]
    - [[Using Chi-squared selector][Using Chi-squared selector]]
    - [[Using a Normalizer][Using a Normalizer]]
    - [[Retrieving our original labels][Retrieving our original labels]]
  - [[Implementing a Spark ML clustering model][Implementing a Spark ML clustering model]]
  - [[Summary][Summary]]
- [[7. Using Spark SQL in Graph Applications][7. Using Spark SQL in Graph Applications]]
  - [[Introducing large-scale graph applications][Introducing large-scale graph applications]]
  - [[Exploring graphs using GraphFrames][Exploring graphs using GraphFrames]]
    - [[Constructing a GraphFrame][Constructing a GraphFrame]]
    - [[Basic graph queries and operations][Basic graph queries and operations]]
    - [[Motif analysis using GraphFrames][Motif analysis using GraphFrames]]
    - [[Processing subgraphs][Processing subgraphs]]
    - [[Applying graph algorithms][Applying graph algorithms]]
    - [[Saving and loading GraphFrames][Saving and loading GraphFrames]]
  - [[Analyzing JSON input modeled as a graph][Analyzing JSON input modeled as a graph]]
  - [[Processing graphs containing multiple types of relationships][Processing graphs containing multiple types of relationships]]
  - [[Understanding GraphFrame internals][Understanding GraphFrame internals]]
    - [[Viewing GraphFrame physical execution plan][Viewing GraphFrame physical execution plan]]
    - [[Understanding partitioning in GraphFrames][Understanding partitioning in GraphFrames]]
  - [[Summary][Summary]]
- [[8. Using Spark SQL with SparkR][8. Using Spark SQL with SparkR]]
  - [[Introducing SparkR][Introducing SparkR]]
  - [[Understanding the SparkR architecture][Understanding the SparkR architecture]]
  - [[Understanding SparkR DataFrames][Understanding SparkR DataFrames]]
  - [[Using SparkR for EDA and data munging tasks][Using SparkR for EDA and data munging tasks]]
    - [[Reading and writing Spark DataFrames][Reading and writing Spark DataFrames]]
    - [[Exploring structure and contents of Spark DataFrames][Exploring structure and contents of Spark DataFrames]]
    - [[Running basic operations on Spark DataFrames][Running basic operations on Spark DataFrames]]
    - [[Executing SQL statements on Spark DataFrames][Executing SQL statements on Spark DataFrames]]
    - [[Merging SparkR DataFrames][Merging SparkR DataFrames]]
    - [[Using User Defined Functions (UDFs)][Using User Defined Functions (UDFs)]]
  - [[Using SparkR for computing summary statistics][Using SparkR for computing summary statistics]]
  - [[Using SparkR for data visualization][Using SparkR for data visualization]]
    - [[Visualizing data on a map][Visualizing data on a map]]
    - [[Visualizing graph nodes and edges][Visualizing graph nodes and edges]]
  - [[Using SparkR for machine learning][Using SparkR for machine learning]]
  - [[Summary][Summary]]
- [[9. Developing Applications with Spark SQL][9. Developing Applications with Spark SQL]]
  - [[Introducing Spark SQL applications][Introducing Spark SQL applications]]
  - [[Understanding text analysis applications][Understanding text analysis applications]]
    - [[Using Spark SQL for textual analysis][Using Spark SQL for textual analysis]]
      - [[Preprocessing textual data][Preprocessing textual data]]
      - [[Computing readability][Computing readability]]
      - [[Using word lists][Using word lists]]
    - [[Creating data preprocessing pipelines][Creating data preprocessing pipelines]]
  - [[Understanding themes in document corpuses][Understanding themes in document corpuses]]
  - [[Using Naive Bayes classifiers][Using Naive Bayes classifiers]]
  - [[Developing a machine learning application][Developing a machine learning application]]
  - [[Summary][Summary]]
- [[10. Using Spark SQL in Deep Learning Applications][10. Using Spark SQL in Deep Learning Applications]]
  - [[Introducing neural networks][Introducing neural networks]]
    - [[Understanding deep learning][Understanding deep learning]]
    - [[Understanding representation learning][Understanding representation learning]]
    - [[Understanding stochastic gradient descent][Understanding stochastic gradient descent]]
  - [[Introducing deep learning in Spark][Introducing deep learning in Spark]]
    - [[Introducing CaffeOnSpark][Introducing CaffeOnSpark]]
    - [[Introducing DL4J][Introducing DL4J]]
    - [[Introducing TensorFrames][Introducing TensorFrames]]
    - [[Working with BigDL][Working with BigDL]]
    - [[Tuning hyperparameters of deep learning models][Tuning hyperparameters of deep learning models]]
    - [[Introducing deep learning pipelines][Introducing deep learning pipelines]]
  - [[Understanding Supervised learning][Understanding Supervised learning]]
    - [[Understanding convolutional neural networks][Understanding convolutional neural networks]]
    - [[Using neural networks for text classification][Using neural networks for text classification]]
  - [[Using deep neural networks for language processing][Using deep neural networks for language processing]]
    - [[Understanding Recurrent Neural Networks][Understanding Recurrent Neural Networks]]
  - [[Introducing autoencoders][Introducing autoencoders]]
  - [[Summary][Summary]]
- [[11. Tuning Spark SQL Components for Performance][11. Tuning Spark SQL Components for Performance]]
  - [[Introducing performance tuning in Spark SQL][Introducing performance tuning in Spark SQL]]
  - [[Understanding DataFrame/Dataset APIs][Understanding DataFrame/Dataset APIs]]
    - [[Optimizing data serialization][Optimizing data serialization]]
  - [[Understanding Catalyst optimizations][Understanding Catalyst optimizations]]
    - [[Understanding the Dataset/DataFrame API][Understanding the Dataset/DataFrame API]]
    - [[Understanding Catalyst transformations][Understanding Catalyst transformations]]
  - [[Visualizing Spark application execution][Visualizing Spark application execution]]
    - [[Exploring Spark application execution metrics][Exploring Spark application execution metrics]]
    - [[Using external tools for performance tuning][Using external tools for performance tuning]]
  - [[Cost-based optimizer in Apache Spark 2.2][Cost-based optimizer in Apache Spark 2.2]]
    - [[Understanding the CBO statistics collection][Understanding the CBO statistics collection]]
    - [[Statistics collection functions][Statistics collection functions]]
      - [[Filter operator][Filter operator]]
      - [[Join operator][Join operator]]
    - [[Build side selection][Build side selection]]
  - [[Understanding multi-way JOIN ordering optimization][Understanding multi-way JOIN ordering optimization]]
  - [[Understanding performance improvements using whole-stage code generation][Understanding performance improvements using whole-stage code generation]]
  - [[Summary][Summary]]
- [[12. Spark SQL in Large-Scale Application Architectures][12. Spark SQL in Large-Scale Application Architectures]]
  - [[Understanding Spark-based application architectures][Understanding Spark-based application architectures]]
    - [[Using Apache Spark for batch processing][Using Apache Spark for batch processing]]
    - [[Using Apache Spark for stream processing][Using Apache Spark for stream processing]]
  - [[Understanding the Lambda architecture][Understanding the Lambda architecture]]
  - [[Understanding the Kappa Architecture][Understanding the Kappa Architecture]]
  - [[Design considerations for building scalable stream processing applications][Design considerations for building scalable stream processing applications]]
  - [[Building robust ETL pipelines using Spark SQL][Building robust ETL pipelines using Spark SQL]]
    - [[Choosing appropriate data formats][Choosing appropriate data formats]]
    - [[Transforming data in ETL pipelines][Transforming data in ETL pipelines]]
    - [[Addressing errors in ETL pipelines][Addressing errors in ETL pipelines]]
  - [[Implementing a scalable monitoring solution][Implementing a scalable monitoring solution]]
  - [[Deploying Spark machine learning pipelines][Deploying Spark machine learning pipelines]]
    - [[Understanding the challenges in typical ML deployment environments][Understanding the challenges in typical ML deployment environments]]
    - [[Understanding types of model scoring architectures][Understanding types of model scoring architectures]]
  - [[Using cluster managers][Using cluster managers]]
  - [[Summary][Summary]]

* Preface
** What this book covers
** What you need for this book
** Who this book is for
** Conventions
** Reader feedback
** Customer support
*** Downloading the example code
*** Downloading the color images of this book
*** Errata
*** Piracy
*** Questions

* 1. Getting Started with Spark SQL
  - The sections in this chapter will cover the following topics along with practice hands-on sessions:
    + What is Spark SQL?
    + Introducing SparkSession
    + Understanding Spark SQL concepts
      * Understanding RDDs, DataFrames, and Datasets
      * Understanding the Catalyst optimizer
      * Understanding Project Tungsten
    + Using Spark SQL in continuous applications
    + Understanding Structured Streaming internals

** What is Spark SQL?
** Introducing SparkSession
   - In Spark 2.0, ~SparkSession~ represents a *unified entry point* for manipulating
     data in Spark.

   - ~SparkSession~ _minimizes_ the number of different /contexts/ a developer
     has to use while working with Spark.
       SparkSession _replaces_ multiple /context/ objects, such as the
     ~SparkContext~, ~SQLContext~, and ~HiveContext~. These contexts are now
     encapsulated within the ~SparkSession~ object.

     =from Jian=
     This means you should use ~SparkSession~ rather than the /contexts/ already
     encapsulated with in it.

   -

** Understanding Spark SQL concepts
*** Understanding Resilient Distributed Datasets (RDDs)
*** Understanding DataFrames and Datasets
*** Understanding the Catalyst optimizer
**** Understanding Catalyst optimizations
**** Understanding Catalyst transformations

*** Introducing Project Tungsten
** Using Spark SQL in streaming applications
*** Understanding Structured Streaming internals

** Summary

* 2. Using Spark SQL for Processing Structured and Semistructured Data
** Understanding data sources in Spark applications
*** Selecting Spark data sources

** Using Spark with relational databases
** Using Spark with MongoDB (NoSQL database)
** Using Spark with JSON data
** Using Spark with Avro files
** Using Spark with Parquet files
** Defining and using custom data sources in Spark
** Summary

* 3. Using Spark SQL for Data Exploration
** Introducing Exploratory Data Analysis (EDA)
** Using Spark SQL for basic data analysis
*** Identifying missing data
*** Computing basic statistics
*** Identifying data outliers

** Visualizing data with Apache Zeppelin
** Sampling data with Spark SQL APIs
*** Sampling with the DataFrame/Dataset API
*** Sampling with the RDD API

** Using Spark SQL for creating pivot tables
** Summary

* 4. Using Spark SQL for Data Munging
** Introducing data munging
** Exploring data munging techniques
*** Pre-processing of the household electric consumption Dataset
*** Computing basic statistics and aggregations
*** Augmenting the Dataset
*** Executing other miscellaneous processing steps
*** Pre-processing of the weather Dataset
*** Analyzing missing data
*** Combining data using a JOIN operation

** Munging textual data
*** Processing multiple input data files
*** Removing stop words

** Munging time series data
*** Pre-processing of the time-series Dataset
*** Processing date fields
*** Persisting and loading data
*** Defining a date-time index
*** Using the  TimeSeriesRDD object
*** Handling missing time-series data
*** Computing basic statistics

** Dealing with variable length records
*** Converting variable-length records to fixed-length records
*** Extracting data from "messy" columns

** Preparing data for machine learning
*** Pre-processing data for machine learning
*** Creating and running a machine learning pipeline

** Summary

* 5. Using Spark SQL in Streaming Applications
** Introducing streaming data applications
** Building Spark streaming applications
*** Implementing sliding window-based functionality
*** Joining a streaming Dataset with a static Dataset
*** Using the Dataset API in Structured Streaming
*** Using output sinks
**** Using the Foreach Sink for arbitrary computations on output
**** Using the Memory Sink to save output to a table
**** Using the File Sink to save output to a partitioned table

*** Monitoring streaming queries

** Using Kafka with Spark Structured Streaming
*** Introducing Kafka concepts
*** Introducing ZooKeeper concepts
*** Introducing Kafka-Spark integration
*** Introducing Kafka-Spark Structured Streaming

** Writing a receiver for a custom data source
** Summary

* 6. Using Spark SQL in Machine Learning Applications
** Introducing machine learning applications
*** Understanding Spark ML pipelines and their components
*** Understanding the steps in a pipeline application development process

** Introducing feature engineering
*** Creating new features from raw data
*** Estimating the importance of a feature
*** Understanding dimensionality reduction
*** Deriving good features

** Implementing a Spark ML classification model
*** Exploring the diabetes Dataset
*** Pre-processing the data
*** Building the Spark ML pipeline
**** Using StringIndexer for indexing categorical features and labels
**** Using VectorAssembler for assembling features into one column
**** Using a Spark ML classifier
**** Creating a Spark ML pipeline
**** Creating the training and test Datasets
**** Making predictions using the PipelineModel
**** Selecting the best model
*** Changing the ML algorithm in the pipeline

** Introducing Spark ML tools and utilities
*** Using Principal Component Analysis to select features
*** Using encoders
*** Using Bucketizer
*** Using VectorSlicer
*** Using Chi-squared selector
*** Using a Normalizer
*** Retrieving our original labels
** Implementing a Spark ML clustering model
** Summary

* 7. Using Spark SQL in Graph Applications
** Introducing large-scale graph applications
** Exploring graphs using GraphFrames
*** Constructing a GraphFrame
*** Basic graph queries and operations
*** Motif analysis using GraphFrames
*** Processing subgraphs
*** Applying graph algorithms
*** Saving and loading GraphFrames

** Analyzing JSON input modeled as a graph
** Processing graphs containing multiple types of relationships
** Understanding GraphFrame internals
*** Viewing GraphFrame physical execution plan
*** Understanding partitioning in GraphFrames

** Summary

* 8. Using Spark SQL with SparkR
** Introducing SparkR
** Understanding the SparkR architecture
** Understanding SparkR DataFrames
** Using SparkR for EDA and data munging tasks
*** Reading and writing Spark DataFrames
*** Exploring structure and contents of Spark DataFrames
*** Running basic operations on Spark DataFrames
*** Executing SQL statements on Spark DataFrames
*** Merging SparkR DataFrames
*** Using User Defined Functions (UDFs)

** Using SparkR for computing summary statistics
** Using SparkR for data visualization
*** Visualizing data on a map
*** Visualizing graph nodes and edges

** Using SparkR for machine learning
** Summary

* 9. Developing Applications with Spark SQL
** Introducing Spark SQL applications
** Understanding text analysis applications
*** Using Spark SQL for textual analysis
**** Preprocessing textual data
**** Computing readability
**** Using word lists

*** Creating data preprocessing pipelines

** Understanding themes in document corpuses
** Using Naive Bayes classifiers
** Developing a machine learning application
** Summary

* 10. Using Spark SQL in Deep Learning Applications
** Introducing neural networks
*** Understanding deep learning
*** Understanding representation learning
*** Understanding stochastic gradient descent

** Introducing deep learning in Spark
*** Introducing CaffeOnSpark
*** Introducing DL4J
*** Introducing TensorFrames
*** Working with BigDL
*** Tuning hyperparameters of deep learning models
*** Introducing deep learning pipelines

** Understanding Supervised learning
*** Understanding convolutional neural networks
*** Using neural networks for text classification

** Using deep neural networks for language processing
*** Understanding Recurrent Neural Networks

** Introducing autoencoders
** Summary

* 11. Tuning Spark SQL Components for Performance
** Introducing performance tuning in Spark SQL
** Understanding DataFrame/Dataset APIs
*** Optimizing data serialization

** Understanding Catalyst optimizations
*** Understanding the Dataset/DataFrame API
*** Understanding Catalyst transformations

** Visualizing Spark application execution
*** Exploring Spark application execution metrics
*** Using external tools for performance tuning

** Cost-based optimizer in Apache Spark 2.2
*** Understanding the CBO statistics collection
*** Statistics collection functions
**** Filter operator
**** Join operator

*** Build side selection

** Understanding multi-way JOIN ordering optimization
** Understanding performance improvements using whole-stage code generation
** Summary

* 12. Spark SQL in Large-Scale Application Architectures
** Understanding Spark-based application architectures
*** Using Apache Spark for batch processing
*** Using Apache Spark for stream processing

** Understanding the Lambda architecture
** Understanding the Kappa Architecture
** Design considerations for building scalable stream processing applications
** Building robust ETL pipelines using Spark SQL
*** Choosing appropriate data formats
*** Transforming data in ETL pipelines
*** Addressing errors in ETL pipelines

** Implementing a scalable monitoring solution
** Deploying Spark machine learning pipelines
*** Understanding the challenges in typical ML deployment environments
*** Understanding types of model scoring architectures

** Using cluster managers
** Summary
