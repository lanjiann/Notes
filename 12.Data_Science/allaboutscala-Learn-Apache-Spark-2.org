#+TITLE: Learning Apache Spark 2
#+COMMENT: allaboutscala.com
#+VERSION: 2018
#+AUTHOR: ???
#+STARTUP: entitiespretty

* Table of Contents                                      :TOC_4_org:noexport:
- [[Chapter 12 Learn Apache Spark 2][Chapter 12 Learn Apache Spark 2]]
  - [[Part 1 ~DataFrame~ SQL Query (with Project Setup)][Part 1 ~DataFrame~ SQL Query (with Project Setup)]]
    - [[Project Setup:][Project Setup:]]
      - [[1. Using StackOverflow dataset][1. Using StackOverflow dataset]]
      - [[2. Add Apache Spark 2 SBT dependencies][2. Add Apache Spark 2 SBT dependencies]]
      - [[3. Bootstrap a SparkSession][3. Bootstrap a SparkSession]]
    - [[DataFrame SQL Query:][DataFrame SQL Query:]]
      - [[1. DataFrame Introduction][1. DataFrame Introduction]]
      - [[2. Create DataFrame from a CSV file][2. Create DataFrame from a CSV file]]
      - [[3. DataFrame schema][3. DataFrame schema]]
      - [[4. Select columns][4. Select columns]]
      - [[5. Filter by column value][5. Filter by column value]]
      - [[6. Count rows][6. Count rows]]
      - [[7. SQL like][7. SQL like]]
      - [[8. Filter chaining][8. Filter chaining]]
      - [[9. SQL In][9. SQL In]]
      - [[10. SQL Group By][10. SQL Group By]]
      - [[11. SQL Group By with filter][11. SQL Group By with filter]]
      - [[12. SQL order by][12. SQL order by]]
      - [[13. Cast column data type][13. Cast column data type]]
      - [[14. Filtered dataframe][14. Filtered dataframe]]
      - [[15. Dataframe Join][15. Dataframe Join]]
      - [[16. Join and select columns][16. Join and select columns]]
      - [[17. Join on explicit columns][17. Join on explicit columns]]
      - [[18. Inner Join][18. Inner Join]]
      - [[19. Left Outer Join][19. Left Outer Join]]
      - [[20. Right Outer Join][20. Right Outer Join]]
      - [[21. Distinct][21. Distinct]]
  - [[Part 2 Spark SQL][Part 2 Spark SQL]]
    - [[Spark SQL Introduction][Spark SQL Introduction]]
    - [[Register temporary table][Register temporary table]]
    - [[List all tables in Spark's catalog][List all tables in Spark's catalog]]
    - [[List catalog tables using Spark SQL][List catalog tables using Spark SQL]]
    - [[Select columns][Select columns]]
    - [[Filter by column value][Filter by column value]]
    - [[Count number of rows][Count number of rows]]
    - [[SQL like][SQL like]]
    - [[SQL where with and clause][SQL where with and clause]]
    - [[SQL In clause][SQL In clause]]
    - [[SQL Group by][SQL Group by]]
    - [[SQL Group by with having clause][SQL Group by with having clause]]
    - [[SQL Order by][SQL Order by]]
    - [[Typed columns, filter and create temp table][Typed columns, filter and create temp table]]
    - [[SQL Inner join][SQL Inner join]]
    - [[SQL Left outer join][SQL Left outer join]]
    - [[SQL Right outer join][SQL Right outer join]]
    - [[SQL Distinct][SQL Distinct]]
    - [[Register User Defined Function (UDF)][Register User Defined Function (UDF)]]
  - [[Part 3 ~DataFrame~ Statistics][Part 3 ~DataFrame~ Statistics]]
    - [[1. DataFrame Statistics Introduction][1. DataFrame Statistics Introduction]]
    - [[2. Create DataFrame from CSV][2. Create DataFrame from CSV]]
    - [[3. Average][3. Average]]
    - [[4. Maximum][4. Maximum]]
    - [[5. Minimum][5. Minimum]]
    - [[6. Mean][6. Mean]]
    - [[7. Sum][7. Sum]]
    - [[8. Group by query with statistics][8. Group by query with statistics]]
    - [[9. DataFrame Statistics using describe()][9. DataFrame Statistics using describe()]]
    - [[10. Correlation][10. Correlation]]
    - [[11. Covariance][11. Covariance]]
    - [[12. Frequent Items][12. Frequent Items]]
    - [[13. Crosstab][13. Crosstab]]
    - [[14. Stratified sampling using sampleBy()][14. Stratified sampling using sampleBy()]]
    - [[15. Approximate Quantile][15. Approximate Quantile]]
    - [[16. Bloom Filter][16. Bloom Filter]]
    - [[17. Count Min Sketch][17. Count Min Sketch]]
    - [[18. Sampling With Replacement][18. Sampling With Replacement]]
  - [[Part 4 ~DataFrame~ Operations][Part 4 ~DataFrame~ Operations]]
    - [[~DataFrame~ Operations Introduction][~DataFrame~ Operations Introduction]]
    - [[Setup ~DataFrame~'s][Setup ~DataFrame~'s]]
    - [[Convert ~DataFrame~ row to Scala Case class][Convert ~DataFrame~ row to Scala Case class]]
    - [[Convert ~DataFrame~ row to Scala Case class using ~map()~ method][Convert ~DataFrame~ row to Scala Case class using ~map()~ method]]
    - [[Create ~DataFrame~ from Collection][Create ~DataFrame~ from Collection]]
    - [[~DataFrame~ Union][~DataFrame~ Union]]
    - [[~DataFrame~ Intersection][~DataFrame~ Intersection]]
    - [[Append column to ~DataFrame~ using ~withColumn()~ method][Append column to ~DataFrame~ using ~withColumn()~ method]]
  - [[Part 5 Spark Functions][Part 5 Spark Functions]]
    - [[Create ~DataFrame~ from ~Tuple~'s][Create ~DataFrame~ from ~Tuple~'s]]
    - [[Get ~DataFrame~ column names][Get ~DataFrame~ column names]]
    - [[~DataFrame~ column names and types][~DataFrame~ column names and types]]
    - [[Json into ~DataFrame~ using ~explode()~][Json into ~DataFrame~ using ~explode()~]]
    - [[Concatenate ~DataFrame~ using ~join()~][Concatenate ~DataFrame~ using ~join()~]]
    - [[Search ~DataFrame~ column using ~array_contains()~][Search ~DataFrame~ column using ~array_contains()~]]
    - [[Check ~DataFrame~ column exists][Check ~DataFrame~ column exists]]
    - [[Split ~DataFrame~ Array column][Split ~DataFrame~ Array column]]
    - [[Rename ~DataFrame~ column][Rename ~DataFrame~ column]]
    - [[Create ~DataFrame~ constant column][Create ~DataFrame~ constant column]]
    - [[~DataFrame~ new column with User Defined Function (UDF)][~DataFrame~ new column with User Defined Function (UDF)]]
    - [[~DataFrame~ first row][~DataFrame~ first row]]
    - [[Format ~DataFrame~ column][Format ~DataFrame~ column]]
    - [[~DataFrame~ column hashing][~DataFrame~ column hashing]]
    - [[~DataFrame~ String functions][~DataFrame~ String functions]]
    - [[~DataFrame~ drop ~null~][~DataFrame~ drop ~null~]]

* Chapter 12 Learn Apache Spark 2
** Part 1 ~DataFrame~ SQL Query (with Project Setup)
*** Project Setup:
**** 1. Using StackOverflow dataset
**** 2. Add Apache Spark 2 SBT dependencies
**** 3. Bootstrap a SparkSession

*** DataFrame SQL Query:
**** 1. DataFrame Introduction
**** 2. Create DataFrame from a CSV file
**** 3. DataFrame schema
**** 4. Select columns
**** 5. Filter by column value
**** 6. Count rows
**** 7. SQL like
**** 8. Filter chaining
**** 9. SQL In
**** 10. SQL Group By
**** 11. SQL Group By with filter
**** 12. SQL order by
**** 13. Cast column data type
**** 14. Filtered dataframe
**** 15. Dataframe Join
**** 16. Join and select columns
**** 17. Join on explicit columns
**** 18. Inner Join
**** 19. Left Outer Join
**** 20. Right Outer Join
**** 21. Distinct

** Part 2 Spark SQL
*** Spark SQL Introduction
*** Register temporary table
*** List all tables in Spark's catalog
*** List catalog tables using Spark SQL
*** Select columns
*** Filter by column value
*** Count number of rows
*** SQL like
*** SQL where with and clause
*** SQL In clause
*** SQL Group by
*** SQL Group by with having clause
*** SQL Order by
*** Typed columns, filter and create temp table
*** SQL Inner join
*** SQL Left outer join
*** SQL Right outer join
*** SQL Distinct
*** Register User Defined Function (UDF)

** Part 3 ~DataFrame~ Statistics
*** 1. DataFrame Statistics Introduction
*** 2. Create DataFrame from CSV
*** 3. Average
*** 4. Maximum
*** 5. Minimum
*** 6. Mean
*** 7. Sum
*** 8. Group by query with statistics
*** 9. DataFrame Statistics using describe()
*** 10. Correlation
*** 11. Covariance
*** 12. Frequent Items
*** 13. Crosstab
*** 14. Stratified sampling using sampleBy()
*** 15. Approximate Quantile
*** 16. Bloom Filter
*** 17. Count Min Sketch
*** 18. Sampling With Replacement

** Part 4 ~DataFrame~ Operations
*** ~DataFrame~ Operations Introduction
*** Setup ~DataFrame~'s
*** Convert ~DataFrame~ row to Scala Case class
*** Convert ~DataFrame~ row to Scala Case class using ~map()~ method
*** Create ~DataFrame~ from Collection
*** ~DataFrame~ Union
*** ~DataFrame~ Intersection
*** Append column to ~DataFrame~ using ~withColumn()~ method

** Part 5 Spark Functions
*** Create ~DataFrame~ from ~Tuple~'s
*** Get ~DataFrame~ column names
*** ~DataFrame~ column names and types
*** Json into ~DataFrame~ using ~explode()~
*** Concatenate ~DataFrame~ using ~join()~
*** Search ~DataFrame~ column using ~array_contains()~
*** Check ~DataFrame~ column exists
*** Split ~DataFrame~ Array column
*** Rename ~DataFrame~ column
*** Create ~DataFrame~ constant column
*** ~DataFrame~ new column with User Defined Function (UDF)
*** ~DataFrame~ first row
*** Format ~DataFrame~ column
*** ~DataFrame~ column hashing
*** ~DataFrame~ String functions
*** ~DataFrame~ drop ~null~
