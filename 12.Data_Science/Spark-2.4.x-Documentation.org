#+TITLE: Spark Official Documentation - 2.3.2
#+COMMENT: Programming Guides
#+VERSION: 2018
#+STARTUP: entitiespretty

* DONE Quick Start
  CLOSED: [2018-10-10 Wed 02:05]
** DONE Interactive Analysis with the Spark Shell
   CLOSED: [2018-10-10 Wed 01:52]
*** DONE Basics
    CLOSED: [2018-10-10 Wed 01:43]
    - Spark's _primary abstraction_ is a _distributed collection of items_ called
      a ~Dataset~.

    - ~Dataset~'s can be created from
      + /Hadoop InputFormats/ (such as HDFS files)
        OR
      + by transforming other ~Dataset~'s.

    - Create ~Dataset~'s from the ="README.md"= file.
      #+BEGIN_SRC scala
        // scala>
        val textFile = spark.read.textFile("README.md")  // transformation
        /// textFile: org.apache.spark.sql.Dataset[String] = [value: string]

        // scala>
        textFile.count()  // action
        /// res0: Long = 126

        // scala>
        textFile.first()  // TODO: ??? ???
        /// res1: String = # Apache Spark

        // scala>
        val linesWithSpark = textFile.filter(_.contains("Spark"))  // transformation
        /// linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]

        // scala>
        textFile.filter(_.contains("Spark")).count()
        // res3: Long = 15
      #+END_SRC

*** DONE More on Dataset Operations
    CLOSED: [2018-10-10 Wed 01:52]
    #+BEGIN_SRC scala
      // scala>
      textFile.map(line => line.split(" ").size).reduce((a, b) => math.max(a, b))  // action
      /// res4: Long = 15

      // scala> /// Use Java library
      import java.lang.Math
      textFile.map(line => line.split(" ").size).reduce((a, b) => Math.max(a, b))  // action
      /// res5: Int = 15

      // scala>
      val wordCounts = textFile.flatMap(line => line.split(" ")).groupByKey(identity).count() // transformation
      /// wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]

      // scala>
      wordCounts.collect()  // action
      /// res6: Array[(String, Int)] = Array((means,1), (under,2), (this,3), (Because,1), (Python,2), (agree,1), (cluster.,1), ...)
    #+END_SRC

*** DONE Caching
    CLOSED: [2018-10-10 Wed 01:47]
    Pulling data sets into a CLUSTER-WIDE /in-memory cache/.
    We do this for the repeatedly accessed data, rather than recompute everytime
    we access the data (the default settings).

    #+BEGIN_SRC scala
      // scala>
      linesWithSpark.cache()
      /// res7: lineWithSpark.type = [value: string]

      // scala>
      linesWithSpark.count
      /// res8: Long = 15

      // scala>
      linesWithSpark.count
      /// res9: Long = 15
    #+END_SRC

** DONE Self-Contained Applications
   CLOSED: [2018-10-10 Wed 02:05]
   #+BEGIN_SRC scala
     /* SimpleApp.scala */
     import org.apache.spark.sql.SparkSession

     object SimpleApp {
       def main(args: Array[String]) {
         val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
         val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
         val logData = spark.read.textFile(logFile).cache()
         val numAs = logData.filter(line => line.contains("a")).count
         val numBs = logData.filter(line => line.contains("b")).count
         println(s"Lines with a: $numAs, Lines with b: $numBs")
         spark.stop()
       }
     }
   #+END_SRC

   - =build.sbt=
     #+BEGIN_SRC scala
       name := "Simple Project"

       version := "1.0"

       scalaVersion := "2.11.8"

       libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.3.2"
     #+END_SRC

   - For sbt to work correctly, we'll need to layout =SimpleApp.scala= and =build.sbt=
     according to the typical directory structure. 
     #+BEGIN_SRC shell
       ## Your directory layout should look like this
       # $
       find .

       ## .
       ## ./build.sbt
       ## ./src
       ## ./src/main
       ## ./src/main/scala
       ## ./src/main/scala/SimpleApp.scala

       ### Package a jar containing your application

       # $
       sbt package
       ## ...
       ## [info] Packaging {..}/{..}/target/scala-2.11/simple-project_2.11-1.0.jar

       ## Use spark-submit to run your application
       # $
       ${SPARK_HOME}/bin/spark-submit \
         --class "SimpleApp" \
         --master local[4] \
         target/scala-2.11/simple-project_2.11-1.0.jar
       ## ...
       ## Lines with a: 46, Lines with b: 23
     #+END_SRC
     We can create a JAR package containing the application's code, then use the
     ~spark-submit~ script to run our program.

     * =from Jian= =???=
       What does the =--master local[4]= mean???
     
** DONE Where to Go from Here
   CLOSED: [2018-10-10 Wed 01:58]
   - For an in-depth overview of the API, start with _the RDD programming guide_
     and _the SQL programming guide_, or see "Programming Guides" menu of the
     Spark official site for other components.

   - For running applications on a cluster, head to the [[https://spark.apache.org/docs/latest/cluster-overview.html][deployment overview]].
     =TODO= =IMPORTANT=

   - Finally, Spark includes several samples in the examples directory ([[https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples][Scala]],
     [[https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples][Java]], [[https://github.com/apache/spark/tree/master/examples/src/main/python][Python]], [[https://github.com/apache/spark/tree/master/examples/src/main/r][R]]). You can run them as follows: =TODO=
     #+BEGIN_SRC shell
       # For Scala and Java, use run-example:
       run-example SparkPi

       # For Python examples, use spark-submit directly:
       spark-submit examples/src/main/python/pi.py

       # For R examples, use spark-submit directly:
       spark-submit examples/src/main/r/dataframe.R
     #+END_SRC

* TODO RDDs, Accumulators, Broadcast Vars
** Overview
** Linking with Spark
** Initializing Spark
*** Using the Shell

** Resilient Distributed Datasets (RDDs)
*** Parallelized Collections
*** External Datasets
*** RDD Operations
**** Basics
**** Passing Functions to Spark
**** Understanding closures
***** Example
***** Local vs. cluster modes
***** Printing elements of an RDD

**** Working with Key-Value Pairs
**** Transformations
**** Actions
**** Shuffle operations
***** Background
***** Performance Impact

*** RDD Persistence
**** Which Storage Level to Choose?
**** Removing Data

** Shared Variables
*** Broadcast Variables
*** Accumulators

** Deploying to a Cluster
** Launching Spark jobs from Java / Scala
** Unit Testing
** Where to Go from Here

* TODO Spark SQL, ~DataFrame~'s and ~Dataset~'s
** Overview
*** SQL
*** Datasets and DataFrames

** Getting Started
*** Starting Point: SparkSession
*** Creating DataFrames
*** Untyped Dataset Operations (aka DataFrame Operations)
*** Running SQL Queries Programmatically
*** Global Temporary View
*** Creating Datasets
*** Interoperating with RDDs
**** Inferring the Schema Using Reflection
**** Programmatically Specifying the Schema

*** Aggregations
**** Untyped User-Defined Aggregate Functions
**** Type-Safe User-Defined Aggregate Functions

** Data Sources
*** Generic Load/Save Functions
**** Manually Specifying Options
**** Run SQL on files directly
**** Save Modes
**** Saving to Persistent Tables
**** Bucketing, Sorting and Partitioning

*** Parquet Files
**** Loading Data Programmatically
**** Partition Discovery
**** Schema Merging
**** Hive metastore Parquet table conversion
***** Hive/Parquet Schema Reconciliation
***** Metadata Refreshing

**** Configuration

*** ORC Files
*** JSON Datasets
*** Hive Tables
**** Specifying storage format for Hive tables
**** Interacting with Different Versions of Hive Metastore

*** JDBC To Other Databases
*** Troubleshooting

** Performance Tuning
*** Caching Data In Memory
*** Other Configuration Options
*** Broadcast Hint for SQL Queries

** Distributed SQL Engine
*** Running the Thrift JDBC/ODBC server
*** Running the Spark SQL CLI

** PySpark Usage Guide for Pandas with Apache Arrow
*** Apache Arrow in Spark
**** Ensure PyArrow Installed

*** Enabling for Conversion to/from Pandas
*** Pandas UDFs (a.k.a. Vectorized UDFs)
**** Scalar
**** Grouped Map

*** Usage Notes
**** Supported SQL Types
**** Setting Arrow Batch Size
**** Timestamp with Time Zone Semantics

** Migration Guide
*** Upgrading From Spark SQL 2.3.0 to 2.3.1 and above
*** Upgrading From Spark SQL 2.2 to 2.3
*** Upgrading From Spark SQL 2.1 to 2.2
*** Upgrading From Spark SQL 2.0 to 2.1
*** Upgrading From Spark SQL 1.6 to 2.0
*** Upgrading From Spark SQL 1.5 to 1.6
*** Upgrading From Spark SQL 1.4 to 1.5
*** Upgrading from Spark SQL 1.3 to 1.4
**** DataFrame data reader/writer interface
**** DataFrame.groupBy retains grouping columns
**** Behavior change on DataFrame.withColumn

*** Upgrading from Spark SQL 1.0-1.2 to 1.3
**** Rename of SchemaRDD to DataFrame
**** Unification of the Java and Scala APIs
**** Isolation of Implicit Conversions and Removal of dsl Package (Scala-only)
**** Removal of the type aliases in org.apache.spark.sql for DataType (Scala-only)
**** UDF Registration Moved to sqlContext.udf (Java & Scala)
**** Python DataTypes No Longer Singletons

*** Compatibility with Apache Hive
**** Deploying in Existing Hive Warehouses
**** Supported Hive Features
**** Unsupported Hive Functionality
**** Incompatible Hive UDF

** Reference
*** Data Types
*** NaN Semantics

* TODO Structured Streaming
** Overview
** Quick Example
** Programming Model
*** Basic Concepts
*** Handling Event-time and Late Data
*** Fault Tolerance Semantics

** API using Datasets and DataFrames
*** Creating streaming DataFrames and streaming Datasets
**** Input Sources
**** Schema inference and partition of streaming DataFrames/Datasets

*** Operations on streaming DataFrames/Datasets
**** Basic Operations - Selection, Projection, Aggregation
**** Window Operations on Event Time
***** Handling Late Data and Watermarking

**** Join Operations
***** Stream-static Joins
***** Stream-stream Joins
****** Inner Joins with optional Watermarking
****** Outer Joins with Watermarking
****** Support matrix for joins in streaming queries

**** Streaming Deduplication
**** Arbitrary Stateful Operations
**** Unsupported Operations

*** Starting Streaming Queries
**** Output Modes
**** Output Sinks
***** Using Foreach

**** Triggers

*** Managing Streaming Queries
*** Monitoring Streaming Queries
**** Reading Metrics Interactively
**** Reporting Metrics programmatically using Asynchronous APIs
**** Reporting Metrics using Dropwizard

*** Recovering from Failures with Checkpointing

** Continuous Processing
** Additional Information

* TODO Spark Streaming (DStreams)
** Overview
** A Quick Example
** Basic Concepts
*** Linking
*** Initializing StreamingContext
*** Discretized Streams (DStreams)
*** Input DStreams and Receivers
*** Transformations on DStreams
*** Output Operations on DStreams
*** DataFrame and SQL Operations
*** MLlib Operations
*** Caching / Persistence
*** Checkpointing
*** Accumulators, Broadcast Variables, and Checkpoints
*** Deploying Applications
*** Monitoring Applications

** Performance Tuning
*** Reducing the Batch Processing Times
*** Setting the Right Batch Interval
*** Memory Tuning

** Fault-tolerance Semantics
** Where to Go from Here

* MLlib (Machine Learning)
** MLlib: Main Guide
*** Basic statistics
*** Pipelines
*** Extracting, transforming and selecting features
*** Classification and Regression
*** Clustering
*** Collaborative filtering
*** Frequent Pattern Mining
*** Model selection and tuning
*** Advanced topics

** MLlib: RDD-based API Guide
*** Data types
*** Basic statistics
*** Classification and regression
*** Collaborative filtering
*** Clustering
*** Dimensionality reduction
*** Feature extraction and transformation
*** Frequent pattern mining
*** Evaluation metrics
*** PMML model export
*** Optimization (developer)

* GraphX (Graph Processing)
** Overview
** Getting Started
** The Property Graph
*** Example Property Graph

** Graph Operators
*** Summary List of Operators
*** Property Operators
*** Structural Operators
*** Join Operators
*** Neighborhood Aggregation
**** Aggregate Messages (aggregateMessages)
**** Map Reduce Triplets Transition Guide (Legacy)
**** Computing Degree Information
**** Collecting Neighbors

*** Caching and Uncaching

** Pregel API
** Graph Builders
** Vertex and Edge RDDs
*** VertexRDDs
*** EdgeRDDs

** Optimized Representation
** Graph Algorithms
*** PageRank
*** Connected Components
*** Triangle Counting

** Examples

* SparkR (R on Spark)
** Overview
** SparkDataFrame
*** Starting Up: SparkSession
*** Starting Up from RStudio
*** Creating SparkDataFrames
**** From local data frames
**** From Data Sources
**** From Hive tables

*** SparkDataFrame Operations
**** Selecting rows, columns
**** Grouping, Aggregation
**** Operating on Columns
**** Applying User-Defined Function
***** Run a given function on a large dataset using dapply or dapplyCollect
****** dapply
****** dapplyCollect

***** Run a given function on a large dataset grouping by input column(s) and using gapply or gapplyCollect
****** gapply
****** gapplyCollect

***** Run local R functions distributed using spark.lapply
****** spark.lapply

*** Running SQL Queries from SparkR

** Machine Learning
*** Algorithms
**** Classification
**** Regression
**** Tree
**** Clustering
**** Collaborative Filtering
**** Frequent Pattern Mining
**** Statistics

*** Model persistence

** Data type mapping between R and Spark
** Structured Streaming
** R Function Name Conflicts
** Migration Guide
*** Upgrading From SparkR 1.5.x to 1.6.x
*** Upgrading From SparkR 1.6.x to 2.0
*** Upgrading to SparkR 2.1.0
*** Upgrading to SparkR 2.2.0
*** Upgrading to SparkR 2.3.0
*** Upgrading to SparkR 2.3.1 and above
