#+TITLE: Spark: The Definitive Guide
#+SUBTITLE: Big Data Processing Made Simple
#+VERSION: 2018
#+AUTHOR: Bill Chambers, Matei Zaharia
#+STARTUP: entitiespretty

* Preface
* Part I. Gentle Overview of Big Data and Spark

** Chapter 1. What Is Apache Spark?
   - Figure 1-1 illustrates all the components and libraries Spark offers to
     end-users.

*** Apache Spark's Philosophy
    - Break down our description of Apache Spark -- a unified computing engine
      and set of libraries for big data -- into its key components:
      + Unified

      + Computing engine

      + Libraries

*** Context: The Big Data Problem
*** History of Spark
*** The Present and Future of Spark
*** Running Spark
**** Downloading Spark Locally
**** Launching Spark's Interactive Consoles
**** Running Spark in the Cloud
**** Data Used in This Book

** Chapter 2. A Gentle Introduction to Spark
*** Spark's Basic Architecture
**** Spark Applications

*** Spark's Language APIs
*** Spark's APIs
*** Starting Spark
*** The ~SparkSession~
*** DataFrames
**** Partitions

*** Transformations
**** Lazy Evaluation

*** Actions
*** Spark UI
*** An End-to-End Example
**** DataFrames and SQL

*** Conclusion

** Chapter 3. A Tour of Spark's Toolset
*** Running Production Applications
*** Datasets: Type-Safe Structured APIs
*** Structured Streaming
*** Machine Learning and Advanced Analytics
*** Lower-Level APIs
*** SparkR
*** Spark's Ecosystem and Packages
*** Conclusion

* Part II. Structured APIs -- DataFrames, SQL, and Datasets
** TODO Chapter 4. Structured API Overview
   This part of the book will be a deep dive into _Spark's Structured APIs_.
   - _The Structured APIs_ are a tool for manipulating _all sorts of data_, from
     unstructured log files to semi-structured CSV files and highly structured
     Parquet files.

   - _The Structured APIs_ refer to three core types of distributed collection
     APIs:
     + Datasets
     + DataFrames
     + SQL tables and views

   - The *MAJORITY* of the _Structured APIs_ apply to *both* /batch/ and /streaming/
     computation. The migration from /batch/ to /streaming/ (or vice versa) can
     be with _little_ to _no_ effort.

   - _The Structured APIs_ are the fundamental abstraction that you will use to
     write the majority of your data flows.

   - In this chapter, we'll introduce the fundamental concepts that you should understand:
     + the typed and untyped APIs (and their differences);
     + what the core terminology is;
     + and, finally, how Spark actually takes your Structured API data flows and executes it on the cluster.
     + We will then provide more specific task-based information for working with certain types of data or data sources.

*** DONE DataFrames and Datasets
    CLOSED: [2019-06-13 Thu 15:32]
    - /DataFrames/ and /Datasets/ are *distributed* *table-like collections* with
      well-defined /rows/ and /columns/.
      + EACH /column/ must have the *same number* of /rows/ as all the other columns

      + EACH /column/ has _type information_ that *must be consistent* for EVERY
        /row/ in the collection.

    - To Spark, /DataFrames/ and /Datasets/ represent _immutable_, _lazily evaluated_
      plans that specify what operations to apply to data residing at a location
      to generate some output.

*** DONE Schemas
    CLOSED: [2019-06-13 Thu 15:32]
    - A schema defines the /column names and types/ of a ~DataFrame~.

    - You can _define /schemas/ manually_ or _read a /schema/ from a data source_
      (often called _schema on read_).

    - Schemas consist of /types/, meaning that you need a way of specifying what
      lies where.

*** DONE Overview of Structured Spark Types
    CLOSED: [2019-06-14 Fri 18:51]
    Spark is effectively a programming language of its own.

    - Internally, Spark uses an engine called *Catalyst* that maintains its /own
      type information/ through the _planning_ and _processing_ of work.

    - The majority of Spark manipulations will operate strictly on Spark types,
      no matter what language you use.

    - =TODO=
      We touch on why this is the case momentarily, but before we can, we need to
      discuss /Datasets/.

**** DataFrames Versus Datasets
     - In essence, within the /Structured APIs/, there are two more APIs:
       + the "untyped" /DataFrames/
       + the "typed" /Datasets/

     - Actually, even /DataFrames/ have types, but Spark maintains them completely
       and only checks whether those types line up to those specified in the
       /schema/ at _runtime_.

     - /Datasets/ are only available to JVM-based languages.
       We *specify* /types/ with /case classes/ or /Java beans/.

**** Columns
     - /Columns/ represent
       + a _simple type_ like an integer or string
       + a _complex type_ like an array or map
       + a /null/ value -- =from Jian= this /null/ is different from Java/Scala ~null~.

     - TODO Details in Chapter 5

**** Rows
     /Rows/ can be created manually from SQL, from RDDs, from data sources, or from scratch.

     - Example:
       ~spark.range(2).toDF.collect~

**** Spark Types

*** TODO Overview of Structured API Execution
**** Logical Planning
**** Physical Planning
**** Execution

*** TODO Conclusion

** TODO Chapter 5. Basic Structured Operations
   This chapter focuses exclusively on fundamental /DataFrame/ operations and
   _AVOIDS_ /aggregations/, /window functions/, and /joins/, which are discussed
   in subsequent chapters =TODO=.

   - Concepts of /DataFrame/ conponents:
     + /record/ and /column/:
       a DataFrame consists of a series of records (like rows in a table), that
       are of type Row, and a number of columns (like columns in a spreadsheet)
       that represent a computation expression that can be performed on each
       individual record in the Dataset.

     + schema :: the _name_ and /type/ of data in each /column/.

     + partitioning :: layout of the /DataFrame/ or /DataSet/'s physical distribution
                       across the cluster.

     + partitioning scheme :: how the partition is allocated.

   - You can set /partitioning scheme/ based on values in a certain /column/ or
     _nondeterministically_.

   - Create a /DataFrame/ from a JSON file:
     #+begin_src scala
       val df = spark.read.format("json")
         .load("/data/flight-data/json/2015-summary.json")
     #+end_src

   - /DataFrames/ have ~printSchema()~ /method/.

*** DONE Schemas
    CLOSED: [2019-06-18 Tue 13:29]
    You can let the data source define a schema (called /schema-on-read/) or
    define it explicitly by yourself.

    - *WARNING*
      TODO
      TODO
      TODO

    - Read "flight data from the United States Bureau of Transportation statistics",
      which is provided in this book source code and data repo, and return its /schema/:
      #+begin_src scala
        // in Scala
        spark.read.format("json").load("/data/flight-data/json/2015-summary.json").schema
        // org.apache.spark.sql.types.StructType = ...
        // StructType(StructField(DEST_COUNTRY_NAME,StringType,true),
        // StructField(ORIGIN_COUNTRY_NAME,StringType,true),
        // StructField(count,LongType,true))
      #+end_src

    - A /schema/ is a ~StructType~ made up of a number of /fields/, ~StructField~'s.

    - Each ~StructField~ has:
      + name
      + type
      + nullable condition

    - /Schemas/ can contain other ~StructType~'s (/Spark's complex types/).
      =TODO= Chpater 6

    - Enforce a specific /schema/ on a ~DataFrame~:
      #+begin_src scala
        import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
        import org.apache.spark.sql.types.Metadata

        val myManualSchema =
          StructType(Array(StructField("DEST_COUNTRY_NAME", StringType, true),
                           StructField("ORIGIN_COUNTRY_NAME", StringType, true),
                           StructField("count", LongType, false, Metadata.fromJson("{\"hello\":\"world\"}"))))

        val df = spark.read.format("json").
          schema(myManualSchema).
          load("/data/flight-data/json/2015-summary.json")
      #+end_src

    - As discussed in Chapter 4, we *CANNOT* simply set /types/ via the _per-language
      types_ because _Spark maintains its OWN type info_.

*** DONE Columns and Expressions
    CLOSED: [2019-07-29 Mon 15:26]
    TODO NOTE
    Columns in Spark are similar to columns in a spreadsheet, R dataframe, or pandas DataFrame.
    You can select, manipulate, and remove columns from DataFrames and these
    operations are represented as expressions.

    To Spark, columns are logical constructions that simply represent a value
    computed on a perrecord basis by means of an expression. This means that to
    have a real value for a column, we need to have a row; and to have a row, we
    need to have a DataFrame. You cannot manipulate an individual column outside
    the context of a DataFrame; you must use Spark transformations within a
    DataFrame to modify the contents of a column.

**** DONE Columns
     CLOSED: [2019-07-29 Mon 13:58]
     - Two simplest ways to refer to columns (both of them need a column name as
       input parameter):
       + ~col~ function
       + ~column~ function

     - Example:
       #+begin_src scala
         // in Scala
         import org.apache.spark.sql.functions.{col, column}

         col("someColumnName")
         column("someColumnName")
       #+end_src

     - We will stick to using ~col~ throughout this book.
       As mentioned, this column _might or might not exist_ in our ~DataFrame~'s.
       + Columns are not resolved until we compare the column names with those we
         are maintaining in the /catalog/.

       + Column and table resolution happens in the /analyzer phase/, as discussed
         in Chapter 4 =TODO=.

     - *NOTE*
       Syntatic sugar of referring to /columns/ (Scala ONLY):
       + ~$"myColumn"~
       + ~'myColumn~

***** DONE Explicit column references
      #+begin_src scala
        df.col("count")
      #+end_src
      This is useful for /joins/, and with this Spark does not need to resolve
      this column itself.

**** DONE Expressions
     CLOSED: [2019-07-29 Mon 15:26]
     - TODO

     - In the simplest case, an expression, created via the ~expr~ function, is
       just a ~DataFrame~ /column reference/.
       ~expr("someCol")~ \equiv{} ~col("someCol")~

***** DONE Columns as Expressions
      CLOSED: [2019-07-29 Mon 15:26]
      ~expr("someCol - 5")~ is the same transformation as performing
      ~col("someCol") - 5~, or even ~expr("someCol") - 5~

***** DONE Accessing a DataFrame's columns
      CLOSED: [2019-07-29 Mon 15:25]
      Use the ~columns~ /property/ to see ALL /columns/ on a ~DataFrame~:
      #+begin_src scala
        spark.read.format("json").load("/data/flight-data/json/2015-summary.json")
          .columns
      #+end_src

*** DONE Records and Rows
    CLOSED: [2019-07-29 Mon 15:36]
    - In Spark, each /row/ in a ~DataFrame~ is a single /record/.

    - Get the first /row/ form a ~DataFrame~: ~df.first~

**** DONE Creating Rows
     CLOSED: [2019-07-29 Mon 15:36]
     - Only a ~DataFrame~ has a /schema/.
       /Row/ doesn't have a /schema/.
       When you create a /row/ manually, you MUST specify the values in the same
       order as the /schema/.
       #+begin_src scala
         import org.apache.spark.sql.Row
         val myRow = Row("Hello", null, 1, false)
       #+end_src

     - /Cast/ is required, when you access a /column/ in a /row/, for exacting a
       value with right type:
       #+begin_src scala
         myRow(0)  // type Any
         myRow(0).asInstanceOf[String]  // String
         myRow.getString(0)  // String
         myRow.getInt(2)  // Int
       #+end_src

       Python doesn't need cast
       #+begin_src python
         myRow[0]
         myRow[2]
       #+end_src

*** TODO DataFrame Transformations
    - Some fundamental objectives that breaked down into several core operations:
      + Add /rows/ or /columns/
      + Remove /rows/ or /columns/
      + Transform a /row/ into a /column/ (or vice versa)
      + Change the order of /rows/ based on the values in /columns/

**** TODO Creating DataFrames
     - Create ~DataFrame~'s from _raw data sources_.
       TODO This is covered extensively in Chapter 9.

     - Example:
       #+begin_src scala
         // in Scala
         val df = spark.read.format("json")
           .load("/data/flight-data/json/2015-summary.json")

         df.createOrReplaceTempView("dfTable")
       #+end_src
       for illustration purposes later in this chapter, we will also register
       this as a temporary view so that we can query it with SQL and show off
       basic transformations in SQL, as well

     - Create ~DataFrame~'s on the fly by taking a set of /rows/ and coverting them
       to a ~DataFrame~:
       #+begin_src scala
         // in Scala
         import org.apache.spark.sql.Row
         import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}

         val myManualSchema = new StructType(
           Array(new StructField("some", StringType, true),
                 new StructField("col", StringType, true),
                 new StructField("names", StringType, false)))

         val myRows = Seq(Row("Hello", null, 1L))
         val myRDD = spark.sparkContext.parallelize(myRows)
         val myDf = spark.createDataFrame(myRDD, myManualSchema)
         mydf.show()
       #+end_src

     - *NOTE*
       With Spark /implicits/, you can call ~.toDF~ on a ~Seq~ type value.
         This does *NOT* play well with ~null~ types, so it's not necessarily
       recommended for production use cases.

**** DONE ~select~ and ~selectExpr~
     CLOSED: [2019-07-30 Tue 18:00]
     - ~select~ and ~selectExpr~ allow you to do the ~DataFrame~ equivalent of
       SQL queries on a table of data:
       #+begin_src sql
         -- in SQL
         SELECT * FROM dataFrameTable
         SELECT columnName FROM dataFrameTable
         SELECT columnName * 10, otherColumn, someOtherCol as c FROM dataFrameTable
       #+end_src

     - Select without transformation:
       #+begin_src scala
         df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(3)
       #+end_src
       In SQL ~SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 3~

     - You can refer to columns in a number of different ways
       #+begin_src scala
         import org.apache.spark.sql.functions.{expr, col, column}
         df.select(
           df.col("DEST_COUNTRY_NAME"),
           col("DEST_COUNTRY_NAME"),
           column("DEST_COUNTRY_NAME"),
           'DEST_COUNTRY_NAME,
           $"DEST_COUNTRY_NAME",
           expr("DEST_COUNTRY_NAME"))
           .show(2)
       #+end_src
       However, you can't mix ~Column~ objects and strings -- mix them is a common
       error, and it will result in a compiler error.

**** DONE Converting to Spark Types (Literals)
     CLOSED: [2019-07-30 Tue 15:25]
     TODO NOTE
     #+begin_src scala
       import org.apache.spark.sql.functions.lit

       df.select(expr("*"), lit(1).as("One")).show(2)

       // -- in SQL
       // SELECT *, 1 as One FROM dfTable LIMIT 2
     #+end_src

**** DONE Adding Columns
     CLOSED: [2019-07-30 Tue 15:30]
     - Set one value
       #+begin_src scala
         df.withColumn("numberOne", lit(1))

         // -- in SQL
         // SELECT *, 1 as numberOne FROM dfTable
       #+end_src

     - Set one boolean value based on equality check
       #+begin_src scala
         df.withColumn("withinCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME"))
       #+end_src

     - Copy a column and give it a new name:
       #+begin_src scala
         df.withColumn("Destination", expr("DEST_COUNTRY_NAME")).columns
       #+end_src

**** DONE Renaming Columns
     CLOSED: [2019-07-30 Tue 17:34]
     ~df.withColumnRenamed("DEST_COUNTRY_NAME", "dest")~

**** TODO Reserved Characters and Keywords
**** DONE Case Sensitivity
     CLOSED: [2019-07-30 Tue 17:45]
     _By default, Spark is case insensitive_,
     but you can configure it to make it _case sensitive_.
     #+begin_src sql
       set spark.sql.caseSensitive true
     #+end_src

**** DONE Removing Columns
     CLOSED: [2019-07-30 Tue 17:47]
     #+begin_src scala
       dr.drop("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME")
     #+end_src

**** DONE Changing a Column's Type (cast)
     CLOSED: [2019-07-30 Tue 17:48]
     #+begin_src scala
       df.withColumn("count2", col("count").cast("long"))
     #+end_src

**** DONE Filtering Rows
     CLOSED: [2019-07-30 Tue 17:52]
     #+begin_src scala
       df.filter(col("count") < 2)

       df.where("count < 2")
     #+end_src
     - You can also chain filters and make AND filters.
         Spark will combine them together and run -- then there is no need to
       write multiple conditions in one expression string, which will reduce the
       readibility.

**** DONE Getting Unique Rows
     CLOSED: [2019-07-30 Tue 17:57]
     Get unique rows base on some columns:
     #+begin_src scala
       df.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME").distinct()

       // -- in SQL
       // SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable
     #+end_src

**** TODO Random Samples
**** TODO Random Splits
**** TODO Concatenating and Appending Rows (Union)
**** TODO Sorting Rows
**** DONE Limit
     CLOSED: [2019-07-30 Tue 17:59]
     ~.limit(n)~ /method/

**** TODO Repartition and Coalesce
**** DONE Collecting Rows to the Driver
     CLOSED: [2019-08-06 Tue 16:56]
     - ~toLocalIterator~ allows you to iterate over the entire dataset
       partition-by-partition in a serial manner.

     - *WARNING*
       Any collection of data to the driver can be very expensive operation!
       + call ~collect~ on a large dataset can crash the driver.

       + Use ~toLocalIterator~ and have very large partitions, you can easily
         crash the /driver node/ and _lose_ the state of your application.
           This is also *expensive* because we can operate on a _one-by-one
         basis, instead of running computation in parallel._

*** DONE Conclusion
    CLOSED: [2019-08-06 Tue 16:47]
    This chapter covered basic operations on /DataFrames/.

** TODO Chapter 6. Working with Different Types of Data
   Chapter 5 presented basic ~DataFrame~ concepts and abstractions. This chapter
   covers building expressions, which are the bread and butter of Spark's
   structured operations. We also review working with a variety of different
   kinds of data, including the following:
   - Booleans
   - Numbers
   - Strings
   - Dates and timestamps
   - Handling ~null~
   - Complex types
   - User-defined functions

*** TODO Where to Look for APIs
*** DONE Converting to Spark Types
    CLOSED: [2019-07-30 Tue 18:38]
    ~lit~

*** TODO Working with Booleans
    - xx

    - *WARNING*
      + TODO Scala's ~==~ and ~===~ ???

      + Use ~===~ or ~=!=~.
        Or use ~not~ function and ~equalTo~ /method/.

    - xx

    - *WARNING*
      null-safe equaivalence test:
      #+begin_src scala
        df.where(col("Description") euNullSafe "hello")
      #+end_src

*** DONE Working with Numbers
    CLOSED: [2019-07-30 Tue 19:13]
    - Functions
      #+begin_src scala
        import org.apache.spark.sql.functions.{expr, pow}

        val fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5
        df.select(expr("CustomerId"), fabricatedQuantity.alias("realquantity"))
      #+end_src

    - We can do all of this as a SQL expression:
      #+begin_src scala
        df.selectExpr("CustomerId", "(POWER((qUANTITY * uNITpRICE), 2.0) + 5) as realQuantity")

        // -- in SQL
        // SELECT customerId, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity
        // FROM dfTable
      #+end_src

    - Rounding
      #+begin_src scala
        import org.apache.spark.sql.functions.{round, bround, lit}

        df.select(round(col("UnitPrice"), 1).alias("rounded"), col("UnitPrice"))

        // round down
        df.select(round(lit("2.5")), bround(lit("2.5")))
      #+end_src

    - Correlation:
      Pearson correlation coefficient
      #+begin_src scala
        import org.apache.spark.sql.functions.{corr}

        df.stat.corr("Quantity", "UnitPrice")
        df.select(corr("Quantity", "UnitPrice"))


        // -- in SQL
        // SELECT corr(Quantity, UnitPrice) FROM dfTable
      #+end_src

    - Compute summary statistics for a column or set of columns.
        This will _take all numeric columns_ and
      *calculate* the _count_, _mean_, _standard deviation_, _min_, and _max_.
      You should use this primarily for viewing in the console because the
      schema might change in the future:
      #+begin_src scala
        df.describe().show()
      #+end_src
      + You can also extract the statistics separately:
        ~import org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}~

    - There are a number of statistical functions available in the ~StatFunctions~
      Package (accessible using ~stat~ as we see in the code block below). These
      are /DataFrame methods/ that you can use to calculate a variety of different
      things. For instance, you can calculate either exact or approximate
      quantiles of your data using the ~approxQuantile~ method TODO TODO TODO:
      #+begin_src scala
        val colName = "UnitPrice"
        val quantileProbs = Array(0.5)
        val relError = 0.05
        df.stat.approxQuantile("UnitPrice", quantileProbs, relError)  // 2.51
      #+end_src

    - You also can use this to see a /cross-tabulation/ or /frequent item pairs/
      (*be careful, this output will be large and is omitted for this reason*):
      #+begin_src scala
        df.stat.crosstab("StockCode", "Quantity")

        df.stat.freqItems(Seq("StockCode", "Quantity"))
      #+end_src
      TODO: ??? The concepts of /cross-tabulation/ or /frequent item pairs/ ???

    - As a last note, we can also *add a unique ID to each row by using the function*
      ~monotonically_increasing_id~. _This function generates a unique value for each
      row, starting with 0_:
      #+begin_src scala
        import org.apache.spark.sql.functions.monotonically_increasing_id

        df.select(monotonically_increasing_id())
      #+end_src

    - *There are functions added with every release, so check the documentation
      for more methods.* Be sure to search the API documentation for more
      information and functions. For instance,
      + there are some random data generation tools (e.g., ~rand()~, ~randn()~)
        with which you can randomly generate data;
        TODO _however, there are potential determinism issues when doing so. (You
        can find discussions about these challenges on the Spark mailing list.)_

      + There are also a number of more advanced tasks like /bloom filtering/ and
        /sketching algorithms/ available in the /stat package/ TODO that we
        mentioned (and linked to) at the beginning of this chapter.
        TODO TODO TODO ???

*** TODO Working with Strings
**** Regular Expressions

*** TODO Working with Dates and Timestamps
*** TODO Working with Nulls in Data
**** Coalesce
**** ifnull, nullIf, nvl, and nvl2
**** drop
**** fill
**** replace

*** TODO Ordering
*** TODO Working with Complex Types
**** Structs
**** Arrays
**** split
**** Array Length
**** ~array_contains~
**** ~explode~
**** Maps

*** TODO Working with JSON
*** TODO User-Defined Functions
*** TODO Conclusion

** Chapter 7. Aggregations
   - Aggregation :: the act of collecting something together and is a cornerstone
                    of big data analytics.

   - In an /aggregation/, we will specify
     + a key or grouping

     + an /aggregation function/ that specifies how you should _transform_ one or
       more columns.

   - With Spark you can aggregate any kind of value into an /array/, /list/, or
     /map/, as we will see in "Aggregating to Complex Types". TODO

   - In addition to working with ANY /type/ of values,
     Spark also allows us to create the following /groupings types/:
     + Summarize a complete ~DataFrame~ by performing an aggregation in
     + "group by"
     + "window"
     + "grouping set"
     + "rollup"
     + "cube"

   - Each /grouping/ returns a ~RelationalGroupedDataset~ on which we specify our
     aggregations.

   - TODO

   - NOTE
     TODO

*** Aggregation Functions
    - All /aggregations/ are available as functions, in addition to the special
      cases that can apear on ~DataFrame~'s or via ~.stat~, like we saw in Chapter 6.

    - You can find most /aggregation functions/ in the /package/
      ~org.apache.spark.sql.functions~

    - *NOTE*
      TODO

**** ~count~
**** ~countDistinct~
**** ~approx_count_distinct~
**** ~first~ and ~last~
**** ~min~ and ~max~
**** ~sum~
**** ~sumDistinct~
**** ~avg~
**** Variance and Standard Deviation
**** skewness and kurtosis
**** Covariance and Correlation
**** Aggregating to Complex Types

*** Grouping
**** Grouping with Expressions
**** Grouping with Maps

*** Window Functions
*** Grouping Sets
**** Rollups
**** Cube
**** Grouping Metadata
**** Pivot

*** User-Defined Aggregation Functions
*** Conclusion

** Chapter 8. Joins
*** Join Expressions
*** Join Types
*** Inner Joins
*** Outer Joins
*** Left Outer Joins
*** Right Outer Joins
*** Left Semi Joins
*** Left Anti Joins
*** Natural Joins
*** Cross (Cartesian) Joins
*** Challenges When Using Joins
**** Joins on Complex Types
**** Handling Duplicate Column Names

*** How Spark Performs Joins
**** Communication Strategies

*** Conclusion

** Chapter 9. Data Sources
*** The Structure of the Data Sources API
**** Read API Structure
**** Basics of Reading Data
**** Write API Structure
**** Basics of Writing Data

*** CSV Files
**** CSV Options
**** Reading CSV Files
**** Writing CSV Files

*** JSON Files
**** JSON Options
**** Reading JSON Files
**** Writing JSON Files

*** Parquet Files
**** Reading Parquet Files
**** Writing Parquet Files

*** ORC Files
**** Reading ORC Files
**** Writing ORC Files

*** SQL Databases
**** Reading from SQL Databases
**** Query Pushdown
**** Writing to SQL Databases

*** Text Files
**** Reading Text Files
**** Writing Text Files

*** Advanced I/O Concepts
**** Splittable File Types and Compression
**** Reading Data in Parallel
**** Writing Data in Parallel
**** Writing Complex Types
**** Managing File Size

*** Conclusion

** Chapter 10. Spark SQL
*** What is SQL?
*** Big Data and SQL: Apache Hive
*** Big Data and SQL: Spark SQL
**** Spark's Relationship to Hive

*** How to Run Spark SQL Queries
**** Spark SQL CLI
**** Spark's Programmatic SQL Interface
**** SparkSQL Thrift JDBC/ODBC Server

*** Catalog
*** Tables
**** Spark-Managed Tables
**** Creating Tables
**** Creating External Tables
**** Inserting into Tables
**** Describing Table Metadata
**** Refreshing Table Metadata
**** Dropping Tables
**** Canching Tables

*** Views
**** Creating Views
**** Dropping Views

*** Databases
**** Creating Databases
**** Setting the Databases
**** Dropping Databases

*** Select Statements
**** case...when...then Statements

*** Advanced Topics
**** Complex Types
**** Functions
**** Subqueries

*** Miscellaneous Features
**** Configurations
**** Setting Configuration Values in SQL

*** Conclusion

** Chapter 11. Datasets
*** When to Use Datasets
*** Creating Datasets
**** In Java: Encoders
**** In Scala: Case Classes

*** Actions
*** Transformations
**** Filtering
**** Mapping

*** Joins
*** Grouping and Aggregations
*** Conclusion

* TODO Part III. Low-Level APIs
** TODO 12. Resilient Distributed Datasets (RDDs)
*** What Are the Low-Level APIs?
**** When to Use the Low-Level APIs?
**** How to Use the Low-Level APIs?

*** About RDDs
**** Types of RDDs
**** When to Use RDDs?
**** Datasets and RDDs of Case Classes

*** Creating RDDs
**** Interoperating Between DataFrames, Datasets, and RDDs
**** From a Local Collection
**** From Data Sources

*** Manipulating RDDs
*** Transformations
**** ~distinct~
**** ~filter~
**** ~map~
**** ~sort~
**** Random Splits

*** Actions
**** ~reduce~
**** ~count~
**** ~first~
**** ~max~ and ~min~
**** ~take~

*** Saving Files
**** ~saveAsTextFile~
**** ~SequenceFiles~
**** Hadoop Files

*** Caching
*** Checkpointing
*** Pipe RDDs to System Commands
**** ~mapPartitions~
**** ~foreachPartition~
**** ~glom~

*** Conclusion

** TODO 13. Advanced RDDs
*** Key-Value Basics (Key-Value RDDs)
**** ~keyBy~
**** Mapping over Values
**** Extracting Keys and Values
**** lookup
**** sampleByKey

*** Aggregations
**** ~countByKey~
**** Understanding Aggregation Implementations
**** Other Aggregation Methods

*** CoGroups
*** Joins
**** Inner Join
**** zips

*** Controlling Partitions
**** coalesce
**** repartition
**** ~repartitionAndSortWithinPartitions~
**** Custom Partitioning

*** Custom Serialization
*** Conclusion

** TODO 14. Distributed Shared Variables
*** Broadcast Variables
*** Accumulators
**** Basic Example
**** Custom Accumulators

*** Conclusion

* TODO Part IV. Production Applications
** TODO 15. How Spark Runs on a Cluster
*** TODO The Architecture of a Spark Application
**** Execution Modes

*** TODO The Life Cycle of a Spark Application (Outside Spark)
**** Client Request
**** Launch
**** Execution
**** Completion

*** TODO The Life Cycle of a Spark Application (Inside Spark)
**** The ~SparkSession~
**** Logical Instructions
**** A Spark Job
**** Stages
**** Tasks

*** TODO Execution Details
**** Pipelining
**** Shuffle Persistence

*** TODO Conclusion

** TODO 16. Developing Spark Applications
*** TODO Writing Spark Applications
*** TODO Testing Spark Applications
**** Strategic Principles
**** Tactical Takeaways
**** Connecting to Unit Testing Frameworks
**** Connecting to Data Sources

*** TODO The Development Process
*** TODO Launching Applications
**** Application Launch Examples

*** TODO Configuring Applications
**** The SparkConf
**** Application Properties
**** Runtime Properties
**** Execution Properties
**** Configuring Memory Management
**** Configuring Shuffle Behavior
**** Environmental Variables
**** Job Scheduling Within an Application

*** TODO Conclusion

** TODO 17. Deploying Spark
*** TODO Where to Deploy Your Cluster to Run Spark Applications
**** On-Premises Cluster Deployments
**** Spark in the Cloud

*** TODO Cluster Managers
**** Standalone Mode
**** Spark on YARN
**** Configuring Spark on YARN Applications
**** Spark on Mesos
**** Secure Deployment Configurations
**** Cluster Networking Configurations
**** Application Scheduling

*** TODO Miscellaneous Considerations
*** TODO Conclusion

** TODO 18. Monitoring and Debugging
*** TODO The Monitoring Landscape
*** TODO What to Monitor
**** Driver and Executor Processes
**** Queries, Jobs, Stages, and Tasks

*** TODO Spark Logs
*** TODO The Spark UI
**** Spark REST API
**** Spark UI History Server

*** TODO Debugging and Spark First Aid
**** Spark Jobs Not Starting
**** Error Before Execution
**** Error During Execution
**** Show Tasks or Stragglers
**** Slow Aggregations
**** Slow Joins
**** Slow Reads and Writes
**** Driver OutOfMemoryError or Driver Unresponsive
**** Executor OutOfMemoryError or Executor Unresponsive
**** Unexpected Nulls in Results
**** No Space Left on Disk Errors
**** Serialization Errors

*** TODO Conclusion

** TODO 19. Performance Tuning
*** TODO Indirect Performance Enhancements
**** Design Choices
**** Object Serialization in RDDs
**** Cluster Configurations
**** Scheduling
**** Data at Rest
**** Shuffle Configurations
**** Memory Pressure and Garbage Collection

*** TODO Direct Performance Enhancements
**** Parallelism
**** Improved Filtering
**** Repartitioning and Coalescing
**** User-Defined Functions (UDFs)
**** Temporary Data Storage (Caching)
**** Joins
**** Aggregations
**** Broadcast Variables

*** TODO Conclusion

* TODO Part V. Streaming
** 20. Stream Processing Fundamentals
*** What Is Stream Processing?
**** Stream Processing Use Cases
**** Advantages of Stream Processing
**** Challenges of Stream Processing

*** Stream Processing Design Points
**** Record-at-a-Time Versus Declarative APIs
**** Event Time Versus Processing Time
**** Continuous Versus Micro-Batch Execution

*** Spark's Streaming APIs
**** The DStream API
**** Structured Streaming

*** Conclusion

** 21. Structured Streaming Basics
*** Structured Streaming Basics
*** Core Concepts
**** Transformations and Actions
**** Input Sources
**** Sinks
**** Output Modes
**** Triggers
**** Event-Time Processing

*** Structured Streaming in Action
*** Transformations on Streams
**** Selections and Filtering
**** Aggregations
**** Joins

*** Input and Output
**** Where Data Is Read and Written (Sources and Sinks)
**** Reading from the Kafka Source
**** Writing to the Kafka Sink
**** How Data Is Output (Output Modes)
**** When Data Is Output (Triggers)

*** Streaming Dataset AP
*** Conclusion

** 22. Event-Time and Stateful Processing
*** Event Time
*** Stateful Processing
*** Arbitrary Stateful Processing
*** Event-Time Basics
*** Windows on Event Time
**** Tumbling Windows
**** Handling Late Data with Watermarks

*** Dropping Duplicates in a Stream
*** Arbitrary Stateful Processing
**** Time-Outs
**** Output Modes
**** ~mapGroupsWithState~
**** ~flatMapGroupsWithState~

*** Conclusion

** 23. Structured Streaming in Production
*** Fault Tolerance and Checkpointing
*** Updating Your Application
**** Updating Your Streaming Application Code
**** Updating Your Spark Version
**** Sizing and Rescaling Your Application

*** Metrics and Monitoring
**** Query Status
**** Recent Progress
**** Spark UI

*** Alerting
*** Advanced Monitoring with the Streaming Listener
*** Conclusion

* TODO Part VI. Advanced Analytics and Machine Learning
** 24. Advanced Analytics and Machine Learning Overview
** 25. Preprocessing and Feature Engineering
** 26. Classification
** 27. Regression
** 28. Recommendation
** 29. Unsupervised Learning
** 30. Graph Analytics
** 31. Deep Learning

* Part VII. Ecosystem
** 32. Language Specifics: Python (PySpark) and R (SparkR and sparklyr)
*** PySpark
**** Fundamental PySpark Differences
**** Pandas Integration

*** R on Spark
**** SparkR
**** sparklyr

*** Conclusion

** 33. Ecosystem and Community
*** Spark Packages
**** An Abridged List of Popular Packages
**** Using Spark Packages
**** External Packages

*** Community
**** Spark Summit
**** Local Meetups

*** Conclusion
