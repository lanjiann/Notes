#+TITLE: Spark Official Documentation - 2.3.2
#+COMMENT: Programming Guides
#+VERSION: 2018
#+STARTUP: entitiespretty

* Table of Contents                                      :TOC_4_org:noexport:
- [[Quick Start][Quick Start]]
  - [[Interactive Analysis with the Spark Shell][Interactive Analysis with the Spark Shell]]
    - [[Basics][Basics]]
    - [[More on Dataset Operations][More on Dataset Operations]]
    - [[Caching][Caching]]
  - [[Self-Contained Applications][Self-Contained Applications]]
  - [[Where to Go from Here][Where to Go from Here]]
- [[RDDs, Accumulators, Broadcast Vars][RDDs, Accumulators, Broadcast Vars]]
  - [[Overview][Overview]]
  - [[Linking with Spark][Linking with Spark]]
  - [[Initializing Spark][Initializing Spark]]
    - [[Using the Shell][Using the Shell]]
  - [[Resilient Distributed Datasets (RDDs)][Resilient Distributed Datasets (RDDs)]]
    - [[Parallelized Collections][Parallelized Collections]]
    - [[External Datasets][External Datasets]]
    - [[RDD Operations][RDD Operations]]
      - [[Basics][Basics]]
      - [[Passing Functions to Spark][Passing Functions to Spark]]
      - [[Understanding closures][Understanding closures]]
      - [[Working with Key-Value Pairs][Working with Key-Value Pairs]]
      - [[Transformations][Transformations]]
      - [[Actions][Actions]]
      - [[Shuffle operations][Shuffle operations]]
    - [[RDD Persistence][RDD Persistence]]
      - [[Which Storage Level to Choose?][Which Storage Level to Choose?]]
      - [[Removing Data][Removing Data]]
  - [[Shared Variables][Shared Variables]]
    - [[Broadcast Variables][Broadcast Variables]]
    - [[Accumulators][Accumulators]]
  - [[Deploying to a Cluster][Deploying to a Cluster]]
  - [[Launching Spark jobs from Java / Scala][Launching Spark jobs from Java / Scala]]
  - [[Unit Testing][Unit Testing]]
  - [[Where to Go from Here][Where to Go from Here]]
- [[Spark SQL, ~DataFrame~'s and ~Dataset~'s][Spark SQL, ~DataFrame~'s and ~Dataset~'s]]
  - [[Overview][Overview]]
    - [[SQL][SQL]]
    - [[Datasets and DataFrames][Datasets and DataFrames]]
  - [[Getting Started][Getting Started]]
    - [[Starting Point: SparkSession][Starting Point: SparkSession]]
    - [[Creating DataFrames][Creating DataFrames]]
    - [[Untyped Dataset Operations (aka DataFrame Operations)][Untyped Dataset Operations (aka DataFrame Operations)]]
    - [[Running SQL Queries Programmatically][Running SQL Queries Programmatically]]
    - [[Global Temporary View][Global Temporary View]]
    - [[Creating Datasets][Creating Datasets]]
    - [[Interoperating with RDDs][Interoperating with RDDs]]
      - [[Inferring the Schema Using Reflection][Inferring the Schema Using Reflection]]
      - [[Programmatically Specifying the Schema][Programmatically Specifying the Schema]]
    - [[Aggregations][Aggregations]]
      - [[Untyped User-Defined Aggregate Functions][Untyped User-Defined Aggregate Functions]]
      - [[Type-Safe User-Defined Aggregate Functions][Type-Safe User-Defined Aggregate Functions]]
  - [[Data Sources][Data Sources]]
    - [[Generic Load/Save Functions][Generic Load/Save Functions]]
      - [[Manually Specifying Options][Manually Specifying Options]]
      - [[Run SQL on files directly][Run SQL on files directly]]
      - [[Save Modes][Save Modes]]
      - [[Saving to Persistent Tables][Saving to Persistent Tables]]
      - [[Bucketing, Sorting and Partitioning][Bucketing, Sorting and Partitioning]]
    - [[Parquet Files][Parquet Files]]
      - [[Loading Data Programmatically][Loading Data Programmatically]]
      - [[Partition Discovery][Partition Discovery]]
      - [[Schema Merging][Schema Merging]]
      - [[Hive metastore Parquet table conversion][Hive metastore Parquet table conversion]]
      - [[Configuration][Configuration]]
    - [[ORC Files][ORC Files]]
    - [[JSON Datasets][JSON Datasets]]
    - [[Hive Tables][Hive Tables]]
      - [[Specifying storage format for Hive tables][Specifying storage format for Hive tables]]
      - [[Interacting with Different Versions of Hive Metastore][Interacting with Different Versions of Hive Metastore]]
    - [[JDBC To Other Databases][JDBC To Other Databases]]
    - [[Troubleshooting][Troubleshooting]]
  - [[Performance Tuning][Performance Tuning]]
    - [[Caching Data In Memory][Caching Data In Memory]]
    - [[Other Configuration Options][Other Configuration Options]]
    - [[Broadcast Hint for SQL Queries][Broadcast Hint for SQL Queries]]
  - [[Distributed SQL Engine][Distributed SQL Engine]]
    - [[Running the Thrift JDBC/ODBC server][Running the Thrift JDBC/ODBC server]]
    - [[Running the Spark SQL CLI][Running the Spark SQL CLI]]
  - [[PySpark Usage Guide for Pandas with Apache Arrow][PySpark Usage Guide for Pandas with Apache Arrow]]
    - [[Apache Arrow in Spark][Apache Arrow in Spark]]
      - [[Ensure PyArrow Installed][Ensure PyArrow Installed]]
    - [[Enabling for Conversion to/from Pandas][Enabling for Conversion to/from Pandas]]
    - [[Pandas UDFs (a.k.a. Vectorized UDFs)][Pandas UDFs (a.k.a. Vectorized UDFs)]]
      - [[Scalar][Scalar]]
      - [[Grouped Map][Grouped Map]]
    - [[Usage Notes][Usage Notes]]
      - [[Supported SQL Types][Supported SQL Types]]
      - [[Setting Arrow Batch Size][Setting Arrow Batch Size]]
      - [[Timestamp with Time Zone Semantics][Timestamp with Time Zone Semantics]]
  - [[Migration Guide][Migration Guide]]
    - [[Upgrading From Spark SQL 2.3.0 to 2.3.1 and above][Upgrading From Spark SQL 2.3.0 to 2.3.1 and above]]
    - [[Upgrading From Spark SQL 2.2 to 2.3][Upgrading From Spark SQL 2.2 to 2.3]]
    - [[Upgrading From Spark SQL 2.1 to 2.2][Upgrading From Spark SQL 2.1 to 2.2]]
    - [[Upgrading From Spark SQL 2.0 to 2.1][Upgrading From Spark SQL 2.0 to 2.1]]
    - [[Upgrading From Spark SQL 1.6 to 2.0][Upgrading From Spark SQL 1.6 to 2.0]]
    - [[Upgrading From Spark SQL 1.5 to 1.6][Upgrading From Spark SQL 1.5 to 1.6]]
    - [[Upgrading From Spark SQL 1.4 to 1.5][Upgrading From Spark SQL 1.4 to 1.5]]
    - [[Upgrading from Spark SQL 1.3 to 1.4][Upgrading from Spark SQL 1.3 to 1.4]]
      - [[DataFrame data reader/writer interface][DataFrame data reader/writer interface]]
      - [[DataFrame.groupBy retains grouping columns][DataFrame.groupBy retains grouping columns]]
      - [[Behavior change on DataFrame.withColumn][Behavior change on DataFrame.withColumn]]
    - [[Upgrading from Spark SQL 1.0-1.2 to 1.3][Upgrading from Spark SQL 1.0-1.2 to 1.3]]
      - [[Rename of SchemaRDD to DataFrame][Rename of SchemaRDD to DataFrame]]
      - [[Unification of the Java and Scala APIs][Unification of the Java and Scala APIs]]
      - [[Isolation of Implicit Conversions and Removal of dsl Package (Scala-only)][Isolation of Implicit Conversions and Removal of dsl Package (Scala-only)]]
      - [[Removal of the type aliases in org.apache.spark.sql for DataType (Scala-only)][Removal of the type aliases in org.apache.spark.sql for DataType (Scala-only)]]
      - [[UDF Registration Moved to sqlContext.udf (Java & Scala)][UDF Registration Moved to sqlContext.udf (Java & Scala)]]
      - [[Python DataTypes No Longer Singletons][Python DataTypes No Longer Singletons]]
    - [[Compatibility with Apache Hive][Compatibility with Apache Hive]]
      - [[Deploying in Existing Hive Warehouses][Deploying in Existing Hive Warehouses]]
      - [[Supported Hive Features][Supported Hive Features]]
      - [[Unsupported Hive Functionality][Unsupported Hive Functionality]]
      - [[Incompatible Hive UDF][Incompatible Hive UDF]]
  - [[Reference][Reference]]
    - [[Data Types][Data Types]]
    - [[NaN Semantics][NaN Semantics]]
- [[Structured Streaming][Structured Streaming]]
  - [[Overview][Overview]]
  - [[Quick Example][Quick Example]]
  - [[Programming Model][Programming Model]]
    - [[Basic Concepts][Basic Concepts]]
    - [[Handling Event-time and Late Data][Handling Event-time and Late Data]]
    - [[Fault Tolerance Semantics][Fault Tolerance Semantics]]
  - [[API using Datasets and DataFrames][API using Datasets and DataFrames]]
    - [[Creating streaming DataFrames and streaming Datasets][Creating streaming DataFrames and streaming Datasets]]
      - [[Input Sources][Input Sources]]
      - [[Schema inference and partition of streaming DataFrames/Datasets][Schema inference and partition of streaming DataFrames/Datasets]]
    - [[Operations on streaming DataFrames/Datasets][Operations on streaming DataFrames/Datasets]]
      - [[Basic Operations - Selection, Projection, Aggregation][Basic Operations - Selection, Projection, Aggregation]]
      - [[Window Operations on Event Time][Window Operations on Event Time]]
      - [[Join Operations][Join Operations]]
      - [[Streaming Deduplication][Streaming Deduplication]]
      - [[Arbitrary Stateful Operations][Arbitrary Stateful Operations]]
      - [[Unsupported Operations][Unsupported Operations]]
    - [[Starting Streaming Queries][Starting Streaming Queries]]
      - [[Output Modes][Output Modes]]
      - [[Output Sinks][Output Sinks]]
      - [[Triggers][Triggers]]
    - [[Managing Streaming Queries][Managing Streaming Queries]]
    - [[Monitoring Streaming Queries][Monitoring Streaming Queries]]
      - [[Reading Metrics Interactively][Reading Metrics Interactively]]
      - [[Reporting Metrics programmatically using Asynchronous APIs][Reporting Metrics programmatically using Asynchronous APIs]]
      - [[Reporting Metrics using Dropwizard][Reporting Metrics using Dropwizard]]
    - [[Recovering from Failures with Checkpointing][Recovering from Failures with Checkpointing]]
  - [[Continuous Processing][Continuous Processing]]
  - [[Additional Information][Additional Information]]
- [[Spark Streaming (DStreams)][Spark Streaming (DStreams)]]
  - [[Overview][Overview]]
  - [[A Quick Example][A Quick Example]]
  - [[Basic Concepts][Basic Concepts]]
    - [[Linking][Linking]]
    - [[Initializing StreamingContext][Initializing StreamingContext]]
    - [[Discretized Streams (DStreams)][Discretized Streams (DStreams)]]
    - [[Input DStreams and Receivers][Input DStreams and Receivers]]
    - [[Transformations on DStreams][Transformations on DStreams]]
    - [[Output Operations on DStreams][Output Operations on DStreams]]
    - [[DataFrame and SQL Operations][DataFrame and SQL Operations]]
    - [[MLlib Operations][MLlib Operations]]
    - [[Caching / Persistence][Caching / Persistence]]
    - [[Checkpointing][Checkpointing]]
    - [[Accumulators, Broadcast Variables, and Checkpoints][Accumulators, Broadcast Variables, and Checkpoints]]
    - [[Deploying Applications][Deploying Applications]]
    - [[Monitoring Applications][Monitoring Applications]]
  - [[Performance Tuning][Performance Tuning]]
    - [[Reducing the Batch Processing Times][Reducing the Batch Processing Times]]
    - [[Setting the Right Batch Interval][Setting the Right Batch Interval]]
    - [[Memory Tuning][Memory Tuning]]
  - [[Fault-tolerance Semantics][Fault-tolerance Semantics]]
  - [[Where to Go from Here][Where to Go from Here]]
- [[MLlib (Machine Learning)][MLlib (Machine Learning)]]
  - [[MLlib: Main Guide][MLlib: Main Guide]]
    - [[Basic statistics][Basic statistics]]
    - [[Pipelines][Pipelines]]
    - [[Extracting, transforming and selecting features][Extracting, transforming and selecting features]]
    - [[Classification and Regression][Classification and Regression]]
    - [[Clustering][Clustering]]
    - [[Collaborative filtering][Collaborative filtering]]
    - [[Frequent Pattern Mining][Frequent Pattern Mining]]
    - [[Model selection and tuning][Model selection and tuning]]
    - [[Advanced topics][Advanced topics]]
  - [[MLlib: RDD-based API Guide][MLlib: RDD-based API Guide]]
    - [[Data types][Data types]]
    - [[Basic statistics][Basic statistics]]
    - [[Classification and regression][Classification and regression]]
    - [[Collaborative filtering][Collaborative filtering]]
    - [[Clustering][Clustering]]
    - [[Dimensionality reduction][Dimensionality reduction]]
    - [[Feature extraction and transformation][Feature extraction and transformation]]
    - [[Frequent pattern mining][Frequent pattern mining]]
    - [[Evaluation metrics][Evaluation metrics]]
    - [[PMML model export][PMML model export]]
    - [[Optimization (developer)][Optimization (developer)]]
- [[GraphX (Graph Processing)][GraphX (Graph Processing)]]
  - [[Overview][Overview]]
  - [[Getting Started][Getting Started]]
  - [[The Property Graph][The Property Graph]]
    - [[Example Property Graph][Example Property Graph]]
  - [[Graph Operators][Graph Operators]]
    - [[Summary List of Operators][Summary List of Operators]]
    - [[Property Operators][Property Operators]]
    - [[Structural Operators][Structural Operators]]
    - [[Join Operators][Join Operators]]
    - [[Neighborhood Aggregation][Neighborhood Aggregation]]
      - [[Aggregate Messages (aggregateMessages)][Aggregate Messages (aggregateMessages)]]
      - [[Map Reduce Triplets Transition Guide (Legacy)][Map Reduce Triplets Transition Guide (Legacy)]]
      - [[Computing Degree Information][Computing Degree Information]]
      - [[Collecting Neighbors][Collecting Neighbors]]
    - [[Caching and Uncaching][Caching and Uncaching]]
  - [[Pregel API][Pregel API]]
  - [[Graph Builders][Graph Builders]]
  - [[Vertex and Edge RDDs][Vertex and Edge RDDs]]
    - [[VertexRDDs][VertexRDDs]]
    - [[EdgeRDDs][EdgeRDDs]]
  - [[Optimized Representation][Optimized Representation]]
  - [[Graph Algorithms][Graph Algorithms]]
    - [[PageRank][PageRank]]
    - [[Connected Components][Connected Components]]
    - [[Triangle Counting][Triangle Counting]]
  - [[Examples][Examples]]
- [[SparkR (R on Spark)][SparkR (R on Spark)]]
  - [[Overview][Overview]]
  - [[SparkDataFrame][SparkDataFrame]]
    - [[Starting Up: SparkSession][Starting Up: SparkSession]]
    - [[Starting Up from RStudio][Starting Up from RStudio]]
    - [[Creating SparkDataFrames][Creating SparkDataFrames]]
      - [[From local data frames][From local data frames]]
      - [[From Data Sources][From Data Sources]]
      - [[From Hive tables][From Hive tables]]
    - [[SparkDataFrame Operations][SparkDataFrame Operations]]
      - [[Selecting rows, columns][Selecting rows, columns]]
      - [[Grouping, Aggregation][Grouping, Aggregation]]
      - [[Operating on Columns][Operating on Columns]]
      - [[Applying User-Defined Function][Applying User-Defined Function]]
    - [[Running SQL Queries from SparkR][Running SQL Queries from SparkR]]
  - [[Machine Learning][Machine Learning]]
    - [[Algorithms][Algorithms]]
      - [[Classification][Classification]]
      - [[Regression][Regression]]
      - [[Tree][Tree]]
      - [[Clustering][Clustering]]
      - [[Collaborative Filtering][Collaborative Filtering]]
      - [[Frequent Pattern Mining][Frequent Pattern Mining]]
      - [[Statistics][Statistics]]
    - [[Model persistence][Model persistence]]
  - [[Data type mapping between R and Spark][Data type mapping between R and Spark]]
  - [[Structured Streaming][Structured Streaming]]
  - [[R Function Name Conflicts][R Function Name Conflicts]]
  - [[Migration Guide][Migration Guide]]
    - [[Upgrading From SparkR 1.5.x to 1.6.x][Upgrading From SparkR 1.5.x to 1.6.x]]
    - [[Upgrading From SparkR 1.6.x to 2.0][Upgrading From SparkR 1.6.x to 2.0]]
    - [[Upgrading to SparkR 2.1.0][Upgrading to SparkR 2.1.0]]
    - [[Upgrading to SparkR 2.2.0][Upgrading to SparkR 2.2.0]]
    - [[Upgrading to SparkR 2.3.0][Upgrading to SparkR 2.3.0]]
    - [[Upgrading to SparkR 2.3.1 and above][Upgrading to SparkR 2.3.1 and above]]

* DONE Quick Start
  CLOSED: [2018-10-10 Wed 02:05]
** DONE Interactive Analysis with the Spark Shell
   CLOSED: [2018-10-10 Wed 01:52]
*** DONE Basics
    CLOSED: [2018-10-10 Wed 01:43]
    - Spark's _primary abstraction_ is a _distributed collection of items_ called
      a ~Dataset~.

    - ~Dataset~'s can be created from
      + /Hadoop InputFormats/ (such as HDFS files)
        OR
      + by transforming other ~Dataset~'s.

    - Create ~Dataset~'s from the ="README.md"= file.
      #+BEGIN_SRC scala
        // scala>
        val textFile = spark.read.textFile("README.md")  // transformation
        /// textFile: org.apache.spark.sql.Dataset[String] = [value: string]

        // scala>
        textFile.count()  // action
        /// res0: Long = 126

        // scala>
        textFile.first()  // TODO: ??? ???
        /// res1: String = # Apache Spark

        // scala>
        val linesWithSpark = textFile.filter(_.contains("Spark"))  // transformation
        /// linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]

        // scala>
        textFile.filter(_.contains("Spark")).count()
        // res3: Long = 15
      #+END_SRC

*** DONE More on Dataset Operations
    CLOSED: [2018-10-10 Wed 01:52]
    #+BEGIN_SRC scala
      // scala>
      textFile.map(line => line.split(" ").size).reduce((a, b) => math.max(a, b))  // action
      /// res4: Long = 15

      // scala> /// Use Java library
      import java.lang.Math
      textFile.map(line => line.split(" ").size).reduce((a, b) => Math.max(a, b))  // action
      /// res5: Int = 15

      // scala>
      val wordCounts = textFile.flatMap(line => line.split(" ")).groupByKey(identity).count() // transformation
      /// wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]

      // scala>
      wordCounts.collect()  // action
      /// res6: Array[(String, Int)] = Array((means,1), (under,2), (this,3), (Because,1), (Python,2), (agree,1), (cluster.,1), ...)
    #+END_SRC

*** DONE Caching
    CLOSED: [2018-10-10 Wed 01:47]
    Pulling data sets into a CLUSTER-WIDE /in-memory cache/.
    We do this for the repeatedly accessed data, rather than recompute everytime
    we access the data (the default settings).

    #+BEGIN_SRC scala
      // scala>
      linesWithSpark.cache()
      /// res7: lineWithSpark.type = [value: string]

      // scala>
      linesWithSpark.count
      /// res8: Long = 15

      // scala>
      linesWithSpark.count
      /// res9: Long = 15
    #+END_SRC

** DONE Self-Contained Applications
   CLOSED: [2018-10-10 Wed 02:05]
   #+BEGIN_SRC scala
     /* SimpleApp.scala */
     import org.apache.spark.sql.SparkSession

     object SimpleApp {
       def main(args: Array[String]) {
         val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
         val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
         val logData = spark.read.textFile(logFile).cache()
         val numAs = logData.filter(line => line.contains("a")).count
         val numBs = logData.filter(line => line.contains("b")).count
         println(s"Lines with a: $numAs, Lines with b: $numBs")
         spark.stop()
       }
     }
   #+END_SRC

   - =build.sbt=
     #+BEGIN_SRC scala
       name := "Simple Project"

       version := "1.0"

       scalaVersion := "2.11.8"

       libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.3.2"
     #+END_SRC

   - For sbt to work correctly, we'll need to layout =SimpleApp.scala= and =build.sbt=
     according to the typical directory structure. 
     #+BEGIN_SRC shell
       ## Your directory layout should look like this
       # $
       find .

       ## .
       ## ./build.sbt
       ## ./src
       ## ./src/main
       ## ./src/main/scala
       ## ./src/main/scala/SimpleApp.scala

       ### Package a jar containing your application

       # $
       sbt package
       ## ...
       ## [info] Packaging {..}/{..}/target/scala-2.11/simple-project_2.11-1.0.jar

       ## Use spark-submit to run your application
       # $
       ${SPARK_HOME}/bin/spark-submit \
         --class "SimpleApp" \
         --master local[4] \
         target/scala-2.11/simple-project_2.11-1.0.jar
       ## ...
       ## Lines with a: 46, Lines with b: 23
     #+END_SRC
     We can create a JAR package containing the application's code, then use the
     ~spark-submit~ script to run our program.

     * =from Jian= =???=
       What does the =--master local[4]= mean???
     
** DONE Where to Go from Here
   CLOSED: [2018-10-10 Wed 01:58]
   - For an in-depth overview of the API, start with _the RDD programming guide_
     and _the SQL programming guide_, or see "Programming Guides" menu of the
     Spark official site for other components.

   - For running applications on a cluster, head to the [[https://spark.apache.org/docs/latest/cluster-overview.html][deployment overview]].
     =TODO= =IMPORTANT=

   - Finally, Spark includes several samples in the examples directory ([[https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples][Scala]],
     [[https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples][Java]], [[https://github.com/apache/spark/tree/master/examples/src/main/python][Python]], [[https://github.com/apache/spark/tree/master/examples/src/main/r][R]]). You can run them as follows: =TODO=
     #+BEGIN_SRC shell
       # For Scala and Java, use run-example:
       run-example SparkPi

       # For Python examples, use spark-submit directly:
       spark-submit examples/src/main/python/pi.py

       # For R examples, use spark-submit directly:
       spark-submit examples/src/main/r/dataframe.R
     #+END_SRC

* TODO RDDs, Accumulators, Broadcast Vars
** Overview
** Linking with Spark
** Initializing Spark
*** Using the Shell

** Resilient Distributed Datasets (RDDs)
*** Parallelized Collections
*** External Datasets
*** RDD Operations
**** Basics
**** Passing Functions to Spark
**** Understanding closures
***** Example
***** Local vs. cluster modes
***** Printing elements of an RDD

**** Working with Key-Value Pairs
**** Transformations
**** Actions
**** Shuffle operations
***** Background
***** Performance Impact

*** RDD Persistence
**** Which Storage Level to Choose?
**** Removing Data

** Shared Variables
*** Broadcast Variables
*** Accumulators

** Deploying to a Cluster
** Launching Spark jobs from Java / Scala
** Unit Testing
** Where to Go from Here

* TODO Spark SQL, ~DataFrame~'s and ~Dataset~'s
** Overview
*** SQL
*** Datasets and DataFrames

** Getting Started
*** Starting Point: SparkSession
*** Creating DataFrames
*** Untyped Dataset Operations (aka DataFrame Operations)
*** Running SQL Queries Programmatically
*** Global Temporary View
*** Creating Datasets
*** Interoperating with RDDs
**** Inferring the Schema Using Reflection
**** Programmatically Specifying the Schema

*** Aggregations
**** Untyped User-Defined Aggregate Functions
**** Type-Safe User-Defined Aggregate Functions

** Data Sources
*** Generic Load/Save Functions
**** Manually Specifying Options
**** Run SQL on files directly
**** Save Modes
**** Saving to Persistent Tables
**** Bucketing, Sorting and Partitioning

*** Parquet Files
**** Loading Data Programmatically
**** Partition Discovery
**** Schema Merging
**** Hive metastore Parquet table conversion
***** Hive/Parquet Schema Reconciliation
***** Metadata Refreshing

**** Configuration

*** ORC Files
*** JSON Datasets
*** Hive Tables
**** Specifying storage format for Hive tables
**** Interacting with Different Versions of Hive Metastore

*** JDBC To Other Databases
*** Troubleshooting

** Performance Tuning
*** Caching Data In Memory
*** Other Configuration Options
*** Broadcast Hint for SQL Queries

** Distributed SQL Engine
*** Running the Thrift JDBC/ODBC server
*** Running the Spark SQL CLI

** PySpark Usage Guide for Pandas with Apache Arrow
*** Apache Arrow in Spark
**** Ensure PyArrow Installed

*** Enabling for Conversion to/from Pandas
*** Pandas UDFs (a.k.a. Vectorized UDFs)
**** Scalar
**** Grouped Map

*** Usage Notes
**** Supported SQL Types
**** Setting Arrow Batch Size
**** Timestamp with Time Zone Semantics

** Migration Guide
*** Upgrading From Spark SQL 2.3.0 to 2.3.1 and above
*** Upgrading From Spark SQL 2.2 to 2.3
*** Upgrading From Spark SQL 2.1 to 2.2
*** Upgrading From Spark SQL 2.0 to 2.1
*** Upgrading From Spark SQL 1.6 to 2.0
*** Upgrading From Spark SQL 1.5 to 1.6
*** Upgrading From Spark SQL 1.4 to 1.5
*** Upgrading from Spark SQL 1.3 to 1.4
**** DataFrame data reader/writer interface
**** DataFrame.groupBy retains grouping columns
**** Behavior change on DataFrame.withColumn

*** Upgrading from Spark SQL 1.0-1.2 to 1.3
**** Rename of SchemaRDD to DataFrame
**** Unification of the Java and Scala APIs
**** Isolation of Implicit Conversions and Removal of dsl Package (Scala-only)
**** Removal of the type aliases in org.apache.spark.sql for DataType (Scala-only)
**** UDF Registration Moved to sqlContext.udf (Java & Scala)
**** Python DataTypes No Longer Singletons

*** Compatibility with Apache Hive
**** Deploying in Existing Hive Warehouses
**** Supported Hive Features
**** Unsupported Hive Functionality
**** Incompatible Hive UDF

** Reference
*** Data Types
*** NaN Semantics

* TODO Structured Streaming
** Overview
** Quick Example
** Programming Model
*** Basic Concepts
*** Handling Event-time and Late Data
*** Fault Tolerance Semantics

** API using Datasets and DataFrames
*** Creating streaming DataFrames and streaming Datasets
**** Input Sources
**** Schema inference and partition of streaming DataFrames/Datasets

*** Operations on streaming DataFrames/Datasets
**** Basic Operations - Selection, Projection, Aggregation
**** Window Operations on Event Time
***** Handling Late Data and Watermarking

**** Join Operations
***** Stream-static Joins
***** Stream-stream Joins
****** Inner Joins with optional Watermarking
****** Outer Joins with Watermarking
****** Support matrix for joins in streaming queries

**** Streaming Deduplication
**** Arbitrary Stateful Operations
**** Unsupported Operations

*** Starting Streaming Queries
**** Output Modes
**** Output Sinks
***** Using Foreach

**** Triggers

*** Managing Streaming Queries
*** Monitoring Streaming Queries
**** Reading Metrics Interactively
**** Reporting Metrics programmatically using Asynchronous APIs
**** Reporting Metrics using Dropwizard

*** Recovering from Failures with Checkpointing

** Continuous Processing
** Additional Information

* TODO Spark Streaming (DStreams)
** Overview
** A Quick Example
** Basic Concepts
*** Linking
*** Initializing StreamingContext
*** Discretized Streams (DStreams)
*** Input DStreams and Receivers
*** Transformations on DStreams
*** Output Operations on DStreams
*** DataFrame and SQL Operations
*** MLlib Operations
*** Caching / Persistence
*** Checkpointing
*** Accumulators, Broadcast Variables, and Checkpoints
*** Deploying Applications
*** Monitoring Applications

** Performance Tuning
*** Reducing the Batch Processing Times
*** Setting the Right Batch Interval
*** Memory Tuning

** Fault-tolerance Semantics
** Where to Go from Here

* MLlib (Machine Learning)
** MLlib: Main Guide
*** Basic statistics
*** Pipelines
*** Extracting, transforming and selecting features
*** Classification and Regression
*** Clustering
*** Collaborative filtering
*** Frequent Pattern Mining
*** Model selection and tuning
*** Advanced topics

** MLlib: RDD-based API Guide
*** Data types
*** Basic statistics
*** Classification and regression
*** Collaborative filtering
*** Clustering
*** Dimensionality reduction
*** Feature extraction and transformation
*** Frequent pattern mining
*** Evaluation metrics
*** PMML model export
*** Optimization (developer)

* GraphX (Graph Processing)
** Overview
** Getting Started
** The Property Graph
*** Example Property Graph

** Graph Operators
*** Summary List of Operators
*** Property Operators
*** Structural Operators
*** Join Operators
*** Neighborhood Aggregation
**** Aggregate Messages (aggregateMessages)
**** Map Reduce Triplets Transition Guide (Legacy)
**** Computing Degree Information
**** Collecting Neighbors

*** Caching and Uncaching

** Pregel API
** Graph Builders
** Vertex and Edge RDDs
*** VertexRDDs
*** EdgeRDDs

** Optimized Representation
** Graph Algorithms
*** PageRank
*** Connected Components
*** Triangle Counting

** Examples

* SparkR (R on Spark)
** Overview
** SparkDataFrame
*** Starting Up: SparkSession
*** Starting Up from RStudio
*** Creating SparkDataFrames
**** From local data frames
**** From Data Sources
**** From Hive tables

*** SparkDataFrame Operations
**** Selecting rows, columns
**** Grouping, Aggregation
**** Operating on Columns
**** Applying User-Defined Function
***** Run a given function on a large dataset using dapply or dapplyCollect
****** dapply
****** dapplyCollect

***** Run a given function on a large dataset grouping by input column(s) and using gapply or gapplyCollect
****** gapply
****** gapplyCollect

***** Run local R functions distributed using spark.lapply
****** spark.lapply

*** Running SQL Queries from SparkR

** Machine Learning
*** Algorithms
**** Classification
**** Regression
**** Tree
**** Clustering
**** Collaborative Filtering
**** Frequent Pattern Mining
**** Statistics

*** Model persistence

** Data type mapping between R and Spark
** Structured Streaming
** R Function Name Conflicts
** Migration Guide
*** Upgrading From SparkR 1.5.x to 1.6.x
*** Upgrading From SparkR 1.6.x to 2.0
*** Upgrading to SparkR 2.1.0
*** Upgrading to SparkR 2.2.0
*** Upgrading to SparkR 2.3.0
*** Upgrading to SparkR 2.3.1 and above
