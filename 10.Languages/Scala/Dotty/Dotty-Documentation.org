#+TITLE: Dotty Documentation
#+VERSION: 3.0.0-M1-bin-20201016-e56ecae-NIGHTLY ---> 3.0.0-M2-bin-20201106-312a420-NIGHTLY
#+AUTHORS: Dotty Contributors
#+STARTUP: entitiespretty, fold

* BLOG
* DONE USAGE
  CLOSED: [2019-11-04 Mon 16:17]
** Getting Started
   # *Getting Started: Users*
   
*** Trying out Dotty
**** In your web browser
     The online Scala playground [[https://scastie.scala-lang.org][Scastie]]
     - an easy way to try Dotty without installing anything;
     - =from Jian= a project of _Scala Center_.

**** sbt
     Use sbt (1.1.4+)
     - Dotty project:
       ~sbt new lampepfl/dotty.g8~

     - Dotty project that _cross compiles_ with Scala 2:
       ~sbt new lampepfl/dotty-cross.g8~

**** IDE support
     Check the *IDE support for Dotty* section.

**** Standalone installation
     - Releases include _THREE_ executables:
       #+begin_src text
         .
         |_ bin
             |_ scalac
             |_ scalad
             |_ scala
       #+end_src
       + scalac is the Dotty compiler
       + scalad is the Dotty Documentation tool
       + scala is the Dotty REPL

** sbt-projects
   # *Using Dotty with sbt*
   A redirect link to the sbt subsection in the -Getting Started- section.

** TODO IDE support for Dotty
   Dotty comes *built-in* with the /Dotty Language Server/, an implementation of
   the /Language Server Protocol/, which means that any editor that implements the
   LSP can be used as a Dotty IDE.
   - Currently, the only IDE we officially support is Visual Studio Code.

*** Prerequisites
    Follow the instructions for the [[https://github.com/lampefl/dotty-example-project][dotty-example-project]].
    
*** Usage
    Install _Visual Studio Code_, and run command ~sbt launchIDE~ in a Dotty
    project.

*** TODO Status
**** Fully supported features
**** Partial working features
**** Unimplemented features
**** Current limitations, to be fixed

*** Feedback

** TODO Worksheet mode in Dotty IDE
   Worksheet use the extension .sc
   
*** How to use the worksheets
*** TODO Implementation details
    More details in [[https://dotty.epfl.ch/docs/usage/worksheet-mode-implementation-details.html][Worksheet mode - Implementation details]]

** DONE Language Versions
   CLOSED: [2020-11-06 Fri 11:31]
   - The default Scala language version currently supported by the Dotty compiler
     is 3.0.
     There are also other language versions that can be specified instead:
     + 3.1 ::
       A preview of changes introduced in the next version after 3.0.
       _Some Scala-2 specific idioms will be *dropped* in this version._
       The feature set supported by this version will be refined over time as we
       approach its release.

     + 3.0-migration ::
       Same as 3.0 but with a Scala 2 compatibility mode that helps moving
       Scala 2.13 sources over to Scala 3. In particular, it
       * *flags* some Scala 2 constructs that are _DISALLOWED in Scala 3_ as
         _migration warnings_ instead of hard errors,

       * *changes* some rules to be _MORE LENIENT and BACKWARDS COMPATIBLE with
         Scala 2.13_

       * gives some _additional warnings_ where the semantics has changed between
         Scala 2.13 and 3.0

       * in conjunction with ~-rewrite~, offer code rewrites _from Scala 2.13 to 3.0_.

     + 3.1-migration ::
       Same as 3.1 but with additional helpers to migrate from 3.0.
       * Similarly to the helpers available under 3.0-migration, these include
         _migration warnings_ and _optional rewrites_.
   
   - There are _TWO ways_ to *specify* a _language version_.
     + With a ~-source~ command line setting, e.g.
       #+begin_src bash
         scalac -source 3.0-migration ........
       #+end_src

     + With a ~scala.language~ _import_ at the top of a compilation unit, e.g:
       #+begin_src scala
         package p

         import scala.language.`3.1`

         class C {
           ...
         }
       #+end_src

   - _Language imports_ *supersede* _command-line settings_ in the /compilation
     units/ where they are specified.
     + *Only one* _language import_ is allowed in a /compilation unit/, and it
       MUST come before any definitions in that unit.
   
** DONE cbt-projects
   CLOSED: [2020-11-06 Fri 11:32]
   # *Using Dotty with cbt*
   =from Jian=: We can ignore this section

** TODO Dottydoc
*** Using existing Templates and Layouts
*** Blog
*** Includes
*** Sidebar
*** Dottydoc Specific Tags and Behavior
**** Linking to API
**** Rendering Docstrings
**** Other extensions

*** Default Layouts
**** =main.html=
***** Variables

**** =sidebar.html=
***** Variables

**** =doc-page.html=
**** =api-page.html=
**** =blog-page.html=

*** Default Includes

* TODO REFERENCE
** Overview
*** Goals
    - The language REDESIGN was guided by _THREE_ main goals:
      * Strengthen Scala's foundations.
        Make the full programming language compatible with the foundational work
        on the /DOT calculus/ and apply the lessons learned from that work.

      * Make Scala *easier* and *safer* to use.
        + Tame powerful constructs such as /implicits/ to provide a gentler
          learning curve.

        + Remove warts and puzzlers.

      * Further IMPROVE the *consistency* and *expressiveness* of Scala's language
        constructs.

    - Corresponding to these goals, the language changes fall into _SEVEN_
      categories: =TODO= =TODO= =TODO=
      1. Core constructs to strengthen foundations,

      2. simplifications and

      3. restrictions, to make the language easier and safer to use,

      4. dropped constructs to make the language smaller and more regular,

      5. changed constructs to remove warts, and increase consistency and usability,

      6. new constructs to fill gaps and increase expressiveness,

      7. a new, principled approach to metaprogramming that replaces today's
         experimental macros.
    
*** TODO Essential Foundations - =RE-READ= =TODO=
    These new constructs directly model core features of /DOT/, /higher-kinded
    types/, and the /SI calculus for implicit resolution/.
    - /Intersection types/, replacing /compound types/,

    - /Union types/,

    - /Type lambdas/, replacing encodings using /structural types/ and
      /type projection/.

    - /Context Functions/, offering abstraction over /given parameters/.
    
*** TODO Simplifications - =RE-READ= =TODO=
    - These constructs replace existing constructs with the aim of making the
      language safer and simpler to use, and to promote uniformity in code style.
      * _Trait Parameters_ replace _early initializers_ with a more generally
        useful construct.

      * _Given Instances_ replace _implicit objects and defs_,
        focussing on intent over mechanism.

      * _Using Clauses_ replace _implicit parameters_,
        avoiding their ambiguities.

      * _Extension Methods_ replace _implicit classes_ with a clearer and simpler
        mechanism.

      * _Opaque Type Aliases_ replace most uses of _value classes_
        while guaranteeing absence of /boxing/.

      * _Toplevel definitions_ replace _package objects_,
        dropping syntactic boilerplate.

      * _Export clauses_ provide a simple and general way to express aggregation,
        which can replace the previous facade pattern of package objects
        inheriting from classes.

      * _Vararg patterns_ now use the form ~: _*~ instead of ~@ _*~,
        mirroring /vararg expressions/,

      * _Creator applications_ allow using _simple function call syntax_ instead
        of ~new~ expressions.
        + ~new~ expressions stay around as a fallback for the cases
          where creator applications cannot be used. =TODO= =???= =TODO=
    
    - With the EXCEPTION of /early initializers/ and /old-style vararg patterns/,
      all superseded constructs *continue to be available in Scala 3.0.*
      The plan is to deprecate and phase them out later.

    - _Value classes_ (superseded by /opaque type aliases/) are a special case.
      There are currently no deprecation plans for /value classes/, since we
      might want to bring them back in a more general form if they are supported
      natively by the JVM as is planned by project Valhalla.
      =TODO= =TODO= =TODO= =FUTURE=
      
*** DONE Restrictions
    CLOSED: [2020-11-07 Sat 00:18]
    - These constructs are restricted to make the language safer.
      * _Implicit Conversions_:
        there is only one way to define implicit conversions instead of many,
        and potentially surprising /implicit conversions/ require a language
        import.

      * _Given Imports_:
        implicits now require a special form of import, to make the import
        clearly visible.

      * _Type Projection_: =TODO= =LEARN MORE=
        only /classes/ can be used as prefix ~C~ of a /type projection/ ~C#A~.
        /Type projection/ on /abstract types/ is *no longer supported* since it
        is unsound.

      * _Multiversal Equality_ implements an "opt-in" scheme to rule out nonsensical
        comparisons with ~==~ and ~!=~.

      * ~@infix~ and ~@alpha~ make method application syntax uniform across code bases
        and require /alphanumeric aliases/ for /all symbolic names/ (proposed, not
        implemented).

    - UNRESTRICTED /implicit conversions/ continue to be available in Scala 3.0,
      _BUT_ will *be deprecated and removed later*.
        Unrestricted versions of the other constructs in the list above are
      available only under ~-source 3.0-migration~.
      
*** TODO Dropped Constructs
*** TODO Changes
*** TODO New Constructs
*** TODO Metaprogramming
*** TODO See Also

** DONE NEW TYPES
   CLOSED: [2020-03-08 Sun 21:34]
*** DONE Intersection types
    CLOSED: [2019-11-10 Sun 17:47]
    The ~&~ operator creates an /intersection type/.

**** Type Checking
     The type ~S & T~ represents values that are of the type ~S~ and ~T~ _at the
     same time_.

     - Example:
       #+begin_src scala
         trait Resettable {
           def reset(): Unit
         }

         trait Growable[T] {
           def add(x: T): this.type
         }

         def f(x: Resettable & Growable[String]) = {
           x.reset()
           x.add("first")
         }
       #+end_src

     - If a /member/ appears in both ~A~ and ~B~, its type in ~A & B~ is the
       /intersection of its type/ in ~A~ and its type in ~B~.
         For instance, assume the definitions:
       #+begin_src scala
         trait A {
           def children: List[A]
         }

         trait B {
           def children: List[B]
         }

         val x: A & B = new C
         val ys: List[A & B] = x.children
       #+end_src
       ~ys~ is of type ~List[A] & List[B]~, _which can be FURTHER SIMPLIFIED
       to_ ~List[A & B]~ _because_ ~List~ is /convariant/.

     - Q :: (One might wonder)
            How the compiler could come up with a definition for ~children~ of
            type ~List[A & B]~ since all its is given are ~children~ definitions
            of type ~List[A]~ and ~List[B]~.

     - A :: The answer is it *does not need to*. TODO ??? ??? ??? TODO
              ~A & B~ is just a type that represents a set of requirements for
            values of the type.
              At the point where a value is constructed, one must make sure that
            all inherited members are correctly defined. So if one _defines a class
             ~C~ that inherits ~A~ and ~B~,_ one needs to give at that point a
            definition of a ~children~ method with the required type.
       #+begin_src scala
         class C extends A with B {
           def children: List[A & B] = ???
         }
       #+end_src

**** More Details
***** Syntax
      Syntactically, an /intersection type/ ~S & T~ is similar to an /infix
      type/, where the _infix operator_ is ~&~.
      - ~&~ is treated as a /soft keyword/.
        + it is a _NORMAL identifier_ with the usual precedence.

        + *BUT*
          a type of the form ~A & B~
          _is *ALWAYS* recognized as_ an /intersection type/,
          _WITHOUT_ trying to resolve ~&~.

      - Syntax:
        #+begin_src text
          Type      ::=  ...| InfixType
          InfixType ::=  RefinedType {id [nl] RefinedType}
        #+end_src

***** Subtyping Rules
      - Subtyping rules
        TODO

      - It is can be proved that ~&~ is *commutative*.

      - Derived:
        Given type constructor ~C~,
        + If ~C~ is /covariant/, ~C[A] & C[B] ~> C[A & B]~
        + If ~C~ is /contravariant/, ~C[A] & C[B] ~> C[A | B]~

***** TODO Erasure
      TODO TODO TODO

***** Relationship with Compound Type (~with~)
      - =from Jian=
        ~A & B~ is different from the ~A with B~ in Scala 2.
        The latter is not commutative!

      - /Intersection types/ ~A & B~ *replace* /compound types/ ~A with B~ in
        Scala 2.
          For the moment, the syntax ~A with B~ is _still allowed_ and
        *interpreted as* ~A & B~, _but its usage as a type (as opposed to in a
        ~new~ or ~extends~ clause) will be *deprecated* and *removed* in the future._

*** DONE Union types
    CLOSED: [2019-07-01 Mon 15:49]
    A ~A | B~ value can be _any value_ of type ~A~ _and_ also _any value_ of
    type ~B~.

    - Example:
      #+begin_src scala
        final case class UserName(name: String)
        final case class Password(hash: Hash)

        def help(id: UserName | Password) = {
          val user = id match {
            case UserName(name) => lookupName(name)
            case Password(hash) => lookupPassword(hash)
          }
          // ...
        }
      #+end_src

    - /Union types/ are _DUALS of /intersection types/.

    - ~|~ is *commutative*: ~A | B~ is the _SAME type_ as ~B | A~.

    - The compiler will assign a /union type/ to an expression *only if such a
      type is _EXPLICITLY given_.*
      #+begin_src scala
        val password = Password(123)
        // val password: Password = Password(123)

        val name = UserName("Eve")
        // val name: UserName = UserName(Eve)

        if (true) name else password
        // val res2: Object & Product = UserName(Eve)

        val either: Password | UserName = if (true) name else password
          // val res2: Password | UserName = UserName(Eve)
      #+end_src
      + ~Object & Product~ is a /supertype/ of ~UserName~ and ~Password~,
        BUT NOT the /least supertype/ ~Password | UserName~
        * =from Jian= In the document, there is is a typo (not wrong, but not very
          meaningful): _Object & Product is a supertype of UserName and ~Product~._
          TODO Create a PR to correct this!

**** TODO More Details
***** Syntax
      Syntactically, /union types/ follow the same rules as /intersection types/,
      BUT have a _LOWER precedence_.

****** Intersection with pattern matching syntax - =IMPORTANT=
       ~|~ is also used in /pattern matching/ to _SEPARATE_ /pattern alternatives/ and
       *has _LOWER PRECEDENCE than_ ~:~ as used in /typed patterns/,* this means that:
       #+begin_src scala
         case _: A | B => ...

         // is still equivalent to:
         case (_: A) | B => ...

         // and NOT to:
         case _: (A | B) => ...
       #+end_src

***** Subtyping Rules
      - ~A~ is always a subtype of ~A | B~ for all ~A~, ~B~.

      - If ~A <: C~ and ~B <: C~ then ~A | B <: C~.

      - Like ~&~, ~|~ is /commutative/ and /associative/:
        #+begin_src text
          A | B       =:= B | A
          A | (B | C) =:= (A | B) | C
        #+end_src

      - ~&~ _is distributive over ~|~:_
        #+begin_src text
          A & (B | C) =:= A & B | A & C
        #+end_src

      - From these rules it follows that: TODO TODO TODO
        *the /least upper bound (lub)/ of a set of type is the union of these
        types.*

        + This *replaces* the definition of /least upper bound/ in the Scala 2
          specification. TODO

***** TODO Motivation - TODO NOTE, TODO Re-READ
***** TODO Join of a union type - TODO ???
****** Example

***** TODO Type inference
****** Example

***** TODO Members
****** Example

***** Exhaustivity checking
***** TODO Erasure

*** DONE Type lambdas
    CLOSED: [2019-07-01 Mon 15:55]
    A /type lambda/ lets one express a /higher-kinded type/ directly, *WITHOUT*
    a /type definition/.

    - =from Jian=
      Scala 2 can do this with /type definition/ and /type projection/.

    - Example:
      ~[+X, Y] =>> Map[Y, X]~

    - /Type parameters/ of /type lambdas/ can have /variances/ and /bounds/.

    - A /parameterized type definition or declaration/ such as ~type T[X] = (X, X)~
      is a shorthand for a PLAIN /type definition/ with a /type lambda/ as its RHS:
      ~type T = [X] =>> (X, X)~

    - TODO
      _More details_ link

*** DONE Match types - TODO _mechanism_
    CLOSED: [2020-03-08 Sun 21:34]
    - A /match type/ reduces to one of a number of right hand sides, depending on
      a /scrutinee type/. Example:
      #+begin_src scala
        type Elem[X] = X match {
          case String      => Char
          case Array[t]    => t
          case Iterable[t] => t
        }
      #+end_src
      + An ~Elem~ with /CONCRETE type parameter/ ~X~ can be reduced _as_ (NOT legal
        code you want to write out explicitly):
        #+begin_src scala
          Elem[String]      =:= Char
          Elem[Array[Int]]  =:= Int
          Elem[List[Float]] =:= Float
          Elem[Nil.type]    =:= Nothing
        #+end_src
        Here ~=:=~ is understood to mean that left and right hand sides are
        *mutually subtypes* of each other.

    - Syntax in general: ~S match { P1 => T1 .... Pn => Tn }~, where
      + ~S~, ~T1~, ..., ~Tn~ are types.
      + ~P1~, ..., ~Pn~ are patterns.
        * /Type variables/ in patterns start as usual with a lower case letter.

    - Match types can form part of *RECURSIVE TYPE definitions*. Example:
      #+begin_src scala
        type LeafElem[X] = X match {
          case String      => Char
          case Array[t]    => LeafElem[t]
          case Iterable[t] => LeafElem[t]
          case AnyVal      => X
        }
      #+end_src

    - _Recursive match type definitions_ can also be given an /upper bound/, like this:
      #+begin_src scala
        type Concat[+Xs <: Tuple, +Ys <: Tuple] <: Tuple = Xs match {
          case Unit    => Ys
          case x *: xs => x *: Concat[xs, Ys]
        }
      #+end_src
      + In this definition, every instance of ~Concat[A, B]~, whether reducible
        or not, is known to be a /subtype/ of ~Tuple~.

      + This is necessary to _make the recursive invocation ~x *: Concat[xs, Ys]~
        type check_, since ~*:~ demands a ~Tuple~ as its right operand.

**** DONE Representation of Match Types
     CLOSED: [2020-03-08 Sun 21:32]
     # =from Jian= Internal Representation of Match Types
     #+begin_src scala
       S match {
         case P1 => T1
         case P2 => T2
         // ...
         case Pn => Tn
       }
     #+end_src
     - It's _internal representation_ (=from Jian= Tasty???) is
       ~Match(S, C1, ..., Cn) <: B~
       + ~Ci~ is of the form ~[Xs] => P => T~
         * ~[Xs]~
           a /type parameter clause/ of the /variables bound/ in pattern ~Pi~.
           _It can be omitted if there is *NO* /bound/._

         * Each case (~Pi => Ti~) is either:
           - a /unary function type/ like ~String => Char~
             OR
           - a /type lambda over a unary function type/ like ~Array[t] => LeafElem[t]~.

         * ~B~ is the declared /upper bound/ of the /match type/, or ~Any~ if no
           such bound is given.

       + Scrutiny, /bound types/ and /pattern types/ must be /first-order types/.
         TODO =from Jian= ??? I don't quite understand this sentence!?!?

**** TODO Match type reduction
**** TODO Subtyping Rules for Match Types
**** TODO Variance Laws for Match Types
**** TODO Typing Rules for Match Expressions
**** TODO Overlapping Patterns
**** TODO Handling Termination
**** TODO Related Work

*** DONE Dependent Function Types
    CLOSED: [2019-07-01 Mon 16:10]
    - A /dependent function type/ describes functions where the _result type_ may
      DEPEND ON the _function's parameter values_. Example:
      #+begin_src scala
        trait Entry {
          type Key
          val key: Key
        }

        def extractKey(e: Entry): e.Key = e.key          // a dependent method
        val extractor: (e: Entry) => e.Key = extractKey  // a dependent function value
        //           ||                   ||
        //           ||     Dependent     ||
        //           ||   Function Type   ||
        //           =======================
      #+end_src

      - Scala _ALREADY_ has /dependent methods/.
        BUT so far (in Scala 2) it was _NOT possible_ to turn such /methods/ into
        /function values/, so that they can be passed as /parameters/ to other
        functions, or returned as results.
        + /Dependent methods/ COULD NOT be turned into /functions/ simply because
          there was no type that could describe them.

      - In dotty the /type/ of the ~extractor~ value above is ~(e: Entry) => e.Key~

    - The /dependent function type/ above is just /syntactic sugar/ for
      #+begin_src scala
        Function1[Entry, Entry#Key] {
          def apply(e: Entry): e.Key
        }
      #+end_src

**** More details

** DONE ENUMS
   CLOSED: [2020-07-11 Sat 04:07]
*** DONE Enumerations
    CLOSED: [2020-07-10 Fri 23:57]
    An /enumeration/ is used to define a /type/ consisting of _a set of NAMED values._

    - Example:
      #+begin_src scala
        enum Color {
          case Red, Green, Blue
        }
      #+end_src
      Desugare to core Scala features are explained in the section _Translation_.
      + This defined a new ~sealed~ /class/ ~Color~ with 3 values:
        * ~Color.Red~
        * ~Color.Green~
        * ~Color.Blue~

      + The _color values_ are members of ~Color~'s /companion object/.

**** DONE Parameterized enums
     CLOSED: [2020-07-10 Fri 19:08]
     /Enums/ CAN BE _parameterized_:
     #+begin_src scala
       enum Color(val rgb: Int) {
         case Red   extends Color(0xFF0000)
         case Green extends Color(0x00FF00)
         case Blue  extends Color(0x0000FF)
       }
     #+end_src
     As the example shows, you can _DEFINE_ the parameter value BY using an
     _EXPLICIT_ ~extends~ /clause/.

**** DONE Methods defined for enums
     CLOSED: [2020-07-10 Fri 19:15]
     - The values of an /enum/ correspond to _UNIQUE integers_.
       The _integer_ associated with an /enum value/ is returned by its ~ordinal~
       /method/.

     - Example:
       #+begin_src scala
         val red = Color.Red
         // val red: Color = Red

         red.ordinal
         // val res0: Int = 0
       #+end_src

     - The /companion object/ of an /enum/ also defines *TWO* utility /methods/.
       + ~valueOf~: obtain an /enum value/ by its _name_:
         ~Color.valueOf("Blue")  // val res0: Color = Blue~

       + ~values~: returns _ALL_ /enum values/ defined in an enumeration in an
         ~Array~:
         ~Color.values  // val res1: Array[Color] = Array(Red, Green, Blue)~

**** DONE User-defined members of enums
     CLOSED: [2020-07-10 Fri 19:23]
     It is _possible_ to add your own definitions to an /enum/.

     - Example:
       #+begin_src scala
         enum Planet(mass: Double, radius: Double) {
           private final val G = 6.67300E-11
           def surfaceGravity = G * mass / (radius * radius)
           def surfaceWeight(otherMass: Double) =  otherMass * surfaceGravity

           case Mercury extends Planet(3.303e+23, 2.4397e6)
           case Venus   extends Planet(4.869e+24, 6.0518e6)
           case Earth   extends Planet(5.976e+24, 6.37814e6)
           case Mars    extends Planet(6.421e+23, 3.3972e6)
           case Jupiter extends Planet(1.9e+27,   7.1492e7)
           case Saturn  extends Planet(5.688e+26, 6.0268e7)
           case Uranus  extends Planet(8.686e+25, 2.5559e7)
           case Neptune extends Planet(1.024e+26, 2.4746e7)
         }
       #+end_src

     - It is also possible to define an *EXPLICIT* /companion object/ for an /enum/:
       #+begin_src scala
         object Planet {
           def main(args: Array[String]) = {
             val earthWeight = args(0).toDouble
             val mass = earthWeight / Earth.surfaceGravity
             for (p <- values)
               println(s"Your weight on $p is ${p.surfaceWeight(mass)}")
           }
         }
       #+end_src
       + =from Jian= ???
         Before compiling, will ~case~'s be merged into the generated /companion
         object/???

**** DONE Compatibility with Java Enums
     CLOSED: [2020-08-22 Sat 22:10]
     If you want to use the Scala-defined enums as Java enums, you can do so by
     extending ~java.lang.Enum~ class as follows:
     - Example
       #+begin_src scala
         enum Color extends java.lang.Enum[Color] { case Red, Green, Blue }

         // Use `Color` as you would use a Java enum:
         Color.Red.compareTo(Color.Green)
         // val res15: Int = -1
       #+end_src
       + There is _NO need to provide_ /constructor arguments/ (as defined in the
         Java API docs) to ~java.lang.Enum~ when extending it â€“ _the compiler will
         GENERATE them AUTOMATICALLY._

     - For a more in-depth example of using Scala 3 /enums/ from Java, see
       [[https://github.com/lampepfl/dotty/tree/master/tests/run/enum-java][this test]]. In this test, the /enums/ are defined in the ~MainScala.scala~
       file and used from a Java source, ~Test.java~.

**** DONE Implementation
     CLOSED: [2020-10-06 Tue 21:32]
     /Enums/ are represented as ~sealed~ /abstract classes/ that extend the
     ~scala.reflect.Enum~ /trait/.

     - ~scala.reflect.Enum~ defines a _SINGLE_ /public method/ ~ordinal~:
       #+begin_src scala
         package scala.reflect

         /** A base trait of all Scala enum definitions */
         super trait Enum extends Any with Product with Serializable {

           /** A number uniquely identifying a case of an enum */
           def ordinal: Int
         }
       #+end_src

     - /Enum values/ *WITH* ~extends~ /clauses/ get *expanded* to /anonymous class
       instances/.
         For instance, the ~Venus~ value above (=from Jian= in Section _User-defined
       members of enums_) would be defined like this:
       #+begin_src scala
         val Venus: Planet =
           new Planet(4.869e24, 6.0518e6) {
             def ordinal: Int = 1
             override def productPrefix: String = "Venus"
             override def toString: String = "Venus"
           }
       #+end_src

     - /Enum values/ *WITHOUT* ~extends~ /clauses/ all share a single implementation
       that can be instantiated using a /private method/ that takes _a tag (=from Jian=
       /ordinal/???)_ and _a name_ as /arguments/.
         For instance, ~Color.Red~ would expand to
         #+begin_src scala
           val Red: Color = $new(0, "Red")
         #+end_src

**** TODO Reference
     For more info, see [[https://github.com/lampepfl/dotty/issues/1970][Issue #1970]] and [[https://github.com/lampepfl/dotty/pull/4003][PR #4003]].

*** DONE Algebraic Data Types
    CLOSED: [2020-07-11 Sat 01:28]
    The ~enum~ concept is general enough to ALSO support ADTs and GADTs.

    - Example:
      #+begin_src scala
        enum Option[+T] {
          case Some(x: T)
          case None
        }
      #+end_src
      + ~case Some~ is a shorthand for writing a /case class/ that _extends_
        ~Option~.

      + ~None~ is NOT parameterized, it is treated as a _normal_ enum value.

      + The ~extends~ clauses can be given explicitly:
        #+begin_src scala
          enum Option[+T] {
            case Some(x: T) extends Option[T]
            case None       extends Option[Nothing]
          }
        #+end_src

      + Note:
        The /parent type/ of the ~None~ value is inferred as ~Option[Nothing]~.
        Generally,
        * all /covariant/ /type parameters/ of the /enum class/ are *minimized* in
          a compiler-generated ~extends~ clause

        * whereas all /contravariant/ /type parameters/ are *maximized*.

        * If ~Option~ was /non-variant/, you would need to give the ~extends~
          /clause/ of ~None~ *EXPLICITLY*.

    - If not directly ~new~ a enumeration, the /type/ is always its parent.
      For example,
      + ~Option.Some(2)~ is of /type/ ~Option[Int]~
      + ~Option.None~ is of /type/ ~Option[Nothing]~
      + ~new Option.Some(2)~ is of /type/ ~Option.Some[Int]~

    - As all other enums, ADTs can define methods.
      #+begin_src scala
        enum Option[+T] {
          case Some(x: T)
          case None

          def isDefined: Boolean = this match {
            case None => false
            case some => true
          }
        }

        object Option {
          def apply[T >: Null](x: T): Option[T] =
            if (x == null) None else Some(x)
        }
      #+end_src

    - /Enumerations/ and /ADTs/ have been presented as two *DIFFERENT concepts*.
      _BUT_ since they _share the SAME /syntactic construct/,_
      1. they can be seen simply as two ends of a spectrum
         AND
      2. it is perfectly possible to construct *hybrids*.

    - For instance, the code below gives an implementation of ~Color~ either with
      three /enum values/ or with a /parameterized case/ that takes an RGB value.
      #+begin_src scala
        enum Color(val rgb: Int) {
          case Red           extends Color(0xFF0000)
          case Green         extends Color(0x00FF00)
          case Blue          extends Color(0x0000FF)
          case Mix(mix: int) extends Color(mix)
        }
      #+end_src


**** TODO Parameter Variance of Enums
     =New added in 2020-09-18=

**** DONE Syntax of Enums
     CLOSED: [2019-07-02 Tue 13:27]
     - TODO NOTE

**** TODO Reference
     For more info, see [[https://github.com/lampepfl/dotty/issues/1970][Issue #1970]].

*** DONE Translation
    CLOSED: [2020-07-11 Sat 04:07]
    # *Translation of Enum and ADTs*
    - In Scala 3, /enums/ are CONVENIENT /syntactic sugar/,
      BUT they are *NOT* essential to understand _Scala's core_.

    - We now explain the *expansion of enums* _in detail_.
      Here are some _terminology_ and _notational conventions_:
      + ~E~ as a NAME of an /enum/,
        ~C~ as a NAME of a /case/ that appears in ~E~.

      + We use ~<...>~ for /syntactic constructs/ that in some circumstances *might
        be empty*.
        * For instance, ~<value-params>~ represents one or more parameter lists
          ~(...)~ or nothing at all.

      + Enum cases fall into _THREE_ categories:
        * /Class cases/ are those /cases/ that are *parameterized*,
          - _EITHER_ with a /type parameter/ section ~[...]~
          - _OR_ with _one or more (possibly empty)_ /parameter sections/ ~(...)~.

        * /Simple cases/ are /cases/ of a *non-generic* /enum/ that have
          *NEITHER /parameters/ NOR an /extends clause/ or /body/.*
          That is, _they consist of a NAME only._

        * /Value cases/ are /cases/ that
          - do *NOT HAVE* a /parameter section/

          - BUT that do *HAVE* a (possibly generated) /extends clause/ and/or a
            /body/.

    - There are _NINE_ *desugaring rules*.
      + Overview:
        * Rule (1) DESUGAR /enum/ definitions.

        * Rules (2) and (3) DESUGAR /simple cases/.

        * Rules (4) to (6) define /extends clauses/ for /cases/ that are MISSING
          them.

        * Rules (7) to (9) define how such /cases/ with /extends clauses/ map into
          /case classes/ or /vals/.

      + Details
        1. An ~enum~ definition
           + ~enum E ... { <defs> <cases> }~ expands to
             (=from Jian=
             Here ~...~ can be anything between the _NAME_ of /enum/ and _BODY_
             of /enum/)
             * a ~sealed abstract class~ that extends the ~scala.Enum~ /trait/
               AND
             * an associated /companion object/ that CONTAINS the _defined cases_,
               expanded according to rules (2 - 8).

           + The /enum trait/
             * _starts with_ a compiler-generated import that imports the names ~<caseIds>~
               of all cases _so that they can be used WITHOUT prefix *IN* the trait._
               #+begin_src scala
                 sealed abstract class E ... extends <parents> with scala.Enum {
                   import E.{ <caseIds> }
                   <defs>
                 }

                 object E { <cases> }
               #+end_src

        2. A /simple case/ consisting of a comma-separated list of /enum NAMES/:
           ~case C_1, ..., C_n~ expands to ~case C_1; ...; case C_n~
           + Any /modifiers/ or /annotations/ on the ORIGINAL case _extend_ to ALL
             EXPANDED cases.

        3. For a /enum/ ~E~,
           its /simple case/ ~case C~ -----> ~val C = $new(n, "C")~.
           + Here, ~$new~ is a /private method/ that creates an instance of ~E~.

        4. For a /enum/ ~E[V1 T1 >: L1 <: U1, ..., Vn Tn >: Ln <: Un]~, where
           _n > 0_ and the /variances/ ~Vi~ is either ~+~ or ~\minus~,
           /simple case/ ~case C~ -----> ~case C extends E[B1, ..., Bn]~, where
           ~Bi~ is ~Li~ if ~Vi~ is ~+~ and ~Ui~ if ~Vi~ is ~\minus~.
           + =TODO= This result is then _further rewritten_ with *rule (8)*.

           + /Simple cases/ of /enums/ with /NON-VARIANT/ /type parameters/ are *not
             permitted* (however /value cases/ with *EXPLICIT* /extends clause/ are)
             * =from Jian=
               A /case/ with /NON-VARIANT/ /type parameters/ (compiler can't infer
               the /type parameters/ of EACH /cases/) *MUST* have an *EXPLICIT*
               /extends clause/ to specify the /type parameters/ of *EACH* /case/.

        5. For a ~enum E~,
           its /class case/ ~case C <type-params> <value-params>~  ----->
           ~case C <type-parmas> <value-parmas> extends E~.
           + This result is then further rewritten with *rule (9)*.

        6. For a ~enum E[Ts]~,
           its /class case/ with NEITHER /type parameters/ NOR an /extends clause/
           ~case C <value-params>~ -----> ~case C[Ts] <value-params> extends E[Ts]~.
           + This result is then _further rewritten_ with *rule (9)*.

           + For /class cases/ that have /type parameters/ themselves, an /extends
             clause/ needs to be GIVEN EXPLICITLY.

        7. For a ~enum E[Ts]~,
           its /class case/ ~case C <value-params> extends <parents>~ ----->
           ~case C[Ts] <value-parmas> extends <parents>~
           *provided* at least one of the /type parameters/ ~Ts~ is mentioned
           + in a /parameter type/ in ~<value-params>~
             OR
           + in a /type argument/ in ~<parents>~.

        8. For a ~enum E[Ts]~,
           it's /value case/ ~case C extends <parents>~ ----->
           ~val C = new <pareents> { <body>; def ordinal = n; $values.register(this) }~
           in ~E~'s /companion object/, and
           + ~n~ starting from ~0~.

           + The statement ~$values.register(this)~ registers the value as one of
             the ~values~ of the enumeration (see below).
               ~$values~ is a /compiler-defined _private_ value/ in the /companion
             object/.

           + The /anonymous class/ (the value referenced by ~C~) also implements the
             /abstract/ ~Product~ /methods/ that it inherits from ~Enum~.

           + It's an *error* =TODO= =???= =TODO=
             if a /value case/ referes to a /type parameter/ of the enclosing ~enum~
             in a /type argument/ of ~<parents>~.

        9. For ~enum E~,
           it's /class case/ ~case C <params> extends <parents>~ ----->
           ~final case class C <params> extends <parents>~ in ~E~'s /companion object/.
           + However, *unlike* for a REGULAR /case class/, the return type of the associated
             ~apply~ method is a /fully parameterized type instance/ of the /enum class/
             ~E~ itself instead of ~C~.

           + ~ordinal~ /method/ is defined as ~def ordinal = n~, where ~n~ the /ordinal
             number/ of the /case/ in the /companion object/, starting from ~0~.

           + It is an *error* =TODO= =???= =TODO=
             if a /value case/ refers to a /type parameter/ of the ENCLOSING ~enum~
             in a /parameter type/ in ~<params>~ or in a /type argument/ of ~<parents>~,
             unless that /parameter/ is already a /type parameter/ of the /case/,
             i.e. the parameter name is defined in ~<params>~.

**** DONE Translation of Enumerations - =TODO=
     CLOSED: [2020-07-11 Sat 03:47]
     - enumerations :: /non-generic enums/ that define one or more *singleton* cases.

     - /Companion objects/ of /enumerations/ define the following additional
       /synthetic members/.
       + A /method/ ~valueOf(name: String): E~.
         It returns the /singleton case value/ whose identifier is ~name~.

       + A /method/ ~values~ which returns an ~Array[E]~ of *ALL* /singleton case/
         values in ~E~, _in the *ORDER* of their definitions._

     - /Companion objects/ of /enumerations/ that contain _at least one_ /simple case/
       define in addtion:
       + A /private method/ ~$new~ which defines a new /simple case value/ with given
         /ordinal number/ and /name/.
         This /method/ can be thought as being defined as follows:
         #+begin_src scala
           private def $new(_$ordinal: Int, $name: String) = new E with runtime.EnumValue {
             def $ordinal = $_ordinal
             override def productPrefix = $name  // if not overridden in `E`
             override def toString = $name       // if not overridden in `E`
           }
         #+end_src

     - The /anonymous class/ also implements the /abstract/ ~Product~ /methods/
       that it _inherits_ from ~Enum~.
       + The ~ordinal~ /method/ above is used to generate the ~ordinal~ /method/
         if the /enum/ does NOT /extend/ a ~java.lang.Enum~ *(as /Scala enums/ do
         NOT /extend/ ~java.lang.Enums~ UNLESS explicitly specified)*.
         In case it does, there is no need to generate ~ordinal~ as ~java.lang.Enum~
         defines it.
           Similarly there is no need to override ~toString~ as that is defined in
         terms of ~name~ in ~java.lang.Enum~. Finally, ~productPrefix~ will call
         ~this.name~ when ~E~ extends ~java.lang.Enum~.
         *
         * =from Jian=
           This is the reason why NOT define ~ordinal~ directly instead.

**** DONE Scopes for Enum Cases
     CLOSED: [2020-07-11 Sat 03:32]
     - A /case/ in an /enum/ is treated similarly to a /secondary constructor/.
       It can access
       + *NEITHER* the enclosing ~enum~ using ~this~
       + *NOR* its /value parameters/ or /instance members/ using simple identifiers.

     - Even though translated /enum cases/ are located in the /enum's companion
       object/, referencing this /object/ or its /members/ via ~this~ or a simple
       identifier is also *ILLEGAL*.
       + The compiler typechecks /enum cases/ in the scope of the enclosing
         /companion object/ BUT flags any such *illegal* accesses as errors.

**** DONE Translation of Java-compatible enums
     CLOSED: [2020-07-11 Sat 04:06]
     - A /Java-compatible enum/ is _an /enum/ that extends ~java.lang.Enum~._
       The translation rules are the same as above, with the reservations
       defined in this section.

     - It is a /compile-time error/ for a /Java-compatible enum/ to have
       /class cases/.
       + =from Jian=
         This is the restriction from the design of /Java enum/.

     - /Cases/ such as ~case C~ expand to a ~@static val~ as opposed to a ~val~.
       This allows them to be generated as /static fields/ of the /enum type/,
       thus _ENSURING they are represented the same way as /Java enums/._

**** DONE Other Rules
     CLOSED: [2020-07-11 Sat 03:40]
     - A normal /case class/ which is *NOT produced* from an /enum case/ is *NOT
       allowed* to /extend/ ~scala.Enum~.
         This _ENSURES_ that the *ONLY* /cases/ of an /enum/ are the ones that are
       EXPLICITLY declared in it (=from Jian= -- make sure *sealed*).

     - If an /enum case/ has an /extends clause/, the /enum class/ *MUST* be one
       of the /classes/ that's extended.
       + =from Jian=
         For example,
         #+begin_src scala
           // Illegal
           enum E[T] {
             case E1[A, B] extends E[A] with F[B]
             case E2[B]    extends F[B]  // Illegal
           }

           // Legal
           enum E[T] {
             case E1[A, B] extends E[A] with F[B]
             case E2[A]    extends E[A]
           }
         #+end_src

** DONE CONTEXTUAL ABSTRACTIONS - =READING=
   CLOSED: [2020-07-19 Sun 03:32]
*** DONE Overview
    CLOSED: [2020-07-17 Fri 02:07]
**** DONE Critique of the Status Quo
     CLOSED: [2020-07-17 Fri 02:07]
     - Scala's /implicits/ are its most distinguished feature.
       They are the fundamental way to *ABSTRACT over context.*
       + They represent a unified paradigm with a great variety of use cases, among
         them:
         * *implementing* /type classes/
         * *establishing* /context/
         * /dependency injection/
         * *expressing* capabilities
         * *computing* NEW /types/ and *proving* _relationships_ between them.

     - Following Haskell, Scala was the _SECOND popular_ language to have some form
       of /implicits/. Other languages have followed suit. E.g
       1. *Rust*'s /traits/
       2. *Swift*'s /protocol extensions/.
       3. Design proposals are also on the table for *Kotlin* as /compile time
          dependency resolution/,
       4. for *C#* as /Shapes/ and /Extensions/
       5. for *F#* as /Traits/.
       6. Implicits are also a common feature of theorem provers such as *Coq* or
          *Agda*.
     - term inference :: GIVEN a /type/, the compiler *synthesizes* a "canonical"
       term that has that /type/

     - Even though these designs use widely different terminology, they are all
       variants of the core idea of /term inference/.
       + Scala *embodies* the idea in a _PURER form_ than most other languages:
         * /implicit parameter/:
           _DIRECTLY_ leads to an /inferred argument term/ that could also be
           written down EXPLICITLY.

         * /type class/ based design:
           _LESS DIRECT_ since they *hide* /term inference/ behind some form of
           /type classification/ and do NOT offer the option of writing the
           inferred quantities (typically, dictionaries) EXPLICITLY.

     - Q :: Given that /term inference/ is where the industry is heading, and
            given that Scala has it in a _VERY *pure* form_, how come /implicits/
            are *NOT* more popular?

     - A :: In fact, it's fair to say that /implicits/ are at the same time
            _Scala's MOST DISTINGUISHED and MOST *Controversial* feature._
              I believe this is due to a number of aspects that together make
            /implicits/ *HARDER to learn THAN NECESSARY* and also make it *HARDER
            to PREVENT ABUSES*.
       + Particular criticisms are:
         1. _Being very powerful, /implicits/ are EASILY *over-used* and *mis-used*._
            * This observation holds in almost all cases when we talk about /implicit
              conversions/, which,
              EVEN THOUGH _conceptually different_,
              _SHARE the *SAME* syntax_ with other /implicit definitions/.
              - For instance, regarding the two definitions
                #+begin_src scala
                  // conditional implicit value
                  implicit def i1(impllicit x: T): C[T] = ...

                  // implicit conversion
                  implicit def i2(x: T): C[T] = ...
                #+end_src

              - /Conditional implicit values/ are a cornerstone for expressing
                /type classes/,
                whereas most applications of /implicit conversions/ have turned
                out to be of *DUBIOUS* value.

              - The problem is that many newcomers to the language start with defining
                /implicit conversions/ since they are easy to understand and seem
                powerful and convenient.
                + Scala 3 will put under a _language flag_ both definitions and
                  applications of /"UNDISCIPLINED" implicit conversions/ between
                  /types/ defined elsewhere.
                    This is a useful step to *PUSH BACK against overuse* of /implicit
                  conversions/.

            * But the problem remains that _syntactically_,
              /conversions/ and /values/ just look *TOO SIMILAR for comfort.*

         2. Another widespread abuse is over-reliance on /implicit imports/.
            + This often leads to INSCRUTABLE /type errors/ that go away with the
              right import incantation, leaving a feeling of frustration.

            + Conversely, it is hard to see what /implicits/ a program uses since
              /implicits/ can hide anywhere in a long list of /imports/.

         3. The syntax of /implicit definitions/ is *TOO minimal*.
            It consists of a single /modifier/, ~implicit~, that can be attached
            to a large number of language constructs.
            + A problem with this for newcomers is that _it conveys mechanism instead
              of intent._
              For instance, a /type class instance/ is an /implicit object or val/
              if UNCONDITIONAL and an ~implicit def~ with ~implicit parameters~
              referring to some class if CONDITIONAL. This describes precisely
              what the /implicit definitions/ translate to -- just drop the
              ~implicit~ /modifier/, and that's it! But the cues that define intent
              are rather indirect and can be easily misread, as demonstrated by
              the definitions of ~i1~ and ~i2~ above.

         4. The syntax of /implicit parameters/ also has shortcomings.
            While /implicit parameters/ are designated specifically, arguments are
            NOT. This leads to two issues:

            + Passing an argument to an /implicit parameter/ _looks like a regular
              application ~f(arg)~._ -- this is *problematic* because it means there
              can be confusion regarding what parameter gets instantiated in a call.
              * For instance,
                in ~def currentMap(implicit ctx: Context): Map[String, Int]~ one
                *CANNOT* write ~currentMap("abc")~ since the string ~"abc"~ is
                taken as /explicit argument/ to the ~implicit ctx~ parameter. One
                _has to_ write ~currentMap.apply("abc")~ instead, which is _AWKWARD_
                and _IRREGULAR_.

            + A /method definition/ can only have one /implicit parameter/ section
              and it _MUST always come LAST_ (=from Jian= if not, how can the compiler
              knows which one is /implicit/).
              * This restriction _NOT ONLY reduces orthogonality_, _BUT ALSO prevents
                some useful program constructs_, such as
                - a /method/ with a /regular parameter/ whose /type/ depends on an
                  /implicit value/.

              * Finally, it's also a bit annoying that /implicit parameters/ must
                have a NAME, even though in many cases that name is never referenced.
                - =from Jian=
                  in my expericen, the percentage is definitely greater than 50%.

         5. /Implicits/ pose challenges for tooling.
            The set of available /implicits/ depends on /context/, so command
            completion has to take /context/ into account. This is feasible in an
            IDE but docs like ScalaDoc that are based static web pages can only
            provide an approximation.

            Another problem is that *failed* _implicit searches_ often give _very
            unspecific error messages_, in particular if some _DEEPLY recursive
            implicit search_ has *failed*.
              Note that the Dotty compiler has already made a lot of progress in
            the error diagnostics area. If a /recursive search/ *fails* some levels
            down, it shows what was constructed and what is missing. Also, it
            suggests imports that can bring missing /implicits/ in scope.

     - None of the shortcomings is fatal,
       after all /implicits/ are very widely used,
       and many libraries and applications rely on them.
       But together, they make code using /implicits/ a lot more *cumbersome* and
       *less clear than it could be.*

     - Historically, many of these shortcomings come from the way /implicits/ were
       gradually "discovered" in Scala.
       1. Scala originally had only /implicit conversions/ with the intended use
          case of "extending" a /class/ or /trait/ after it was defined,
       2. 1. is what is expressed by /implicit classes/ in later versions of Scala.
       3. /Implicit parameters and instance definitions/ came later in 2006 and
          we picked similar syntax since it seemed convenient.
          + For the same reason, NO effort was made to *distinguish* /implicit
            imports or arguments/ *from* _normal ones_.

     - Existing Scala programmers by and large have gotten used to the status quo
       and see little need for change.
       _BUT_ for newcomers this status quo presents a _big hurdle_.
       + I believe if we want to overcome that hurdle, we should take a step back
         and allow ourselves to consider a radically new design.

**** DONE The New Design
     CLOSED: [2020-07-17 Fri 02:06]
     - The following pages introduce a *REDESIGN* of /contextual abstractions/ in
       Scala. *They introduce _four_ fundamental CHANGES*:
       1. /Given Instances/ (use keyword ~given~):
          a new way to define basic terms that can be synthesized.
          + They _replace_ /implicit definitions/.

          + The core principle of the proposal:
            rather than mixing the ~implicit~ /modifier/ with a large number of
            features, we have a SINGLE WAY to define terms that can be synthesized
            for types.

       2. /Using Clauses/ (use keyword ~using~):
          a new syntax for _IMPLICIT parameters and their arguments_.
          + It *unambiguously* aligns /parameters/ and /arguments/, solving a number
            of language warts.

          + It also allows us to have _SEVERAL ~using~ clauses_ in a definition.
            * =from Jian=
              Scala 2 /implicit parameters and arguments/ can't do this -- if
              _not explicitly_ mark ~using~
              1. how can the compiler know if a parameter list is
                 - a normal parameter list
                   OR
                 - a manually pass /context parameters/
              2. if the compiler doesn't know which is which,
                 it doesn't know if some term inference need to be applied.

            * =from Jian=
              Here is an example of, if we don't need to mark ~using~ when manually
              pass the /context parameters/, what ambiguity can happen:
              #+begin_src scala
                def f(using a: T1, a2: T2)(c: T1, d: T2)(using e: T1, f: T2) = ...

                given x: T1 = ...
                given y: T2 = ...

                f(x, y)(x, y)
              #+end_src
              If we *ASSUME* Scala 3 doesn't require keyword ~using~ when explicitly
              passing /context parameters/, the meaning of ~f(x, y)~ can have ambiguity.
              Write down the possible interpretation in legal Scala 3 syntax:
              - ~f(using x, y)(x, y)~:
                LEGAL! The second /context parameter list/ will be inferred.

              - ~f(x, y)(using x, y)~:
                LEGAL! The first /context parameter list/ will be inferred.

              - ~f(using x, y)(using x, y)~:
                ILLEGAL! The /normal parameter list/ is not provided.

       3. /"Given" Imports/:
          a new class of /import selectors/ that _SPECIFICALLY import givens_
          and _NOTHING else_.
          + =from Jian=
            * Import /givens/ by their _names_ is like normal import sytax.
            * Import /givens/ by their _types_ need to use the keyword ~given~.

       4. /Implicit Conversions/:
          now expressed as /given instances/ of a standard ~Conversion~ class.
          All other forms of /implicit conversions/ WILL _be phased out_.

     - This section also contains pages describing other language features that
       are _related to_ /context abstraction/. These are:
       + /Context Bounds/, which carry over *unchanged*.

       + /Extension Methods/ REPLACE /implicit classes/ in a way that _INTEGRATES
         BETTER with /type classes/._

       + /Implementing Type classes/ demonstrates how some common /type classes/
         can be implemented using the new constructs, e.g. /extension method/.

       + /Type class Derivation/ introduces constructs to AUTOMATICALLY *derive*
         /type class instances/ for ADTs.

       + /Multiversal Equality/ introduces a special type class to support /type
         safe equality/.

       + /Context Functions/ provide a way to abstract over /context parameters/.

       + /By-Name Context Parameters/ are an essential tool to DEFINE /recursive
         synthesized values/ WITHOUT looping.

       + _Relationship with Scala 2 Implicits_ discusses the relationship between
         old-style implicits and new-style givens and how to migrate from one to
         the other.

     - Overall, the _new design_ achieves a BETTER *SEPARATION* of /term inference/
       *FROM* _the REST of the language_:
       + There is a *single way* to define /givens/ instead of a multitude of forms
         all taking an ~implicit~ /modifier/.

       + There is a *single way* to introduce /implicit parameters and arguments/
         _instead of_ conflating ~implicit~ with normal arguments.

       + There is a *separate way* to _import givens_ that does *NOT allow* them
         to *hide* in a sea of normal imports.

       + And there is a *single way* to define an /implicit conversion/ which is
         clearly marked as such and _does NOT require SPECIAL syntax._

     - This design thus
       + *avoids* feature interactions
       + makes the language more *consistent* and *orthogonal*.
       + make /implicits/ _easier to learn_ and _harder to abuse_.
       + greatly improve the *clarity* of the 95% of Scala programs that use
         /implicits/.
       + fulfil the promise of /term inference/ in a principled way
         that is also _accessible_ and _friendly_.

     - Q :: Could we achieve the same goals by tweaking existing implicits?

     - A :: After having tried for a long time, I believe now that this is
            *impossible*.
       1. Some of the problems are clearly _syntactic_ and
          _require different syntax_ to solve them.
          =from Jian=
          + For example, mutiple /context parameter lists/ is impossible in Scala 2.
            Scala 3 enable this feature by introducing new syntax, and manually
            passing /context parameters/ must explicitly use ~using~.

          + Make the /imports/ to /implicits/ explicitly.

          + Distinguish _define_ /implicits/ and _use_ /implicits/.

       2. There is the problem how to migrate.
          + Requirement:
            We cannot change the rules in mid-flight. At some stage of language
            evolution we need to accommodate both the new and the old rules.

          + Solution candiates:
            * With a syntax change, this is easy:
              1. *Introduce* the _NEW syntax_ with new rules,
              2. *Support* the _OLD syntax_ for a while to *facilitate* _cross compilation_,
              3. *Deprecate* and *phase out* the _OLD syntax_ at some later time.

            * (NOT actually available)
              Keeping the same syntax does not offer this path, and in fact does
              not seem to offer any viable path for evolution

       3. Even if we would somehow succeed with migration, if we don't choose to
          use new syntax in the new design, we still have the problem how to
          teach this.
          + We cannot make existing tutorials go away.
            * Almost all existing tutorials start with /implicit conversions/, which
              is not encouraged and the Scala 2 syntax will go away in Scala 3.1+;

            * They use _normal imports_, which will go away, and they explain calls
              to methods with /implicit parameters/ by expanding them to plain
              applications, which will also go away.
              =from Jian= new syntax need ~using~

          + This means that we'd have to add modifications and qualifications to
            all existing literature and courseware, likely _causing more confusion
            with beginners instead of less_.

          + By contrast,
            with a _NEW syntax_ there is a clear criterion:
            Any book or courseware that mentions ~implicit~ is OUTDATED and SHOULD
            BE UPDATED.

*** DONE Given Instances
    CLOSED: [2020-07-17 Fri 03:46]
    /Given instances/ (or, simply, "givens") define "canonical" values of certain
    /types/ that serve for /synthesizing arguments/ to /context parameters/ (=from
    Jian= through /using clause/).

    - =from Jian=
      The concepts of /context parameters/ and /using clauses/ will be introduced
      in the next section -- here what we need to know is /given instances/ and
      /context parameters/ (or /using clauses/) are _dual to each other_.

    - Example:
      #+begin_src scala
        trait Ord[T] {
          def compare(x: T, y: T): Int
          extension (x: T) def < (y: T) = compare(x, y) < 0
          extension (x: T) def > (y: T) = compare(x, y) > 0
        }

        given intOrd as Ord[Int] {
          def compare(x: Int, y: Int) =
            if (x < y) -1 else if (x > y) +1 else 0
        }

        given listOrd[T](using ord: Ord[T]) as Ord[List[T]] {
          def compare(xs: List[T], ys: List[T]): Int = (xs, ys) match {
            case (Nil, Nil) => 0
            case (Nil, _)   => -1
            case (_, Nil)   => +1
            case (x :: xs1, y :: ys1) =>
              val fst = ord.compare(x, y)
              if (fst != 0) fst else compare(xs1, ys1)
          }
        }
      #+end_src
      This code defines a /trait/ ~Ord~ (type class) with two /given instances/.

**** DONE Anonymous Givens
     CLOSED: [2020-07-17 Fri 02:28]
     The name of a /given instance/ *can be left out*.
     #+begin_src scala
       given Ord[Int] { /* ... */ }
       given [T](using Ord[T]) as Ord[List[T]] { /* ... */ }
     #+end_src
     If the name of a /given/ is missing,
     the compiler will _synthesize a name_ from the implemented type(s).

     - Note: =FIX-DOC= Add :
       The _name synthesized by the compiler_ is chosen to be _readable_ and
       _reasonably concise_.
       * For instance, the two instances above would get the names:
         ~given_Ord_Int~ and ~given_Ord_List_T~

     - The precise rules for synthesizing names are found in the subsection
       _Anonymous Given Instances_ of section _Relationship with Scala 2 Implicits_.
       + These rules *do not guarantee* absence of name conflicts between /given
         instances/ of /types/ that are "too similar".
         *To AVOID /conflicts/ one can use /named instances/.*

     - Note: =FIX-DOC= Add :
       To ensure robust binary compatibility,
       _publicly available libraries_ *should prefer* /named instances/.
       =IMPORTANT= =!!!= =IMPORTANT=

**** DONE Alias Givens - =IMPROVE DOC=
     CLOSED: [2020-07-17 Fri 03:44]
     An alias can be used to define a /given instance/ that is equal to some
     expression. E.g.:
     (=FIX-DOC= =IMPROVE-DOC= Here it's better to use the same example as below,
     then people can compare their syntax)
     #+begin_src scala
       given global as ExecutioinContext = new ForkJoinPool()
       given factory(using config: Config) as Factory = MemoizingFactory(config)
     #+end_src
     - When the first time ~global~ is accessed, the RHS is evaludated, which is
       then returned for _this and ALL subsequent_ accesses to ~global~.
       + =from Jian=
         More initialization rules see the "Given Instance Initialization" below.

     - This operation is /thread-safe/.

     - /Alias givens/ can be _anonymous_ as well, e.g.
       (=FIX-DOC= =IMPROVE-DOC= Here it's better to use the same example as above,
       then people can compare their syntax)
       #+begin_src scala
         given ExecutioinContext = new ForkJoinPool()
         given (using config: Config) as Factory = MemoizingFactory(config)
       #+end_src

     - An /alias given/ can have /type parameters/ and /context parameters/ just
       like any other /given/, _but it can ONLY implement A SINGLE TYPE._
       + =from Jian= =TODO= =Re-visit= =NOT SURE=
         Here _A SINGLE TYPE_ means:
         #+begin_src scala
           // Here:
           // - `A` is a type parameter
           // - `Abc` and `Lmn` are concrete types
           // - `Bc` and `Mn` are type constructors

           // Legal:
           given [T](using config: Bc[T]) as Lmn = ...

           // Illegal:
           given [T](using config: Abc) as Mn[T] = ...
         #+end_src
         * RATIONALE (=from Jian= my understanding, may be not comprehensive):
           - _Given instance syntax_ is a kind of _definition syntax_,
             and its duty is /given instances/ creation, can be /generics/ or not.

           - /Alias givens/ is designed only for *aliasing*,
             and its is duty is to create a name that is considered as a /given/,
             and it is actually a /reference/ which points to another /instance/,
             which can be a /given instance/ or a /regular non-given instance/.
             * Allow an /alias given/ to be a /generics/ _is equaivalent to_ allow
               it pointing to multiple /instances/!
                 If a _poit to_ is NOT deterministic, why do we need this feature?

             * Don't allow it to be /generics/ is also a design that can promise
               *orthogonality* between /given instances/ and /alias givens/
               - One benefit of /alias givens/ is, since it can _ONLY implement A
                 SSINGLE TYPE_, every /alias given/ refer one /instance/ -- when
                 using an /alias given/ you are sure that there is only one /instance/
                 this alias refers, you *don't need to worry* about _multiple
                 /instances/ creations_

**** DONE Given Macros
     CLOSED: [2020-07-17 Fri 03:38]
     /Given aliases/ can have the ~inline~ and ~transparent~ modifiers.
     - Example:
       #+begin_src scala
         transparent inline given mkAnnotations[A, T] as Annotations[A, T] = ${
           // code producing a value of a subtype of Annotations
         }
       #+end_src
       Since ~mkAnnotations~ is ~transparent~, the /type/ of an application is the
       _type of its right hand side_, which can be a proper /subtype/ of the declared
       /result type/ ~Annotations[A, T]~.

**** DONE Given Instance Initialization
     CLOSED: [2020-07-11 Sat 22:31]
     - A /given instance/
       + without /type parameters/ or /context parameters/
         *is initialized on-demand, the first time it is accessed.*
         * =from Jian=
           this is /thread safe/, mentioned in the above _Alias Given_ section

       + has /type parameters/ or /context parameters/, a *FRESH* /instance/ is
         created _for EACH reference_. --- =from Jian= common sense

**** DONE Syntax
     CLOSED: [2020-07-17 Fri 03:46]
     #+begin_src text
       TmplDef  ::= ...
                 |  'given' GivenDef

       GivenDef ::= [GivenSig] Type '=' Expr  // Comment: this is the "alias givens" syntax
                 |  [GivenSig] ConstrApps [TemplateBody]

       GivenSig ::= [id] [DefTypeParamClause] {UsingParamClause} 'as'
     #+end_src

*** DONE Using Clauses
    CLOSED: [2020-07-17 Fri 04:19]
    - Functional programming tends to _express most dependencies_ AS _simple function
      parameterization_.
      + Pros:
        clean and powerful,

      + Cons:
        sometimes leads to functions that take _MANY_ /parameters/
        where _the same value is passed over and over again_ in _LONG call chains_
        to _MANY_ functions.
        * Q :: What is a good way to get rid of this?

        * A :: /Context parameters/ can help here
               since they ENABLE
               the compiler to *synthesize* repetitive /arguments/
               INSTEAD OF the programmer having to write them EXPLICITLY.
          - =from Jian=
            /context parameters/ is a powerful solution but not the only solution.
            When exploit the more basic concepts /scopes/ and /class/, we know if
            we can try to put common variables in a /scope/ that can be access by
            functions (methods), then we don't need to pass those variables as
            parameters to functions.
            + Actually I can describe this as /term inferene by scope/ (I don't
              know if this name showed up in other literatures), whereas the /context
              parameters/ is /term inference by type/.
                Similarly, I can call /inheritence from supertype(s)/ as /term
              inference by subtyping relation/.
              * Summary:
                If consider in this way, I discover that /term inference/ is
                everywhere. Though they are based on different mechanisms, they
                are all /term inferences/.

    - =from Jian=
      /Givens/ tell the compiler that when applicable (means *in scope* and *match
      type*) they can be used to *synthesize* /arguments/ when /Using clauses/ show
      up.

    - Example:
      Assume required /givens/, here they are ~Ord[Int]~ and ~Ord[List[Int]]~, are
      *in scope* or can be *synthesize* with in scope /givens/.
      #+begin_src scala
        def max[T](x: T, y: T)(using ord: Ord[T]): T =
          if ord.compare(x, y) < 0 then y else x

        // The explicit way
        max(2, 3)(using intOrd)

        // The implicit way
        max(2, 3)
        max(List(1, 2, 3), Nil)
      #+end_src
      From the /givens/ defined in the last section,
      + ~intOrd~ is defined, it is in scope, and it can be used for ~max(2, 3)~
      + ~listOrd~ is defined, it is in scope, the /context parameter/ it need is
        ~intOrd~, which is also in scope, and thus the an instance of ~Ord[List[Int]]~
        can be *synthesized*.

**** DONE Anonymous Context Parameters
     CLOSED: [2020-07-17 Fri 04:03]
     In many situations,
     the _NAME_ of a /context parameter/ _need *NOT* be mentioned EXPLICITLY
     *AT ALL*,_ since it is used only in *synthesized arguments* for other
     /context parameters/.
     - Example:
       #+begin_src scala
         def maximum[T](xs: List[T])(using Ord[T]): T =
           xs.reduceLeft(max)
       #+end_src
       Here the /context parameter/ of type ~Ord[T]~ is *synthesized*
       + for ~max~,
       + *NOT* for EXPLICIT USE as inside ~max~.

     - Generally, /context parameters/ may be defined either as
       =from Jian= NO mixture of the two ways below is allowed!!!
       + a FULL /parameter list/ ~(p_1: T_1, ..., p_n: T_n)~
         =from Jian= Here FULL means *BOTH* _names_ and /types/ ARE PROVIDED.
         OR
       + a sequence of /types/ ~T_1, ..., T_n~.

     - Resaonable restriction:
       /Vararg parameters/ are *not* supported in /using clauses/.

**** DONE Inferring Complex Arguments
     CLOSED: [2020-07-17 Fri 04:06]
     #+begin_src scala
       def descending[T](using asc: Ord[T]): Ord[T] = new Ord[T] {
         def compare(x: T, y: T) = asc.compare(y, x)
       }

       def minimum[T](xs: List[T])(using Ord[T]) =
         maximum(xs)(using descending)

       // minimum(xs)
       //
       // EVALUATION by SUBSTITUTING a FUNCTON CALL with Its BODY:
       // maximum(xs)(using descending)
       //
       // CONTEXT ARGUMENTS INFERENCE:
       // maximum(xs)(using descending(using listOrd))
       // maximum(xs)(using descending(using listOrd(using intOrd)))
     #+end_src

**** DONE Multiple Using Clauses
     CLOSED: [2020-07-17 Fri 04:15]
     There can be SEVERAL /using clauses/ in a definition and /using clauses/ can be
     freely mixed with normal parameter clauses.

     Example:
     #+begin_src scala
       def f(u: Universe)(using ctx: u.Context)(using s: ctx.Symbol, k: ctx.Kind) = ...
     #+end_src

     - *Multiple* /using clauses/ are matched left-to-right in applications.
       Example:
       #+begin_src scala
         object global extends Universe { type Context = ... }
         given ctx  as global.Context { type Symbol = ...; type Kind = ... }
         given sym  as ctx.Symbol
         given kind as ctx.Kind
       #+end_src
       Then the following calls are all valid (and normalize to the last one)
       #+begin_src scala
         f(global)  // source code
         f(global)(using ctx)  // step 1
         f(global)(using ctx)(using sym, kind)  // step 2 -- Done
       #+end_src

     - Invalid, for example:
       ~f(global)(using sym, kind)~
       + =from Jian=
         When the compiler search a function, if the function is a named function
         (like in this example), the compiler will try to match its whole signature,
         including names and parameter types. The compiler can't support this syntax.
         If it can, it doesn't do left to right match. If it doesn't do left to right
         match, I can create some ambiguity in a example:
         #+begin_src scala
           def g(u: Universe)(using s: ctx.Symbol, k: ctx.Kind)(using s1: ctx.Symbol, k1 ctx.Kind) = ...
           g(global)(using sym, kind)
         #+end_src
         We don't know the last parameter list ~(using sym, kind)~ is for the second one,
         or for the third one.

**** DONE Summoning Instances
     CLOSED: [2020-07-17 Fri 04:18]
     - =from Jian=
       ~sommon~ from ~Predef~ is a replacement of the ~implicitly~ in Scala 2.

     - The ~summon~ is simply defined as /the (*non-widening*) identity function/
       over a /context parameter/:
       #+begin_src scala
         def sommon[T](using x: T): x.type = x
       #+end_src
       + =from Jian=
         The *non-widening* is the DIFFERENCE between ~summon~ and ~implicitly~.
         This is why we say ~summon~ can provide more concise result.
         =IMPORTANT=
         =TODO= example of when ~summon~ can work, but ~implicitly~ can't work.

**** DONE Syntax
     CLOSED: [2020-07-17 Fri 04:19]
     #+begin_src text
       ClsParamClause      ::=  ... | UsingClsParamClause
       DefParamClauses     ::=  ... | UsingParamClause
       UsingClsParamClause ::=  '(' 'using' (ClsParams | Types) ')'
       UsingParamClause    ::=  '(' 'using' (DefParams | Types) ')'
       ParArgumentExprs    ::=  ... | '(' 'using' ExprsInParens ')'
     #+end_src

*** DONE Context Bounds
    CLOSED: [2020-07-17 Fri 04:21]
    A /context bound/ is a *SHORTHAND* for expressing the common pattern (a.k.a
    type class pattern) of an /context parameter/ that depends on *One* /type
    parameter/.
    #+begin_src scala
      def maximum[T: Ord](xs: List[T]): T = xs.reduceLeft(max)
    #+end_src

    - The /context parameter(s)/ *generated from* /context bounds/ come *LAST*
      in the definition of the containing /method/ or /class/. E.g.
      #+begin_src scala
        def f[T: C1 : C2, U: C3](x: T)(using y: U, z: V): R

        // would expand to

        def f[T, U](x: T)(using y: U, z: V)(using C1[T], C2[T], C3[U]): R
      #+end_src

     - /Context bounds/ can be combined with /subtype bounds/.
       _If both are present, /subtype bounds/ *come first*,_ e.g.
       ~def g[T <: B : C](x: T):R = ...~

**** Migration - =RE-READ=
     - To ease migration, /context bounds/ in Dotty
       + in Scala 3.0
         /context bounds/ in Dotty _map to_ /OLD-STYLE implicit parameters/ for
         which /arguments/ can be passed
         * _EITHER_ with a /using clause/
         * _OR_ with a normal application as in Scala 2.

       + From Scala 3.1 on,
         /context bounds/ in Dotty will _map to_ /context parameters/ instead,
         as is described above.

     - If _the source version is 3.1_ and _the =-migration= command-line option is
       set_,
       ANY pairing of an /evidence context parameter/ stemming from a /context
       bound/ with a /normal argument/ (=from Jian= not marked with ~using~) will
       give a *MIGRATION WARNING* which indicates that a /using clause/ is needed
       instead.
       + The _REWRITE_ can be done AUTOMATICALLY under =-rewrite=.

**** Syntax
     #+begin_src text
       TypeParamBounds ::= [SubtypeBounds] {ContextBound}
       ContextBound    ::= ':' Type
     #+end_src

*** DONE Given Imports
    CLOSED: [2020-07-18 Sat 21:33]
    # Importing Givens
    =from Jian= The /given imports syntax/ can be applied as syntax for ~export~.

    A _special form_ of /import wildcard selector/ is used to *import /given
    instances/.*
    - Example:
      #+begin_src scala
        object A {
          class TC
          given tc as TC
          def f(using TC) = ???
        }

        object B {
          import A._
          import A.given
          // ...
        }
      #+end_src
      + In Dotty,
        * Import *EVERYTHING except givens* (this is _different from Scala 2_):
          ~import A._~

        * Import *ALL givens*:
          ~import A.given~

        * Import *everything* in ~A~:
          ~import A.{given, _}~

    - There are *TWO* main _benefits_ arising from these rules:
      + It is MADE CLEARER where /givens/ in scope are coming from.
        * =from Jian=
          This AMBIGUITY only appear when use wildcard import in Scala 2.
          - This is why you can still import /givens/ through their names if you
            don't use /wildcard import/ -- no ~given~ is required.
            For example, ~import A.tc~ is legal!

        * In particular,
          it is *NOT POSSIBLE to HIDE* imported /givens/ in a long list of
          /regular wildcard imports/.

      + It ENABLES importing ALL /givens/ WITHOUT importing anything else.
        This is _particularly important since /givens/ can be ANONYMOUS_, so the
        usual recourse of using /named imports/ is NOT practical --
        =from Jian= next subsection will introduce /importing (/givens/) by type/.

**** DONE Importing By Type
     CLOSED: [2020-07-18 Sat 21:14]
     Since /givens/ can be _anonymous_ it is _NOT always practical to import them
     by their name_, and /wildcard imports/ are typically used instead.
     - =from Jian=
       However, there is no reason when you want to import SOME /anonymous
       givens/ you must import ALL /anonymous givens/.
       + Avoid using /wildcard imports/:
         /By-type imports/ syntax is introduced.
         It provides a _MORE SPECIFIC alternative_ to /wildcard imports/, which
         makes it clearer what is imported.

     - =from Jian=
       /Importing by type/ is actually /Importing givens by type/.
       /Importing non-givens by type/ is *NOT allowed*.
       + Actually,
         /Importing non-givens by type/ is allowed at least in Dotty 0.23,
         but this feature was removed from 0.24+ (when I write this sentence down,
         current doc version is 0.26.0)

     - =from Jian=
       Check the last example below and you will notice
       + /by name imports/
         don't need ~given~ _no matter the imported IS /givens/ or NOT_

       + ONLY /by type imports/
         need ~given~.

     - Examples:
       + ~import A.{given TC}~

       + ~import A.{given T1, given T2, ..., given Tn}~

       + Example code:
         #+begin_src scala
           object Instances {
             given intOrd as Ordering[Int]
             given listOrd[T: Ordering] as Ordering[List[T]]
             given ec as ExecutionContext = ...
             given im as Monoid[Int]
           }

           import A.{given Ordering[?], given ExecutionContext}
         #+end_src
         =IMPORTANT= easy to forget this usage
         This would import the ~intOrd~ (for ~listOrd~), ~listOrd~, and ~ec~
         instances but leave out the ~im~ instance, since it fits none of the
         specified bounds.

       + /By-type imports/ can be *mixed* with /by-name imports/.
         If BOTH are present in an import clause, *by-type imports come last*.
         ~import A.{im, given Ordering[?]}~

**** DONE Migration
     CLOSED: [2020-07-18 Sat 21:20]
     - The rules for /imports/ stated above have the consequence that a library
       would have to
       *MIGRATE* in lockstep with all its users
       *from* /old style implicits/ and /normal imports/
       *to* /givens/ and /given imports/.

     - The following modifications avoid this hurdle to migration.
       + A /given import selector/ also brings /old style implicits/ into scope.
         So, in _Scala 3.0_ an /old-style implicit definition/ can be brought into
         scope
         * EITHER by a ~_~
         * OR by a ~given _~ /wildcard selector/.

       + In _Scala 3.1_,
         /old-style implicits/ ACCESSED THROUGH a ~_~ /wildcard import/ will give
         a *deprecation warning*.

       + In some version *AFTER* 3.1,
         /old-style implicits/ ACCESSED THROUGH a ~_~ /wildcard import/ will give a
         *compiler error*.

     - These rules mean that library users
       + can use ~given \under{}~ /selectors/ to ACCESS /old-style implicits/ in _Scala 3.0_,
         AND
       + will be gently nudged and then forced to do so in later versions.
         Libraries can then *switch to* /given instances/ *once* their user base has
         migrated.

**** DONE Syntax
     CLOSED: [2020-07-18 Sat 21:30]

*** DONE Extension Methods
    CLOSED: [2020-03-10 Tue 00:59]
    /Extension methods/ allow one to add /methods/ to a /type/ after the /type/
    is defined.
    - =from Jian=
      A way to extend a closed system (not own, or better not change source code).

    - Example:
      + Definition:
        #+begin_src scala
          case class Circle(x: Double, y: Double, radius: Double)

          extension (c: Circle)
            def circumference: Double = c.radius * math.Pi * 2
        #+end_src

      + Invoke as regular /methods/:
        #+begin_src scala
          val circle = Circle(0, 0, 1)
          circle.circumference
        #+end_src

**** DONE Translation of Extension Methods
     CLOSED: [2020-07-19 Sun 01:08]
     - extension methods :: /methods/ that have a parameter clause in front of the
       defined identifier.

     - An /extension method/ named ~f~ translates to /method/ named ~extension_f~
       that TAKES the _leading parameter_ section AS its _first argument list_.

     - So, the definition of ~circumference~ above translates to the plain
       method, and can also be invoked as such:
       #+begin_src scala
         def extension_circumference(c: Circle): Double = c.radius * math.Pi * 2

         assert(circle.circumference == circumference(circle))
       #+end_src

**** DONE Operators
     CLOSED: [2020-07-19 Sun 01:12]
     - Use /extension method syntax/ to define /operators/.
       + This case is indicated by *omitting the period* between the leading
         parameter list and the operator.

       + This syntax _mirrors_ the way the /operator/ is applied.

     - Examples:
       #+begin_src scala
         extension (x: String)
           def < (y: String): Boolean = ...

         extension (x: Elem)
           def +: (xs: Seq[Elem]): Seq[Elem] = ...

         extension (x: Number)
           @infix def min (y: Number): Number = ...

         "ab" < "c"
         1 +: List(2, 3)
         x min 3
       #+end_src
       + For /alphanumeric extension operators/ like ~min~ an ~@infix~ annotation
         is *implied*.

       + The translations:
         #+begin_src scala
           def extension_<(x: String)(y: String): Boolean = ...
           def extension_+:(xs: Seq[Elem])(x: Elem): Seq[Elem] = ...
           @infix def extension_min(x: Number)(y: Number): Number = ...
         #+end_src
         * =IMPORTANT= =!!!= =IMPORTANT=
           Remember that in Scala ~:~ suffixed operators are all /right associative/!!!
           This is why ~+:~ in the translation, the order of ~x~ and ~xs~ are swapped!
           - This is similar to the operator of ~Seq~.
             The Scala compiler *preprocesses* an _infix operation_ ~x \plus{}: xs~ *to*
             ~xs.\plus{}:(x)~.

**** DONE Generic Extensions
     CLOSED: [2020-07-19 Sun 01:17]
     This section will discuss /extension method/ of /generic type/.

     - Examples:
       #+begin_src scala
         extension [T](xs: List[T])
           def second = xs.tail.head

         extension [T: Numeric](x: T)
           def + (y: T): T = summon[Numeric[T]].plus(x, y)
       #+end_src

     - If an /extension method/ has /type parameters/,
       they come _immediately after_ ~extension~ and
       are _followed by_ the /extended parameter/.

     - When calling a /generic extension method/, any EXPLICITLY given /type
       arguments/ should follow the /method/ NAME (of course, usually they are
       left out since they can be inferred).
       + So the ~second~ /method/ could be instantiated as follows.
         ~List(1, 2, 3).second[Int]~

     - /Extensions/ can also take /using clauses/.
       For instance, the ~+~ extension above could equivalently be written with a
       /using clause/:
       #+begin_src scala
         extension [T](x: T)(using n: Numeric[T])
           def + (y: T): T = n.plus(x, y)
       #+end_src

     - Note:
       /Type parameters/ have to be given *immediately after* the ~extension~
       keyword; they *CANNOT* be given AFTER the ~def~.
       + _This RESTRICTION might be lifted_
         *in the future ONCE* we support /multiple type parameter clauses/ in a
         /method/.
           By contrast, there can be /using clauses/ _in front_ as well as _after_
         the ~def~.

**** DONE Collective Extensions
     CLOSED: [2020-07-19 Sun 01:33]
     Define several /extension methods/ that *SHARE the SAME left-hand parameter
     type.*
     =from Jian= I prefer to call it /left-hand parameter type/ the /receiver type/.

     - In this case one can "pull out" the common parameters into a single /extension/
       and enclose all methods in the following braces or indented region followed
       by ~:~.
       + Examples:
         #+begin_src scala
           extension (ss: Seq[String]):
             def longestStrings: Seq[String] =
               val maxLength = ss.map(_.length).max
               ss.filter(_.length == maxLength)

             def longestString: String = longestStrings.head
         #+end_src

     - Note the right-hand side of ~longestString~: it calls ~longestStrings~
       directly, implicitly assuming the common extended value ~ss~ as receiver.

     - /Collective extensions/ like these are a shorthand for individual extensions
       where each method is defined separately. For instance, the first extension
       above expands to
       #+begin_src scala
         extension (ss: Seq[String])
           def longestStrings: Seq[String] =
             val maxLength = ss.map(_.length).max
             ss.filter(_.length == maxLength)

         extension (ss: Seq[String]):
           def longestString: String = ss.longestStrings.head
       #+end_src
       + *CAUTION*
         Now the ~longestStrings.head~ write down its /receiver/ *explicitly*.
         Since ~longestStrings~ and ~longestString~ are defined INDEPENDENTLY, and
         *NO assumption about the SHARED /receiver/ can be made!*
         * This is clear if we re-write the expansion as:
           #+begin_src scala
             // No change for `longestStrings`, receiver is `(ss: Seq[String])`

             extension (ss1: Seq[String]):
               def longestString: String = ss1.longestStrings.head
           #+end_src

     - /Collective extensions/ also can take /type parameters/ and
       have /using clauses/.
       + Example:
         #+begin_src scala
           extension [T](xs: List[T])(using Ordering[T])
             def smallest(n: Int): List[T] = xs.sorted.take(n)

             def smallestIndices(n: Int): List[Int] =
               val limit = smallest(n).max
               xs.zipWithIndex.collect { case (x, i) if x <= limit => i }
         #+end_src

**** DONE Translation of Calls to Extension Methods - =RE-READ=
     CLOSED: [2020-07-19 Sun 03:28]
     - Prerequisite:
       To *convert* a /reference/ *to* an /extension method/,
       the compiler has to know about the /extension method/ -- we say in this
       case that the /extension method/ is applicable at the point of /reference/.

     - There are _FOUR_ possible ways for an /extension method/ to be applicable
       (assume the /reference/ is in the form of ~r.m~):
       1. The /extension method/ is visible under a simple name,
          by being
          + *defined* or
          + *inherited* or
          + *imported* in a /scope/ *enclosing* the /reference/.

       2. The /extension method/ is a /member/ of some /given instance/ that is
          visible at the point of the _reference_.

       3. the /extension method/ _is *defined IN* the /implicit scope/
          of the /type/ of ~r~._

       4. the /extension method/ _is *defined IN* some /given instance/
          in the /implicit scope/ of the /type/ of ~r~._

     - Examples of each rule
       + Example of rule 1 above:
         #+begin_src scala
           trait IntOps:
             extension (i: Int) def isZero: Boolean = i == 0

           extension (i: Int) def safeMod(x: Int): Option[Int] =
             // extension method defined in same scope IntOps
             if x.isZero
             then None
             else Some(i % x)

           object IntOpsEx extends IntOps:
               extension (i: Int) def safeDiv(x: Int): Option[Int] =
             // extension method brought into scope via inheritance from IntOps
             if x.isZero
             then None
             else Some(i / x)

           trait SafeDiv:
             import IntOpsEx._ // brings safeDiv and safeMod into scope

             extension (i: Int) def divide(d: Int) : Option[(Int, Int)] =
               // extension methods imported and thus in scope
               (i.safeDiv(d), i.safeMod(d)) match
                 case (Some(d), Some(r)) => Some((d, r))
                 case _                  => None
         #+end_src

       + Example of rule 2 above:
         #+begin_src scala
           given ops1 as IntOps  // brings safeMod into scope

           1.safeMod(2)
         #+end_src

       + Example of rule 3 and 4 above:
         #+begin_src scala
           class List[T]:
             ...

           object List:
             extension [T](xs: List[List[T]])
               def flatten: List[T] = xs.foldLeft(Nil: List[T])(_ ++ _)

             given [T: Ordering] as Ordering[List[T]]:
               extension (xs: List[T])
                 def < (ys: List[T]): Boolean = ...
           end List

           // extension method available since it is in the implicit scope of List[List[Int]]
           List(List(1, 2), List(3, 4)).flatten

           // extension method available since it is in the given Ordering[List[T]],
           // which is itself in the implicit scope of List[Int]
           List(1, 2) < List(3)
         #+end_src

     - The *precise* rules for *resolving* a selection to an /extension method/
       are as follows.
       Assume a selection ~e.m[Ts]~ where ~m~ is not a /member/ of ~e~, where
       the /type arguments/ ~[Ts]~ are _OPTIONAL_, and where ~T~ is the expected
       /type/.
       *The following TWO rewritings are tried _in order_:*
       1. The selection is rewritten to ~extension_m[Ts](e)~.

       2. If the first rewriting does _NOT_ typecheck with expected type ~T~, and
          there is an extension method ~m~ in some eligible object ~o~, the
          selection is rewritten to ~o.extension_m[Ts](e)~. An object ~o~ is
          eligible if
          + ~o~ forms part of the /implicit scope/ of ~T~, or

          + ~o~ is a /given instance/ that is visible at the point of the
            application, or

          + ~o~ is a /given instance/ in the /implicit scope/ of ~T~.

          This second rewriting is attempted at the time where the compiler also
          tries an /implicit conversion/ from ~T~ to a /type/ containing ~m~.
          *If there is more than one way of rewriting, an _ambiguity error_ results.*

     - An /extension method/ can also be used as an /identifier/ by itself
       (=from Jian= without an explicit qualifier).
       + If an /identifier/ ~m~ does _NOT_ resolve,
         the /identifier/ is rewritten to:
         * ~x.m~ if the identifier appears in an /extension/ with /parameter/ ~x~
         * ~this.m~ otherwise

       + The rewritten term is again tried as an application of an /extension method/.
         Example:
         #+begin_src scala
           extension (s: String)
             def position(ch: Char, n: Int): Int =
               if n < s.length && s(n) != ch
               then position(ch, n + 1)
               else n
         #+end_src
         + The recursive call ~position(ch, n + 1)~ expands to ~s.position(ch, n + 1)~
           in this case.

         + The whole /extension method/ rewrites to
           #+begin_src scala
             def extension_position(s: String)(ch: Char, n: Int): Int =
               if n < s.length && s(n) != ch
               then extension_position(s)(ch, n + 1)
               else n
           #+end_src

**** DONE More Details
     CLOSED: [2020-07-19 Sun 03:30]
     1. To avoid confusion,
        NAMES of /normal methods/ are *NOT ALLOWED to start with* ~extension_~.

     2. A /named import/ such as ~import a.m~ of an /extension method/ in ~a~
        will make ~m~ *only* available as an /extension method/.
        + To access it under ~extension_m~ that name as to be imported separately.
          Example:
          #+begin_src scala
            object DoubleOps:
              extension (x: Double) def ** (exponent: Int): Double =
                require(exponent >= 0)
                if exponent == 0
                then 1
                else x * (x ** (exponent - 1))

            import DoubleOps.{**, extension_**}
            assert(2.0 ** 3 == extension_**(2.0)(3))
          #+end_src

**** DONE Syntax
     CLOSED: [2020-07-19 Sun 03:31]
     - ~extension~ is a /soft keyword/.
       + It is recognized as a /keyword/
         _ONLY_ if
         * it appears at the start of a statement
           AND
         * it is followed by ~[~ or ~(~.

       + In all other cases it is treated as an /identifier/.

*** DONE Implementing Type classes
    CLOSED: [2020-07-20 Mon 00:04]
    - Type Class :: an /abstract/, /parameterized/ type that lets you add new
      behavior to any *closed* /data type/ *without* using /sub-typing/.
      + /extension methods/ is a technical way (new syntax) to enhance a *closed*
        /data type/

      + /type class/ is a *systematically strategy* of enhancing a *closed* /data
        type/, and it can exploit the /extension methods/ technique.

    - Examples of use-cases:
      + expressing how a /type/ you don't own (from the standard or 3rd-party library)
        conforms to such behavior

      + expressing such a behavior for MULTIPLE /types/ *without* involving
        /sub-typing/ relationships (one extends another) between those /types/
        (see: /ad hoc polymorphism/ for /instance/)

    - One concept can have multiple implementations. In Scala 3,
      + Type Classes :: /generic traits/ that are *NOT defined through* the ~extends~
        keyword, but by /given instances/.

    - Some examples of common type classes are in the next subsections.

**** DONE Semigroups and monoids
     CLOSED: [2020-07-19 Sun 22:34]
     #+begin_src scala
       trait SemiGroup[T]:
         extension (x: T) def combine (y: T): T

       trait Monoid[T] extends SemiGroup[T]:
         def unit: T

       object Monoid {
         def apply[T](using m: Monoid[T]) = m
       }

       // For `String`
       given Monoid[String]:
         extension (x: String) def combine (y: String): String = x.concat(y)
         def unit: String = ""

       // For `Int`
       given Monoid[Int]:
         extension (x: Int) def combine (y: Int): Int = x + y
         def unit: Int = 0

       //// If no `Monoid` companion object `apply` method
       // def combineAll[T: Monoid](xs: List[T]): T =
       //  xs.foldLeft(summon[Monoid[T]].unit)(_.combine(_))

       def combineAll[T: Monoid](xs: List[T]): T =
         xs.foldLeft(Monoid[T].unit)(_.combine(_))
     #+end_src

**** DONE Functors
     CLOSED: [2020-07-19 Sun 22:40]
     - Functor :: a type provides the ability for its values to be "mapped over".

     - We can represent all types that can be "mapped over" with ~F~ -- a /type
       constructor/ that need ONE /type argument/ to construct a /type/.
       + Therefore we write it ~F[_]~, hinting that the /type constructor/ ~F~
         takes another /type/ as argument.

     - The instance of ~Functor~ for ~List~ now becomes:
       + WITHOUT /extension method/:
         #+begin_src scala
           trait Functor[F[_]]:
             def [A, B](x: F[A]).map(mapper: A => B): F[B]

           given Functor[List]:
             def map[A, B](x: List[A], f: A => B): List[B] =
               x.map(f)  // `List` already has a `map` method

           def assertTransformation[F[_]: Functor, A, B](expected: F[B], original: F[A], mapping: A => B): Unit =
             assert(expected == summon[Functor[F]].map(original, mapping))

           assertTransformation(List("a1", "b1"), List("a", "b"), elt => s"${elt}1")
         #+end_src
         * When define the /type class/, use ~F[_]~ to indicate ~F~ is an /kind-1
           type constructor/.

         * When implement the /given instance/, use ~List~ is enough to tell the
           compiler that it is a /kind-1 type constructor/.

       + WITH /extension method/:
         #+begin_src scala
           trait Functor[F[_]]:
             extension [A, B](x: F[A])
               def map(f: A => B): F[B]

           given Functor[List]:
             extension [A, B](xs: List[A])
               def map(f: A => B): List[B] =
                 xs.map(f) // List already has a `map` method

           def assertTransformation[F[_]: Functor, A, B](expected: F[B], original: F[A], mapping: A => B): Unit =
             assert(expected == original.map(mapping))

           assertTransformation(List("a1", "b1"), List("a", "b"), elt => s"${elt}1")
         #+end_src

**** DONE Monads
     CLOSED: [2020-07-20 Mon 00:04]
     - A ~Monad~ for type ~F[_]~ is a ~Functor[F]~ with _TWO_ more operations:
       * ~flatMap~, which turns an ~F[A]~ into an ~F[B]~ when given a function of
         type ~A => F[B]~,

       * ~pure~, which creates an ~F[A]~ from a single value ~A~.

     - Implementation:
       #+begin_src scala
         // "A `Monad` for type `F[_]` is a `Functor[F]`" => thus has the `map` ability
         trait Monad[F[_]] extends Functor[F]:

           /** The unit value for a monad */
           def pure[A](x: A): F[A]

           extension [A, B](x: F[A])
             /** The fundamental composition operation */
             def flatMap(f: A => F[B]): F[B]

             /** The `map` operation can now be defined in terms of `flatMap` */
             def map(f: A => B) = x.flatMap(f.andThen(pure))

         end Monad
       #+end_src

***** ~List~
      #+begin_src scala
        given listMonad as Monad[List]:
          def pure[A](x: A): List[A] =
            List(x)

          extension [A, B](xs: List[A])
            def flatMap(f: A => List[B]): List[B] =
              xs.flatMap(f)  // rely on the existing `flatMap` method of `List`
      #+end_src

***** ~Option~
      #+begin_src scala
        given optionMonad as Monad[Option]:
          def pure[A](x: A): Option[A] =
            Option(x)

          extension [A, B](xo: Option[A])
            def flatMap(f: A => Option[B]): Option[B] =
              xo match {
                case Some(x) => f(x)
                case None    => None
              }
      #+end_src

***** ~Reader~
      - Reader Monad :: it is used to COMBINE /functions/ that ALL need the *same*
        /data/.
        + =from Jian=
          * If not using /reader monad/, _this *same* data_ will be encoded as a
            /parameter/ for each functions that need to be combined.

          * /Reader monad/ is the one that help us to access _this common data_
            _WITHOUT passing it EXPLICITLY_ to all functions that need to be combined.

        + This _common DATA_ is usually some _configuration_, _context_, _environment
          variables_, _etc_.

      - Let's define a ~Config~ type, and two functions using it:
        #+begin_src scala
          trait Config:
            // ...
          end Config

          def compute(i: Int)(config: Config): String = ???
          def layout(str: String)(config: Config): Unit = ???
        #+end_src
        + =Fix-Doc=
          From the context, this ~layout~ should be named as ~show~.

        + Requirement:
          Combine ~compute~ and ~show~ into a single function.
          * Naive Solution (NO one will like this, especially when new use it frequently):
            ~show(compute(i)(config))(config)~

        + Addition requirement:
          Can we avoid passing ~config~ to both functions, and put ~config~ in
          /context/ that can be accessed by both functions.
          * Postulated Solution (if you know a /function/ can be a /monad/, it may
            be easy for you to guess we have a solution of this form):
            #+begin_src scala
              def computeAndShow(i: Int): Config => Unit =
                compute(i).flatMap(show)
            #+end_src
            Then let's try to implement this ~flatMap~.

      - Let's define a /monad instance/ for functions that need a /context/ ~Config~.
        =from Jian=
        Here /context/ is more general than /context parameter/. Of course, it is
        possible to change the signature of ~compute~ and ~show~, and make their
        ~config: Config~ parameter a /context parameter/. However, this is need to
        change a existing and may be *closed* system, which is not always an
        acceptable solution. This is why we need a more flexible way to introduce
        the /context/ info through /reader monads/.

        1. Define a /type/ named ~ConfigDependent~ representing a function that
           when passed a ~Config~ produces a ~Result~.
           #+begin_src scala
             type ConfigDependent[Result] = Config => Result
           #+end_src

        2. The /monad instance/ will look like this:
           #+begin_src scala
             given configDependentMonad as Monad[ConfigDependent]:

               def pure[A](x: A): ConfigDependent[A] =
                 config => x

               extension [A, B](x: ConfigDependent[A])
                 def flatMap(f: A => ConfigDependent[B]): ConfigDependent[B] =
                   config => f(x(config))(config)

             end configDependentMonad
           #+end_src

      - =from Jian=
        Here is a better implementation -- /functions/ a often used as /reader
        monads/, we can give it a view that is similar to /functions/:
        The /type/ ~ConfigDependent~ can be written using /type lambdas/.
        Using this syntax would turn the previous ~configDependentMonad~ into:
        #+begin_src scala
          type ConfigDepdenent = [Result] =>> Config => Result

          given configDependentMonad as Monad[[Result] =>> Config => Result]:

            def pure[A](x: A): Config => A =
              config => x

            extension [A, B](x: Config => A)
              def flatMap(f: A => Config => B): Config => B =
                config => f(x(config))(config)

          end configDependentMonad
        #+end_src

      - It is likely that we would like to use this pattern with other kinds of
        environments than our ~Config~ /trait/.
          The ~Reader~ /monad/ allows us to *abstract away* ~Config~ as a /type
        parameter/, named ~Ctx~ in the following definition:
        #+begin_src scala
          given readerMonad[Ctx] as Monad[[X] =>> Ctx => X]:

            def pure[A](x: A): Ctx => A =
              ctx => x

            extension [A, B](x: Ctx => A)
              def flatMap(f: A => Ctx => B): Ctx => B =
                ctx => f(x(ctx))(ctx)

          end readerMonad
        #+end_src
        + =from Jian=
          Here is a use case for /type lambda/.

**** DONE Summary
     CLOSED: [2020-07-20 Mon 00:02]
     - The definition of a /type class/ is expressed with _a /parameterised type/
       with /abstract members/,_ such as a /trait/.

     - The main _DIFFERENCE_ between /subtype polymorphism/ and /ad-hoc polymorphism
       with type classes/ is how the definition of the /type class/ is implemented,
       in relation to the type it acts upon:
       + /Ad-hoc polymorphism with type classes/:
         the implementation for a /concrete type/, =from Jian= the TARGET type, is
         expressed through
         * a /given instance definition/, which is supplied as
           an /implicit argument/ alongside the value it acts upon.

       + /Subtype polymorphism/:
         the implementation is *mixed INTO* the /parents of a class/, and ONLY a
         SINGLE term is required to perform a polymorphic operation.

     - Compare the application of the /subtype polymorphism/ and /ad-hoc polymorphism
       with type classes/:
       + /ad-hoc polymorphism with type classes/
         * take more effort to set up,
         * _BUT_ is more extensible.

       + /subtype polymorphism/:
         * add a new /interface/ to a /class/ requires *changing the source code
           of that /class/.* --
           =from Jian= usually we don't want to change the source code frequently!

     - To conclude, we have seen that
       /traits/ and /given instances/, combined with other _constructs_ like
       /extension methods/, /context bounds/, and /type lambdas/ allow a *concise*
       and *natural* expression of /type classes/.
       + =from Jian=
         From the /reader monad/ example, you can see without /type lambdas/, its
         representation will be not *natural* -- let the type simulate the view
         of /function/.

*** DONE Type Class Derivation - =TODO=
    CLOSED: [2020-07-12 Sun 23:07]
    /Type class derivation/ is a way to *automatically* GENERATE /given instances/
    for /type classes/ which satisfy some simple conditions.

    - A /type class/ in this sense is *ANY* /trait/ or /class/ with *one* /type
      parameter/ determining the type being operated on.

    - Common examples of /type class/ are ~Eq~, ~Ordering~, or ~Show~.

    - For example, given the following ~Tree~ algebraic data type (ADT) with a
      ~dervies~ clause,
      #+begin_src scala
        enum Tree[T] derives Eq, Ordering, Show {
          case Branch(left: Tree[T], right: Tree[T])
          case Left(elem: T)
        }
      #+end_src
      + _The ~derives~ clause_ *generates* the following /given instances/ for the
        ~Eq~, ~Ordering~ and ~Show~ /type classes/ _in the /companion object/ of ~Tree~,_
        #+begin_src scala
          given [T: Eq]       as Eq[Tree[T]]    = Eq.derived
          given [T: Ordering] as Ordering[Tree] = Ordering.derived
          given [T: Show]     as Show[Tree]     = Show.derived
        #+end_src

      + We say that
        * ~Tree~ is the /deriving type/
        * the ~Eq~, ~Ordering~ and ~Show~ /given instances/ are /derived instances/.

**** DONE Types supporting ~derives~ clauses - =TODO= _NOT stable in details_
     CLOSED: [2020-07-12 Sun 22:25]
     *ALL* data types CAN HAVE _a ~derives~ clause_.

     - This document _FOCUSES PRIMARILY_ on data /types/ which also have a /given
       instance/ of the ~Mirror~ /type class/ available.
       + =from Jian=
         Reason of this document forcuses on the way of implementing ~derives~
         with ~Mirror~:
         ~Mirror~ is a structure, in the standard library, that is designed as an
         auxiliary to help implementing ~derives~
         * Use ~Mirror~ is *NOT the ONLY way* to implement ~derives~.

         * Use ~Mirror~ is considered the simplest and easist way to implement
           ~derives~. It uses less _advanced features_ of Scala 3.

     - /Instances/ of the ~Mirror~ /type class/ are generated *AUTOMATICALLY* by
       the compiler for,
       + /enums/ and /enum cases/
       + /case classes/ and /case objects/
       + /sealed classes or traits/ _that have *ONLY* /case classes/ and /case
         objects/ as children_

     - ~Mirror~ /type class instances/ provide
       + information at the _type level_ about the components and labelling of the /type/.
       + minimal _term level_ infrastructure
         to allow higher level libraries to provide comprehensive derivation support.
         =from Jian= Check libraries like _shapeless 3_.

     - ~Mirror~ /type class/ definition
       #+begin_src scala
         sealed trait Mirror {

           /** The mirrored *-type */
           type MirroredMonoType

           /** The name of the type */
           type MirroredLabel <: String

           /** The names of the elements of the type */
           type MirroredElemLabels <: Tuple
         }

         object Mirror {

           /** The Mirror for a product type */
           trait Product extends Mirror {
             /** Create a new instance of type `T` with elements taken from product `p`. */
             def fromProduct(p: scala.Product): MirroredMonoType
           }

           trait Sum extends Mirror { self =>
             /** The ordinal number of the case class of `x`. For enums, `ordinal(x) == x.ordinal` */
             def ordinal(x: MirroredMonoType): Int
           }

           trait Singleton extends Product {
             type MirroredMonoType = this.type
             type MirroredType = this.type
             type MirroredElemTypes = EmptyTuple
             type MirroredElemLabels = EmptyTuple
             def fromProduct(p: scala.Product) = this
           }

           type Of[T] = Mirror {
             type MirroredType = T
             type MirroredMonoType = T
             type MirroredElemTypes <: Tuple
           }

           type ProductOf[T] = Mirror.Product {
               type MirroredType = T
               type MirroredMonoType = T
               type MirroredElemTypes <: Tuple
             }

           type SumOf[T] = Mirror.Sum {
             type MirroredType = T
             type MirroredMonoType = T
             type MirroredElemTypes <: Tuple
           }

         }
       #+end_src
       + ~Product~ types (i.e. /case classes and objects/, and /enum cases/) have
         /mirrors/ which are *subtypes* of ~Mirror.Product~.

       + ~Sum~ types (i.e. /sealed class/ or /traits with product children/, and
         /enums/) have /mirrors/ which are *subtypes* of ~Mirror.Sum~.

     - For the ~Tree~ ADT from above the following ~Mirror~ /instances/ will be
       *AUTOMATICALLY* provided by the compiler,
       =from Jian=
       Since the generated ~Mirror~ /instances/ are put in the /companion object/
       of ~Tree~, and this is the reason why we don't need the ~Tree~ qualifier in
       the following example.
       #+begin_src scala
         // Mirror for `Tree` -- `SumOf[Tree]`
         Mirror.Sum {
           type MirroredType = Tree[_]
           type MirroredElemTypes = (Branch[_], Leaf[_])
           type MirroredMonoType = Tree[_]
           type MirroredLabels = "Tree"
           type MirroredElemLabels = ("Branch", "Leaf")

           def ordinal(x: MirroredMonoType): Int = x match {
             case _: Branch[_] => 0
             case _: Leaf[_]   => 1
           }
         }

         // Mirror for `Branch` -- `ProductOf[Branch]`
         Mirror.Product {
           type MirroredType = Branch[_]
           type MirroredElemTypes = (Tree[_], Tree[_])
           type MirroredMonoType = Branch[_]
           type MirroredLabels = "Branch"
           type MirroredElemLabels = ("left", "right")

           def fromProduct(p: Product): MirroredMonoType =
             new Branch(...)
         }

         // Mirror for `Leaf` -- `ProductOf[Leaf]`
         Mirror.Product {
           type MirroredType = Leaf[_]
           type MirroredElemTypes = Tuple1[_]
           type MirroredMonoType = Leaf[_]
           type MirroredLabels = "Leaf"
           type MirroredElemLabels = Tuple1["elem"]

           def fromProduct(p: Product): MirroredMonoType =
             new Leaf(...)
         }
       #+end_src

     - Note the following properties of ~Mirror~ /types/,
       + Properties are encoded _using /types/ RATHER THAN /terms/._
         This means that
         * they have _no_ runtime footprint _unless_ used
         * they are a _compile time feature_ for use with Dotty's metaprogramming
           facilities.

       + The /kinds/ of ~MirroredType~ and ~MirroredElemTypes~ match the /kind/
         of the data type the /mirror/ is an /instance/ for.
         * This allows ~Mirror~'s to support /ADTs/ of *all* /kinds/.

       + There is NO DISTINCT /representation type/ for /sums/ or /products/
         (ie. there is no ~HList~ or ~Coproduct~ type as in Scala 2 versions of shapeless).
           Instead the collection of child types of a data type is represented by
         an ordinary, possibly parameterized, /tuple type/.
         * Dotty's metaprogramming facilities can be used to work with these /tuple
           types/ as-is, and _higher level libraries_ can be *built on top of them*.

         * =from Jian=
           It seems ~HList~ or ~Coproduct~ like structures are already partially
           (or minimalized) implemented and merged. Will see if the complete version
           of them can affect _type class derivation_!
           =TODO= =TODO= =TODO=

       + For both /product/ and /sum/ types, the elements of ~MirroredElemTypes~
         are arranged in *definition order* (i.e. ~Branch[T]~ precedes ~Leaf[T]~ in
         ~MirroredElemTypes~ for ~Tree~ because ~Branch~ is defined _BEFORE_ ~Leaf~
         in the source file).
         * This means that ~Mirror.Sum~ *differs* in this respect from /shapeless's
           generic representation/ for ADTs in Scala 2, where the constructors
           are *ordered alphabetically by name*.

       + The methods ~ordinal~ and ~fromProduct~ are defined in terms of
         ~MirroredMonoType~ which is the /type/ of /kind-*/ which is obtained from
         ~MirroredType~ by *wildcarding* its /type parameters/.

**** TODO Type classes supporting automatic deriving - =TODO=
     - A /trait/ or /class/ can appear in a /derives clause/ if its /companion
       object/ defines a /method/ named ~derived~.

     - The /signature/ and _implementation_ of a ~derived~ /method/ for a /type
       class/ ~TC[_]~ are arbitrary but it is typically of the following form,
       #+begin_src scala
         def derived[T](using Mirror.Of[T]): TC[T] = ...
       #+end_src

     - That is, the ~derived~ /method/ takes a /context parameter/ of (some /subtype/
       of) type ~Mirror~ which
       + _DEFINES_ the shape of the /deriving type/ ~T~,
         AND
       + _COMPUTES_ the /type class/ _implementation_ according to that shape.

       This is all that the provider of an ADT with a ~derives~ _clause_ has to
       know about the _derivation_ of a /type class instance/.

     - Note that ~derived~ /methods/ may
       + have /context ~Mirror~ parameters/ _INDIRECTLY_
         * e.g.
           by having a /context argument/ which in turn has a /context ~Mirror~
           parameter/

          OR  =TODO= =Fix-Doc=

       + NOT have /context ~Mirror~ parameters/ AT ALL
         * e.g.
           they might use some completely different user-provided mechanism, for
           instance using Dotty /macros/ or /runtime reflection/.

     - We expect that _(*direct* or *indirect*) ~Mirror~ based implementations_
       will be the most common and that is what this document emphasises.
       + _(*direct* or *indirect*) ~Mirror~ based implementations_ means ~derived~'s
         always have a ~Mirror~ /context parameter/ (*direct* or *indirect*).

     - /Type class/ authors will most likely use /higher level derivation/ or
       /generic programming libraries/ to implement ~derived~ methods.
       + An example of how a ~derived~ /method/ might be implemented using only
         the low level facilities described above and Dotty's general
         metaprogramming features is _provided BELOW_.
         1. It is not anticipated that /type class/ authors would normally
            implement a ~derived~ method in this way,

         2. however this walkthrough can be taken as a guide for authors of the
            /higher level derivation libraries/ that we expect typical /type class/
            authors will use (for a fully worked out example of such a library,
            see _shapeless 3_).

***** TODO How to write a type class ~derived~ method using low level mechanisms
      - The low-level method we will use to implement a type class derived method
        in this example exploits three new type-level constructs in Dotty:
        inline methods, inline matches, and implicit searches via summonInline
        or summonFrom. Given this definition of the Eq type class,
        #+begin_src scala
          trait Eq[T] {
            def eqv(x: T, y: T): Boolean
          }
        #+end_src
        we need to implement a method Eq.derived on the companion object of Eq
        that produces a given instance for Eq[T] given a Mirror[T].

      - Here is a possible implementation,
        #+begin_src scala
          inline given derived[T](using m: Mirror.Of[T]) as Eq[T] = {
            val elemInstances = summonAll[m.MirroredElemType]          // (1)
            inline m match {                                           // (2)
              case s: Mirror.SumOf[T]     => eqSum(s, elemInstances)
              case p: Mirror.ProductOf[T] => eqProduct(p, elemInstances)
            }
          }
        #+end_src

      - Note that derived is defined as an inline given. This means that the method
        will be expanded at call sites (for instance the compiler generated
        instance definitions in the companion objects of ADTs which have a
        derived Eq clause), and also that it can be used recursively if
        necessary, to compute instances for children.

      - The body of this method (1) first materializes the Eq instances for all
        the child types of type the instance is being derived for. This is
        either all the branches of a sum type or all the fields of a product
        type. The implementation of summonAll is inline and uses Dotty's
        summonInline construct to collect the instances as a List,
        #+begin_src scala
          inline def summonAll[T <: Tuple]: List[Eq[_]] = inline erasedValue[T] match {
            case _: Unit      => Nil
            case _: (t *: ts) => summonInline[Eq[t]] :: summonAll[ts]
          }
        #+end_src
        with the instances for children in hand the derived method uses an
        inline match to dispatch to methods which can construct instances for
        either sums or products (2). Note that because derived is inline the
        match will be resolved at compile-time and only the left-hand side of
        the matching case will be inlined into the generated code with types
        refined as revealed by the match.

      - In the sum case, eqSum, we use the runtime ordinal values of the arguments
        to eqv to first check if the two values are of the same subtype of the
        ADT (3) and then, if they are, to further test for equality based on the
        Eq instance for the appropriate ADT subtype using the auxiliary method
        check (4).
        #+begin_src scala
          def eqSum[T](s: Mirror.SumOf[T], elems: List[Eq[_]]): Eq[T] =
            new Eq[T] {
              def eqv(x: T, y: T): Boolean = {
                val ordx = s.ordinal(x)                            // (3)
                                    (s.ordinal(y) == ordx) && check(elems(ordx))(x, y) // (4)
              }
            }
        #+end_src

      - In the product case, ~eqProduct~ we test the runtime values of the arguments
        to ~eqv~ for equality as products based on the ~Eq~ instances for the fields
        of the data type (5),
        #+begin_src scala
          def eqProduct[T](p: Mirror.ProductOf[T], elems: List[Eq[_]]): Eq[T] =
            new Eq[T] {
              def eqv(x: T, y: T): Boolean =
                iterator(x).zip(iterator(y)).zip(elems.iterator).forall {  // (5)
                  case ((x, y), elem) => check(elem)(x, y)
                }
            }
        #+end_src

      - Pulling this all together we have the following complete implementation,
        #+begin_src scala
          import scala.deriving._
          import scala.compiletime.{erasedValue, summonInline}

          inline def summonAll[T <: Tuple]: List[Eq[_]] = inline erasedValue[T] match {
            case _: Unit => Nil
            case _: (t *: ts) => summonInline[Eq[t]] :: summonAll[ts]
          }

          trait Eq[T] {
            def eqv(x: T, y: T): Boolean
          }

          object Eq {
            given Eq[Int] {
              def eqv(x: Int, y: Int) = x == y
            }

            def check(elem: Eq[_])(x: Any, y: Any): Boolean =
              elem.asInstanceOf[Eq[Any]].eqv(x, y)

            def iterator[T](p: T) = p.asInstanceOf[Product].productIterator

            def eqSum[T](s: Mirror.SumOf[T], elems: List[Eq[_]]): Eq[T] =
              new Eq[T] {
                def eqv(x: T, y: T): Boolean = {
                  val ordx = s.ordinal(x)
                  (s.ordinal(y) == ordx) && check(elems(ordx))(x, y)
                }
              }

            def eqProduct[T](p: Mirror.ProductOf[T], elems: List[Eq[_]]): Eq[T] =
              new Eq[T] {
                def eqv(x: T, y: T): Boolean =
                  iterator(x).zip(iterator(y)).zip(elems.iterator).forall {
                    case ((x, y), elem) => check(elem)(x, y)
                  }
              }

            inline given derived[T](using m: Mirror.Of[T]) as Eq[T] = {
              val elemInstances = summonAll[m.MirroredElemTypes]
              inline m match {
                case s: Mirror.SumOf[T]     => eqSum(s, elemInstances)
                case p: Mirror.ProductOf[T] => eqProduct(p, elemInstances)
              }
            }
          }
        #+end_src
        we can test this relative to a simple ADT like so,
        #+begin_src scala
          enum Opt[+T] derives Eq {
            case Sm(t: T)
                case Nn
          }

          object Test extends App {
            import Opt._
            val eqoi = summon[Eq[Opt[Int]]]
            assert(eqoi.eqv(Sm(23), Sm(23)))
            assert(!eqoi.eqv(Sm(23), Sm(13)))
            assert(!eqoi.eqv(Sm(23), Nn))
          }
        #+end_src

      - In this case the code that is generated by the inline expansion for the
        derived Eq instance for Opt looks like the following, after a little
        polishing,
        #+begin_src scala
          given derived$Eq[T](using eqT: Eq[T]) as Eq[Opt[T]] =
            eqSum(summon[Mirror[Opt[T]]],
                  List(
                    eqProduct(summon[Mirror[Sm[T]]], List(summon[Eq[T]]))
                      eqProduct(summon[Mirror[Nn.type]], Nil)
                  )
            )
        #+end_src

      - Alternative approaches can be taken to the way that ~derived~ methods can
        be defined. For example, more aggressively inlined variants using Dotty
        macros, whilst being more involved for type class authors to write than
        the example above, can produce code for type classes like Eq which
        eliminate all the abstraction artefacts (eg. the ~Lists~ of child
        instances in the above) and generate code which is indistinguishable
        from what a programmer might write by hand. As a third example, using a
        higher level library such as shapeless the type class author could
        define an equivalent ~derived~ method as,
        #+begin_src scala
          given eqSum[A](using inst: => K0.CoproductInstances[Eq, A]) as Eq[A] {
            def eqv(x: A, y: A): Boolean = inst.fold2(x, y)(false)(
              [t] => (eqt: Eq[t], t0: t, t1: t) => eqt.eqv(t0, t1)
            )
          }

          given eqProduct[A](using inst: K0.ProductInstances[Eq, A]) as Eq[A] {
            def eqv(x: A, y: A): Boolean = inst.foldLeft2(x, y)(true: Boolean)(
              [t] => (acc: Boolean, eqt: Eq[t], t0: t, t1: t) => Complete(!eqt.eqv(t0, t1))(false)(true)
            )
          }

          inline def derived[A](using gen: K0.Generic[A]) as Eq[A] = gen.derive(eqSum, eqProduct)
        #+end_src

      - The framework described here
        ENABLES _all three_ of these approaches
        WITHOUT MANDATING any of them.

      - For a brief discussion on how to use /macros/ to write a /type class
        derived method/ please read more at
        [[https://dotty.epfl.ch/docs/reference/contextual/derivation-macro.html][How to write a type class derived method using macros]].

**** DONE Deriving instances elsewhere
     CLOSED: [2020-07-12 Sun 22:47]
     Sometimes one would like to
     *derive* a /type class instance/ for an ADT *after the ADT is defined*,
     WITHOUT being able to change the code of the ADT itself.

     - To do this, simply define an /instance/ using the ~derived~ /method/ of
       the /type class/ as right-hand side.

     - E.g, to implement ~Ordering~ for ~Option~ define,
       #+begin_src scala
         given [T: Ordering] as Ordering[Option[T]] = Ordering.derived
       #+end_src

     - Assuming the ~Ordering.derived~ /method/ has a /context parameter/ of /type/
      ~Mirror[T]~, it will be satisfied by
      + the compiler generated ~Mirror~ /instance/ for ~Option~
        AND
      + the /derivation/ of the /instance/ will be _EXPANDED_ on the RHS of this
        definition in _the same way as_ an /instance/ DEFINED in ADT /companion
        objects/.

**** DONE Syntax
     CLOSED: [2020-07-12 Sun 22:47]
**** DONE Discussion
     CLOSED: [2020-07-12 Sun 23:06]
     - This _/type class/ derivation framework_ is *INTENTIONALLY* very *small* and
       *low-level*.

     - There are essentially *TWO* pieces of infrastructure in *compiler-generated
       ~Mirror~ instances*,
       + /type members/ encoding properties of the /mirrored types/.

       + a MINIMAL _value level_ mechanism for working generically with /terms/
         of the /mirrored types/.

     - The ~Mirror~ infrastructure _can be seen as_ an /extension/ of the existing
       ~Product~ infrastructure for /case classes/:
       typically ~Mirror~ types will be implemented by the ADTs /companion object/,
       hence the /type members/ and the ~ordinal~ or ~fromProduct~ /methods/ will
       be members of that object.

       + The primary motivation for this design decision, and the decision to encode
         properties via /types/ rather than /terms/:
         to keep the /bytecode and runtime footprint/ of the feature *small enough*
         to make it possible to provide ~Mirror~ instances *unconditionally*.

     - Whilst ~Mirrors~ encode properties precisely via /type members/, the _value
       level_ ~ordinal~ and ~fromProduct~ are somewhat _weakly typed_ (because they
       are defined in terms of ~MirroredMonoType~) just like the members of ~Product~.
       - This means that
         code for /generic type classes/ has to _ENSURE_ that
         1. /type exploration/ and /value selection/ PROCEED in lockstep
            AND
         2. 1. has to assert this conformance in some places using /casts/.
            * If /generic type classes/ are *correctly* written these /casts/ will
              *never fail*.

     - As mentioned, however,
       + the compiler-provided mechanism is _INTENTIONALLY very low level_
         AND
       + it is ANTICIPATED that
         /higher level type class derivation/ and /generic programming/
         _libraries_ will build on this and Dotty's other metaprogramming
         facilities
         * PURPOSE:
           to *hide* these low-level details *from* /type class/ authors and
           general users.

     - _/Type class/ derivation_ in the style of both _shapeless_ and _Magnolia_
       are possible (a prototype of shapeless 3, which combines aspects of both
       shapeless 2 and Magnolia has been developed alongside this language feature)
       as is a _MORE AGGRESSIVELY /inlined style/,_ supported by Dotty's new
       _quote/splice macro_ and _inlining_ facilities.

*** DONE Multiversal Equality - =TODO= =RE-READ=
    CLOSED: [2020-05-23 Sat 23:31]
    - /Universal equality/ is *convenient*.
      _BUT_ it is also dangerous since it *undermines* /type safety/.

    - /Multiversal equality/ is an _opt-in way_ to make /universal equality/ SAFER.
        It uses a /binary type class/ ~Eql~ to indicate that values of *two* given
      /types/ can be compared with each other.

    - If we want to disable /universal equality/ check for ~T~, we can do
      #+begin_src scala
        class T derives Eql
      #+end_src
      Then if we compare an object of ~T~ with the other /types/, the error can
      be catched:
      #+begin_src scala
        val x = ...  // of type T
        val y = ...  // of type S, but should be T
        x == y       // can't typecheck because T drevies the type class Eql
      #+end_src

    - Alternatively, one can also provide an ~Eql~ /given instance/ directly,
      like this:
      #+begin_src scala
        given Eql[T, T] = Eql.derived
      #+end_src
      This definition effectively says that values of /type/ ~T~ can (only) be compared
      to other values of type ~T~ when using ~==~ or ~!=~.
      + The definition
        * _affects_ /type checking/
        * BUT it has _no significance_ for /runtime/ behavior (=from Jian= GOOD!!!),
          since
          - ~==~ always maps to ~equals~
          - ~!=~ always maps to the negation of ~equals~

      + The right hand side ~Eql.derived~ of the definition is a value that has
        any ~Eql~ instance as its type.

      + Here is the definition of /class/ ~Eql~ and its /companion object/:
       #+begin_src scala
         package scala
         import annotation.implicitNotFound

         @implicitNotFound("Values of types ${L} and ${R} cannot be compared with == or !=")
         sealed trait Eql[-L, -R]

         object Eql {
           object derived extends Eql[Any, Any]
         }
       #+end_src

    - One can have *several* ~Eql~ /given instances/ for *one* /type/.
      + Example:
        If we define
        #+begin_src scala
          given Eql[A, A] = Eql.derived
          given Eql[B, B] = Eql.derived
          given Eql[A, B] = Eql.derived
          given Eql[B, A] = Eql.derived
        #+end_src
        , then only values of type ~A~ can be compared with values of type ~B~.

    - The ~scala.Eql~ object defines a number of ~Eql~ /given instances/ that
      together define a rule book for what /standard types/ can be compared.
      =from Jian= More details in the section "Predefined ~Eql~ Instances".

    - For *backward compatibility*,
      There's also a *"FALLBACK"* /instance/ named ~eqlAny~ that allows comparisons
      over *ALL* /types/ that do *NOT themselves have an ~Eql~ /given/.*
      + The *primary motivation* for having ~eqlAny~ is _backwards compatibility_.
        If no concern, on can disable ~eqlAny~ by enabling the language feature
        *strictEquality* by:
        * Command line option: =-language:strictEquality=
        * imports: ~import scala.language.strictEquality~

      + ~eqlAny~ is defined as follows:
        #+begin_src scala
          def eqlAny[L, R]: Eql[L, R] = Eql.derived
        #+end_src

      + Even though ~eqlAny~ is _NOT_ declared a ~given~,
        the compiler will *still* construct an ~eqlAny~ instance as answer to an
        /implicit search/ for the type ~Eql[L, R]~, _UNLESS_:
        * ~L~ or ~R~ have ~Eql~ instances defined on them,
          OR
        * the language feature ~strictEquality~ is _enabled_

**** DONE Deriving ~Eql~ Instances
     CLOSED: [2020-07-20 Mon 01:35]
     #+begin_src scala
       class Box[T](x: T) derives Eql
     #+end_src
     - By the usual rules of /type class derivation/, this _generates_ the following
       ~Eql~ /instance/ in the /companion object/ of ~Box~:
       #+begin_src scala
         given [T, U](using Eql[T, U]) as Eql[Box[T], Box[U]] = Eql.derived
       #+end_src

     - Examples:
       #+begin_src scala
         new Box(1) == new Box(1L)   // ok since there is an instance for `Eql[Int, Long]`
         new Box(1) == new Box("a")  // error: can't compare
         new Box(1) == 1             // error: can't compare
       #+end_src
       =from Jian=
       See next subsection to know WHY "there is an instance for ~Eql[Int, Long]~"

**** DONE Precise Rules for Equality Checking - =TODO= _Verify my understanding to rule 2!!!_
     CLOSED: [2020-07-20 Mon 01:53]
     - If the ~strictEquality~ feature is enabled then a comparison using
       ~x \equal{}\equal{} y~ or ~x != y~ between values ~x: T~ and ~y: U~ is _legal iff there
       is a given of type ~Eql[T, U]~._
       + =from Jian=
         This doc use "if" in ths paragraph, and in this note I replace it with
         "iff", which I think is better because it's right and more strict!

     - In the default case where the ~strictEquality~ feature is _NOT enabled_
       the comparison is also legal if
       1. ~T~ and ~U~ are the _same_
          OR
       2. one of ~T~, ~U~ is a /subtype/ of the /lifted version of the other type/,
          OR
       3. neither ~T~ nor ~U~ have a reflective ~Eql~ instance.

     - Explanations:
       + /lifting/ a type ~S~ means
         * *replacing* ALL /references/ to /abstract types/ in /covariant positions/
           of ~S~ by _their /upper bound/,_

           AND

         * *replacing* ALL /refinement types/ in /covariant positions/ of ~S~ by
           _their parent_.

       + a /type/ ~T~ has a _reflexive_ ~Eql~ /instance/
         if the _implicit search_ for ~Eql[T, T]~ succeeds.

     - =from Jian= =TODO= =TODO=
       + Why ~Eql[-T, -U]~ is /contravariant/ for ~T~ or ~U~.

       + I don't quite understand the rule 2.

       + My understanding to rule 2 is:
         ~S[+X]~ is the definition.
         ~U <: A~ and ~T <: S[A]~.
         ~U~ and ~T~ can be compared.

       + In the "Explanations" above:
         * Q :: WHY "covariant"? WHY NOT "contravariant" or "invariant"?

**** DONE Predefined ~Eql~ Instances
     CLOSED: [2020-07-20 Mon 02:28]
     - The ~Eql~ object defines instances for comparing
       + the /primitive types/
         * ~Byte~
         * ~Short~
         * ~Char~
         * ~Int~
         * ~Long~
         * ~Float~
         * ~Double~
         * ~Boolean~
         * ~Unit~,

       + ~java.lang.Number~
       + ~java.lang.Boolean~
       + ~java.lang.Character~

       + ~scala.collection.Seq~
       + ~scala.collection.Set~

     - Instances are defined so that *every* one of the /types/ mentioned above
       has /a *reflexive* ~Eql~ instance/, and the following holds:
       + /Primitive numeric types/ can be compared with _each other_.

       + /Primitive numeric types/ can be compared with *subtypes* of
         ~java.lang.Number~ (and _vice versa_).

       + ~Boolean~ can be compared with ~java.lang.Boolean~ (and _vice versa_).

       + ~Char~ can be compared with ~java.lang.Character~ (and _vice versa_).

       + Two /sequences/ (of *arbitrary subtypes* of ~scala.collection.Seq~) can
         be compared with _each other_ *if their element types can be compared.*
         The two sequence types need not be the same.

       + Two /sets/ (of *arbitrary subtypes* of ~scala.collection.Set~) can be
         compared with _each other_ *if their element types can be compared.*
         The two set types need not be the same.

       + Any /subtype/ of ~AnyRef~ can be compared with ~Null~ (and _vice versa_).

     - =from Jian=
       /a *reflexive* ~Eql~ instance/ for each /types/ mentioned above means
       any type ~A~ that is mentioned above *can't be compared* to any type ~B~
       if ~Eql[A, B]~ is *not* defined. This is true even when ~strictEquality~ is
       disabled. This is mentioned in the above subsection "Precise Rules for
       Equality Checking".
       + When ~strictEquality~ is *enabled*,
         if there is no ~Eql[A, B]~, values of ~A~ and ~B~ can't be compared.

       + When ~strictEquality~ is *turned off*,
         if there is NO ~Eql[A, B]~, because of the /*reflexive* ~Eql~ instance/
         and *rule 3*, ~A~ and ~B~ can't be compared.
         * *Note*:
           In this case, when there is no /*reflexive* ~Eql~ instance/, even
           without ~Eql[A, B]~, values of ~A~ and ~B~ can be compared because of
           ~eqlAny~.

**** TODO Why Two Type Parameters? - =START=
     - One particular feature of the Eql type is that it takes two type parameters,
       representing the types of the two items to be compared. By contrast,
       conventional implementations of an equality type class take only a single
       type parameter which represents the common type of both operands. One
       type parameter is simpler than two, so why go through the additional
       complication? The reason has to do with the fact that, rather than coming
       up with a type class where no operation existed before, we are dealing
       with a refinement of pre-existing, universal equality. It is best
       illustrated through an example.

     - Say you want to come up with a safe version of the ~contains~ /method/ on
       ~List[T]~. The original definition of ~contains~ in the standard library was:
       #+begin_src scala
         class List[+T] {
           // ...
           def contains(x: Any): Boolean
         }
       #+end_src
       1. That uses /universal equality/ in an *unsafe* way
          since it permits arguments of *any* /type/ to be compared with the
          list's elements.

       2. The "obvious" alternative definition
          #+begin_src scala
            class List[+T] {
              // ...
              def contains(x: T): Boolean
            }
          #+end_src
          *not* work, since it refers to the /covariant parameter/ ~T~ in a
          /nonvariant/ context.

       3. The only variance-correct way to use the /type parameter/ ~T~ in contains
          is as a /lower bound/:
          #+begin_src scala
            class List[+T] {
              // ...
              def contains[U >: T](x: U): Boolean
            }
          #+end_src
          This /generic version/ of ~contains~ is the one used in the current
          (Scala 2.13) version of ~List~. It looks different but it admits exactly
          the same applications as the ~contains(x: Any)~ definition we started with.

       4. Make it _more useful (i.e. restrictive)_ by adding an ~Eql~ parameter:
          #+begin_src scala
            class List[+T] {
              // ...
              def contains[U >: T](x: U)(using Eql[T, U]): Boolean
            }
          #+end_src
          This version of ~contains~ is *equality-safe*!
          More precisely, given ~x: T~, ~xs: List[T]~ and ~y: U~, then ~xs.contains(y)~
          is type-correct iff ~x == y~ is type-correct.

       5. Unfortunately, the crucial ability to "lift" equality type checking from
          simple equality and pattern matching to arbitrary user-defined
          operations gets lost if we restrict ourselves to an equality class
          with a single type parameter. Consider the following signature of
          contains with a hypothetical Eql1[T] type class:
          #+begin_src scala
            def contains[U >: T](x: U)(using Eql1[U]): Boolean
          #+end_src

       6. This version could be applied just as widely as the original ~contains(x: Any)~
          method, since the ~Eql1[Any]~ fallback is always available! So we have gained
          nothing. What got lost in the transition to a single parameter /type
          class/ was the original rule that ~Eql[A, B]~ is available only if
          neither ~A~ nor ~B~ have a reflexive ~Eql~ /instance/. That rule simply
          cannot be expressed if there is a single type parameter for ~Eql~.

       7. The situation is different under ~-language:strictEquality~.
          In that case, the ~Eql[Any, Any]~ or ~Eql1[Any]~ /instances/ would
          *never be available*, and both the single and two-parameter versions
          would indeed _coincide_ for most practical purposes.
          + =from Jian=
            Why only *most* practical purposes, not ALL???

       8. But assuming ~-language:strictEquality~ immediately and everywhere poses
          migration problems which might well be unsurmountable.
          Consider again ~contains~, which is in the standard library.
          + Parameterizing it with the ~Eql~ /type class/ as in 1) is an immediate
            win since it rules out non-sensical applications while still allowing
            all sensible ones. So it can be done almost at any time, modulo binary
            compatibility concerns.

          + On the other hand, parameterizing ~contains~ with ~Eql1~ as in would
            make ~contains~ unusable for all /types/ that have not yet declared an
            ~Eql1~ /instance/, including all types coming from Java. This is clearly
            unacceptable. It would lead to a situation where, rather than
            migrating existing libraries to use safe equality, the only upgrade
            path is to have parallel libraries, with the new version only
            catering to types deriving ~Eql1~ and the old version dealing with
            everything else. Such a split of the ecosystem would be very
            problematic, which means the cure is likely to be worse than the
            disease.

       9. For these reasons, it looks like a two-parameter type class is the only
          way forward because it can take the existing ecosystem where it is and
          migrate it towards a future where more and more code uses safe
          equality.

       10. In applications where ~-language:strictEquality~ is the default one could
           also introduce a *one-parameter type alias* such as
           #+begin_src scala
             type Eq[-T] = Eql[T, T]
           #+end_src
           Operations needing safe equality could then use this alias instead of
           the _two-parameter ~Eql~ class_. But it would *only work under
           ~-language:strictEquality~,* since otherwise the universal ~Eq[Any]~
           instance would be available everywhere.

       11. More on /multiversal equality/ is found in a [[https://www.scala-lang.org/blog/2016/05/06/multiversal-equality.html][blog post]] and a [[https://github.com/lampepfl/dotty/issues/1247][GitHub issue]].

*** DONE Context Functions - =TODO= =RE-READ=
    CLOSED: [2020-07-14 Tue 02:39]
    - Context functions :: functions with *ONLY* /context parameters/.
      + Their /types/ are /context function types/.
      + /Context functions/ are written using ~?=>~ as the "arrow" sign, which is
        different from /Non-context functions/.
      + Example:
        #+begin_src scala
          type Executable[T] = ExecutionContext ?=> T

          given ec as ExecutionContext = ...

          def f(x: Int): ExecutionContext ?=> Int = ...
          // could be written as follows with the type alias from above
          // def f(x: Int): Executable[Int] = ...

          f(2)(using ec)  // explicit argument
          f(2)            // argument is inferred
        #+end_src

    - Conversely,
      + IF ::
        * An expression ~e~ show up in a position that the *EXPECTED* /type/ should
          be a /context function type/ ~(T_1, ..., T_n) ?=> U~

        * ~e~ *is NOT ALREADY* an /context function literal/ with the /context type/
          ~(T_1, ..., T_n) ?=> U~

      + THEN ::
        ~e~ *is converted to* a /context function literal/ by *rewriting it to*
        #+begin_src scala
          (using x_1: T1, ..., x_n: Tn) => e
        #+end_src
        where the NAMES ~x_1, ..., x_n~ are ARBITRARY.
        * This expansion is performed *before* the expression ~e~ is typechecked,
          which means _that ~x_1, ..., x_n~ are available as /givens/ that ~e~
          can use._

        * =from Jian= CAUTION:
          ~e~ itself doesn't need be of type ~U~.
          HOWEVER, ~e~ combine with ~x_1, ..., x_n~ must be of type ~U~.
          + For example, _continuing_ with the previous definitions:
            #+begin_src scala
              def g(arg: Executable[Int]) =  // ...

              g(22)    // is expanded to g((using ev: ExecutionContext) => 22)
              g(f(2))  // is expanded to g((using ev: ExecutionContext) => f(2)(using ev))

              g(ExecutionContext ?=> f(3))  // is expanded to g((using ev: ExecutionContext) => f(3)(using ev))
              g((using ctx: ExecutionContext) => f(22)(using ctx))  // is left as it is
            #+end_src

**** DONE Example: Builder Pattern - =TODO= RE-READ
     CLOSED: [2020-07-14 Tue 00:02]
     /Context function types/ have considerable *EXPRESSIVE power*.

     - For instance,
       here is how they can _support the "builder pattern",_ where the aim is to
       construct tables like this:
       #+begin_src scala
         table {
           row {
             cell("top left")
             cell("top right")
           }
           row {
             cell("bottom left")
             cell("bottom right")
           }
         }
       #+end_src

     - The idea is to define /classes/ for ~Table~ and ~Row~ that allow the addition
       of elements via ~add~:
       #+begin_src scala
         class Table {
           val rows = new ArrayBuffer[Row]
           def add(r: Row): Unit = rows += r
           override def toString = rows.mkString(start = "Table(", sep = ", ", end = ")")
         }

         class Row {
           val cells = new ArrayBuffer[Cell]
           def add(c: Cell): Unit = cells += c
           override def toString = cells.mkString(start = "Row(", sep = ", ", end = ")")
         }

         case class Cell(elem: String)
       #+end_src

     - Then, the ~table~, ~row~ and ~cell~ /constructor methods/ can be defined
       _with /context function types/ as parameters_ to *AVOID* the plumbing
       boilerplate that would otherwise be necessary.
       #+begin_src scala
         def table(init: Table ?=> Unit) = {
           given t as Table // note the use of a creator application; same as: given t as Table = new Table
           init
           t
         }

         def row(init: Row ?=> Unit)(using t: Table) = {
           given r as Row
           init
           t.add(r)
         }

         def cell(str: String)(using r: Row) =
           r.add(new Cell(str))
       #+end_src

     - With that setup, the _table construction_ code above compiles and _expands to_:
       #+begin_src scala
         table { (using $t: Table) =>

           row { (using $r: Row) =>
             cell("top left")(using $r)
             cell("top right")(using $r)
           }(using $t)

           row { (using $r: Row) =>
             cell("bottom left")(using $r)
             cell("bottom right")(using $r)
           }(using $t)
         }
       #+end_src

**** DONE Example: Postconditions - =TODO= RE-READ
     CLOSED: [2020-07-14 Tue 02:39]
     Define constructs for _checking arbitrary postconditions_ using an /extension
     method/ *ensuring* so that the checked result can be referred to simply by
     ~result~.
     #+begin_src scala
       object PostConditions {
         opaque type WrappedResult[T] = T

         def result[T](using r: WrappedResult[T]): T = r

         extension [T](x: T)
           def ensuring(condition: WrappedResult[T] ?=> Boolean): T = {
             assert(condition(using x))
             x
           }
       }

       import PostConditions.{ensuring, result}

       val s = List(1, 2, 3).sum.ensuring(result == 6)
     #+end_src
     - The example combines
       + /opaque type aliases/
       + /context function types/
       + /extension methods/

     - Combine the structures above can create a *zero-overhead abstraction*.

     - Explanations:
       + We use a /context function type/ ~WrappedResult[T] ?=> Boolean~ as the
         /type/ of the ~condition~ of ~ensuring~.

       + An argument to ~ensuring~ such as (~result == 6~) will therefore have a
         /given/ of /type/ ~WrappedResult[T]~ _in scope_ to pass along to the
         ~result~ /method/.

       + ~WrappedResult~ is a *FRESH* /type/,
         to make sure that we do *NOT* get _unwanted /givens/ in scope_ (this is
         good practice in all cases where /context parameters/ are involved).

       + Since ~WrappedResult~ is an /opaque type alias/, its values _need NOT be
         boxed_,
         AND
         since ~ensuring~ is added as an /extension method/, its argument does *not*
         need boxing either.

       + Hence, the implementation of ~ensuring~ is as about _as *efficient* as the
         best possible code one could write by hand_:
         #+begin_src scala
           {
             val result = List(1, 2, 3).sum
             assert(result == 6)
             result
           }
         #+end_src
         * =from Jian= =TODO= =Verify=
           Inspect the function calls to ~result~ and ~condition~ (after inserting
           required /using clauses/) that _can't be inlined_, comparing to this write
           by hand code, there are *two extra function call cost* in the /context
           function/ implementation.

**** TODO Reference

*** DONE Implicit Conversions
    CLOSED: [2020-03-11 Wed 00:14]
    /Implicit conversions/ are defined by /given instances/ of the
    ~scala.Conversion~ class.

    - ~scala.Conversion~ class is defined in package scala as follows:
      #+begin_src scala
        abstract class Conversion[-T, +U] extends (T => U)
      #+end_src

    - Example:
      #+begin_src scala
        given Conversion[String, Token] {
          def apply(str: String): Token = new KeyWord(str)
        }
      #+end_src
      + Express more concisely as:
        #+begin_src scala
          given Conversion[String, Token] = new KeyWord(_)
        #+end_src

    - An /implicit conversion/ is applied automatically by the compiler in _THREE_
      situations:
      + _Type_ doesn't match, but an after an /implicit conversion/, type can match.

      + _Method Name_ doesn't match, but an after an /implicit conversion/, method
        can be found.

      + _Method Name matches, but Method Signature doesn't match_, but an after
        an /implicit conversion/, /method signature/ can match.

**** Examples
     1. In ~Predef~
        #+begin_src scala
          given int2Integer as Conversion[Int, java.lang.Integer] =
            java.lang.Integer.valueOf(_)
        #+end_src

     2. /Magnet pattern/ that use /implicit conversion/:
        #+begin_src scala
          object Completions {

            // The argument "magnet" type
            enum CompletionArg {
              case Error(s: String)
              case Response(f: Future[HttpResponse])
              case Status(code: Future[StatusCode])
            }

            object CompletionArg {

              // conversions defining the possible arguments to pass to `complete`
              // these always come with CompletionArg
              // They can be invoked explicitly, e.g.
              //
              //   CompletionArg.fromStatusCode(statusCode)

              given fromString     as Conversion[String, CompletionArg]               = Error(_)
              given fromFuture     as Conversion[Future[HttpResponse], CompletionArg] = Response(_)
              given fromStatusCode as Conversion[Future[StatusCode], CompletionArg]   = Status(_)
            }

            import CompletionArg._

            def complete[T](arg: CompletionArg) = arg match {
              case Error(s)     => ...
              case Response(f)  => ...
              case Status(code) => ...
            }
          }
        #+end_src
        + =from Jian= Why does ~complete~ have a /type parameter/ ~T~.

        + This setup is more complicated than simple overloading of ~complete~ (the
          traditional way of implementing the /magnet pattern/),
          BUT it can still be useful
          * *if normal /overloading/ is not available* (as in the case above, since
            we cannot have two overloaded methods that take ~Future[...]~ arguments),
            =from Jian= ??? /Type erasure/ ???
            _OR_
          * if normal overloading would lead to a _combinatorial explosion of variants_.

*** DONE By-Name Context Parameters - =TODO= =RE-READ=
    CLOSED: [2020-05-23 Sat 00:01]
    =from Jian= This section discussion the /LAZY context parameters/.

    - /Context parameters/ can be DECLARED /by-name/ to *avoid* a /divergent inferred
      expansion/.

    - Example:
      #+begin_src scala
        trait Codec[T] {
          def write(x: T): Unit
        }

        given intCodec as Codec[Int] = ???

        given optionCodec[T](using ev: => Codec[T]) as Codec[Option[T]] {
          def write(xo: Option[T]) = xo match {
            case Some(x) => ev.while(x)
            case None    =>
          }

          // TODO: from Jian: can this work for "by-name context parameters"
          // def write(xo: Option[T]) =
          //  xo.map(ev.write)
        }

        val s = summon[Codec[Option[Int]]]

        s.write(Some(33))
        s.write(None)
      #+end_src
      + As is the case for a normal (non-context parameter) /by-name parameter/,
        the argument for the /context parameter/ ~ev~ is evaluated on demand.
          In the example above, if the ~xo~ is ~None~, it is *NOT* evaluated at all.

    - TODO ??? TODO -- =Try to understand this=
      The /synthesized argument/ for a /context parameter/
      is backed by a _LOCAL_ ~val~
      if this is necessary to prevent an otherwise /diverging expansion/.

    - The precise steps for /synthesizing an argument/ for a /by-name context
      parameter/ of type ~=> T~ are as follows: TODO ??? TODO
      1. Create a new /given/ of type ~T~:
         #+begin_src scala
           given lv as T = ???
         #+end_src
         where ~lv~ is an arbitrary fresh name.

      2. This /given/ is not immediately available as candidate for argument
         inference (making it immediately available could result in a loop in
         the synthesized computation). But it becomes available in all nested
         contexts that look again for an argument to a /by-name context parameter/.

      3. If this search succeeds with expression ~E~, and ~E~ contains references
         to ~lv~, replace ~E~ by
         #+begin_src scala
           { given lv as T = E; lv }
         #+end_src
         Otherwise, return ~E~ unchanged.

    - In the example above, the definition of s would be *EXPANDED* as follows.
      #+begin_src scala
        val s = summon[Test.Codec[Option[Int]]](
          optionCodec[Int](using intCodec)
        )
      #+end_src
      /No local given instance/ was generated because _the /synthesized argument/
      is *not* /recursive/._

**** TODO Reference
     For more info, see
     - Issue _#1998: Let by-name implicit parameters have lazy semantics_
       and the associated
     - _SIP-NN - BYNAME IMPLICIT ARGUMENTS_.

*** DONE Relationship with Scala 2 Implicits
    CLOSED: [2020-07-14 Tue 03:52]
    Many, but *NOT all*, of the _Scala 3's NEW /contextual abstraction/ features_
    can be mapped to _Scala 2's /implicits/._

    This page gives a rundown on the relationships between new and old features.

**** DONE Simulating Scala 3 Contextual Abstraction Concepts with Scala 2 Implicits
     CLOSED: [2020-07-14 Tue 03:50]
***** DONE Given Intances
      CLOSED: [2020-07-14 Tue 02:58]
      - /Given instances/ can be mapped to _COMBINATIONS_ of /implicit objects/,
        /classes/ and /implicit methods/.
        1. /Given instances without parameters/ ---> /implicit objects/.
           #+begin_src scala
             // Dotty
             given intOrd as Ord[Int] { ... }

             // Scala 2
             implicit object IntOrd extends Ord[Int] { ... }
           #+end_src

        2. /Parameterized givens/ ---> COMBINATIONS of /classes/ and /implicit methods/.
           #+begin_src scala
             // Dotty
             given listOrd[T](using ord: Ord[T]) as Ord[List[T]] { ... }

             // Scala 2
             class ListOrd[T](implicit ord: Ord[T]) extends Ord[List[T]] { ... }
             final implicit def ListOrd[T](implicit ord: Ord[T]): ListOrd[T] = new ListOrd[T]
           #+end_src

        3. /Alias givens/ map to /implicit methods/ OR /implicit lazy vals/.
           =from Jian= Remember! /Alias givens/ won't eagerly evaluate its RHS value.
           =from Jian= Remember! Here the "alias" means assignment.
           #+begin_src scala
             // Dotty
             given global as ExecutionContext = new ForkJoinContext()

             val ctx: Context = ...
             given Context = ctx
           #+end_src

           would map to

           #+begin_src scala
             // Scala 2
             final implicit lazy val global: ExecutionContext = new ForkJoinContext()

             val ctx: Context = ...
             final implicit def given_Context = ctx
           #+end_src
           + If an alias has _NEITHER /type parameters/ NOR /context parameters/,_
             it is treated as a ~lazy val~,
             * unless the right hand side is a simple reference, in which case we
               can use a forwarder to that reference *WITHOUT CACHING it*.

***** DONE Anonymous Given Intances
      CLOSED: [2020-07-14 Tue 03:07]
      /Anonymous given instances/ get *compiler synthesized* NAMES, which are
      generated _in a reproducible way FROM the implemented type(s)._
      - =from Jian=
        The overview above actually means Scala 2 doesn't have this feature,
        BUT Scala 2 to can _SIMULATE_ dotc work, and _MANUALLY write down_ the
        same code.

      - Examples:
        #+begin_src scala
          given Ord[Int] { ... }
          //// dotc generate:
          // given given_Ord_Int as Ord[Int] { ... }

          given [T](using ord: Ord[T]) as Ord[List[T]] { ... }
          //// dotc generate:
          // given given_Ord_List_T[T](using ord: Ord[T]) as Ord[List[T]] { ... }
        #+end_src

      - The SYNTHESIZED _type names_ are formed from =TODO= =FIX-DOC=
        =from Jian= I don't think rule 3 is clear enough to explain the above 2nd example
        1. the prefix ~given_~,
        2. the simple name(s) of the implemented type(s), leaving out any prefixes,
        3. the simple name(s) of the toplevel argument type constructors to these types.

      - /Tuples/ are treated _as transparent_,
        i.e. a type ~F[(X, Y)]~ would get the synthesized name ~F_X_Y~.

      - *Directly implemented* /function types/ ~A => B~ are represented as ~A_to_B~.

      - /Function types/ used as arguments to OTHER /type constructors/ are
        represented as ~Function~.
        =TODO= Example??? =TODO=

***** DONE Using Clauses
      CLOSED: [2020-07-14 Tue 03:22]
      - /Using clauses/ correspond largely to Scala-2's /implicit parameter clauses/.
        E.g.
        #+begin_src scala
          // Dotty
          def max[T](x: T, y: T)(using ord: Ord[T]): T

          // Scala 2
          def max[T](x: T, y: T)(implicit ord: Ord[T]): T
        #+end_src

      - _The main difference concerns applications of such parameters._
        + Dotty:
          /Explicit arguments (not synthesized, manually written down)/ to parameters
          of /using clauses/ *must* be written as ~(using ...)~, *mirroring the
          definition syntax*.
          E.g, ~max(2, 3)(using IntOrd)~.

        + Scala 2:
          uses normal applications ~max(2, 3)(IntOrd)~ instead.

        + SUMMARY:
          The /Scala 2 syntax/ has some _inherent ambiguities_ and _restrictions_
          which are *overcome by the NEW (Dotty) syntax*. For instance,
          * /multiple implicit parameter lists/ are _NOT available in the *old*
            syntax_,
            - =TODO= =TODO= =TODO=
              EVEN THOUGH they can be _simulated using /auxiliary objects/ in
              the "Aux" pattern -- check /Shapeless/._

      - The ~summon~ method corresponds to ~implicitly~ in Scala 2.
        *It is _PRECISELY_ the SAME as the the method in Shapeless.*
        The difference between ~summon~ (or ~the~) and ~implicitly~ is that
        ~summon~ can return a *MORE _PRECISE_ type* than the type that was asked for.
        + =from Jian=
          Check the source code, API doc, or the _Using Clauses_ section, you'll see
          why -- *no widening*:
          #+begin_src scala
            def impicitly[T](implicit x: T): T = x

            inline def summon[T](implicit x: T): x.type = x
          #+end_src
          * I guess the ~summon~ will final replace ~implicit~ with ~using~ for
            its /context parameter/ prefix since a future version (Scala 3.1+).

***** DONE Context Bounds
      CLOSED: [2020-07-14 Tue 03:23]
      /Context bounds/ are the *same* in both language versions.
      They *expand* to the respective forms of /implicit parameters/.

      - Note:
        To ease migration,
        /context bounds/ in Dotty map for a limited time to /old-style implicit
        parameters/ for which arguments can be passed either in a /using clause/
        or in a /normal argument list/.
        + Once old-style implicits are deprecated,
          /context bounds/ will map to /using clauses/ instead.

***** DONE Extension Methods
      CLOSED: [2020-07-14 Tue 03:31]
      - /Extension methods/ have *NO DIRECT counterpart* in Scala 2,
        BUT they can be _SIMULATED_ with /implicit classes/. For instance, the
        /extension method/
        #+begin_src scala
          extension (c: Circle) def circumference: Double = c.radius * math.Pi * 2
        #+end_src
        could be _SIMULATED_ to some degree by
        #+begin_src scala
          implicit class CircumDecorator(c: Circle) extends AnyVal {
            def circumference: Double = c.radius * math.Pi * 2
          }
        #+end_src

      - /ABSTRACT extension methods/ in /traits/ that are implemented in /given
        instances/ have *NO DIRECT counterpart* in Scala-2.
        + The ONLY way to _SIMULATE_ these is to make /implicit classes/ available
          through /imports/.

        + =TODO=
          The _Simulacrum macro library_ can automate this process in some cases.

***** DONE Type class Derivation
      CLOSED: [2020-07-14 Tue 03:33]
      /Type class derivation/ has *NO DIRECT counterpart* in the Scala 2 language.

      - Comparable functionality can be achieved by _macro-based libraries_ such as
        + _Shapeless_
        + _Magnolia_
        + _scalaz-deriving_

***** DONE Context Function Types
      CLOSED: [2020-07-14 Tue 03:33]
      /Context function types/ have *NO analogue* in Scala 2.

***** DONE Implicit By-Name Parameters
      CLOSED: [2020-07-14 Tue 03:36]
      /Implicit by-name parameters/ are *NOT supported* in Scala 2,
      but can be _EMULATED to some degree_ by the ~Lazy~ type in _Shapeless_.

**** DONE Simulating Scala 2 Implicits in Scala 3
     CLOSED: [2020-07-14 Tue 03:51]
***** DONE Implicit Conversions
      CLOSED: [2020-07-14 Tue 03:51]
      /Implicit conversion/ methods in Scala 2 can be expressed as /given instances/
      of the ~scala.Conversion~ /class/ in Dotty.
      - E.g.
        instead of ~implicit def stringToToken(str: String): Token = new Keyword(str)~
         one can write
         #+begin_src scala
           given stringToToken as Conversion[String, Token] {
             def apply(str: String): Token = KeyWord(str)
           }

           // OR

           given stringToToken as Conversion[String, Token] = KeyWord(_)
         #+end_src

***** DONE Implicit Classes
      CLOSED: [2020-07-14 Tue 03:50]
      - /Implicit classes/ in Scala 2 are often used to define /extension methods/,
        which are *DIRECTLY supported* in Dotty.

      - OTHER uses of /implicit classes/ can be _SIMULATED_ by a pair of
        =from Jian= Examples for "OTHER uses"???
        + a REGULAR /class/
          and
        + a /given ~Conversion~ instance/.

***** DONE Implicit Values
      CLOSED: [2020-07-14 Tue 03:45]
      - /Implicit ~val~ definitions/ in Scala 2 can be expressed in Dotty using a
        + regular ~val~ definition
          AND
        + an /alias given/.

      - E.g.,
        Scala 2's ~lazy implicit val pos: Position = tree.sourcePos~ can be
        expressed in Dotty as
        #+begin_src scala
          lazy val pos: Position = tree.sourcePos
          given Position = pos
        #+end_src

***** DONE Abstract Implicits
      CLOSED: [2020-07-14 Tue 03:48]
      - An _ABSTRACT IMPLICIT ~val~ or ~def~ in Scala 2_ can be expressed in Dotty
        using
        + a REGULAR /abstract definition/
          AND
        + an /alias given/.

      - E.g.,
        Scala 2's ~implicit def symDecorator: SymDecorator~ can be expressed in
        Dotty as
        #+begin_src scala
          def symDecorator: SymDecorator
          given SymDecorator = symDecorator
        #+end_src

**** DONE Implementation Status and Timeline
     CLOSED: [2020-07-14 Tue 03:43]
     - The Dotty implementation implements BOTH
       + Scala-2's /implicits/
       + the new abstractions.

     - In fact, support for Scala-2's implicits is an _essential part_ of the common
       language subset between 2.13/2.14 and Dotty.

     - *Migration to the new abstractions* will be supported
       by making _AUTOMATIC rewritings_ available.

     - Depending on adoption patterns,
       /old style implicits/ might start to be *deprecated* in a _version
       *FOLLOWING* Scala 3.0_.

** TODO METAPROGRAMMING
*** DONE Overview - =TODO= =RE-READ=
    CLOSED: [2020-06-24 Wed 03:50]
    The following pages introduce the *redesign* of /metaprogramming/ in Scala.
    The following fundamental facilities:
    1. /Inline/
       ~inline~ is a new /soft modifier/ that *guarantees* that a definition will
       be inlined at the point of use.

       - The primary motivation:
         *reduce the overhead* behind
         + _function calls_
         + _access to values_.

       - The _expansion_ will be performed by the Scala compiler _during the *Typer*
         /compiler phase/._

       - ~inline~ is a *COMMAND* (*MUST DO*) to the *compiler*.
           This is _DIFFERENT_ from some other ecosystems, in which /inline/ is a
         request that _might be_ satisfied by the compiler.
         + The _REASON_:
           /inlining/ in Scala can drive other _compile-time operations_, like
           * /inline pattern matching/ (enabling /type-level programming/)

           * /macros/ (enabling /compile-time, generative, metaprogramming/)

           * /runtime code generation/ (/multi-stage programming/)
             - =from Jian=
               WHY this is considered as one kind of drive othe _compile-time
               operations_.

    2. /Macros/ construct code at /compile-time/
       - /Macros/ are built on _TWO_ well-known fundamental operations:
         + quotation :: *convert program code to data*, specifically, a (tree-like)
           representation of this code.
           * It is expressed as
             - ~'{...}~ for /expressions/
             - ~'[...]~ for /types/

         + splicing :: *convert a program's representation to program code*
           * expressed as ~${ ... }~.

       - Together with ~inline~, _these two abstractions_ allow to construct
         program code programmatically.

    3. =TODO=
       /Staging/ construct new code at /runtime/.
       That way, code generation can depend not only on static data but also on
       data available at runtime. This splits the evaluation of the program in
       two or more phases or ... /stages/.
         Consequently, this method generative programming is called /"Multi-Stage
       Programming"/. /Staging/ is built on the _SAME_ foundations as /macros/.
       It uses /quotes/ and /splices/, but _LEAVES OUT_ /inline/.

    4. =TODO=
       /TASTy Reflection/
       + /Quotations/ are a "black-box" representation of code.
         They can be parameterized and composed using /splices/ but their
         structure cannot be analyzed from the outside.
       + /Tasty reflection/ gives a way to analyze code structure by partly
         revealing the representation type of a piece of code in a standard API.
         TODO
         The _representation type_ is a form of /typed abstract syntax tree/,
         which gives rise to the "TASTy` moniker.

    5. =TODO=
       /TASTy Inspection/
       /Typed abstract syntax trees/ are serialized in a custom compressed
       binary format in =.tasty= files. /TASTy inspection/ allows to _load_
       these files and _analyze_ their content's tree structure.

*** DONE Inline
    CLOSED: [2020-05-26 Tue 04:23]
**** DONE Inline Definitions
     CLOSED: [2020-03-11 Wed 00:43]
     - ~inline~ :: a new /soft modifier/ that *guarantees* that a definition will
                   be inlined at the point of use.
     - Example:
       #+begin_src scala
         object Config {
           inline val logging = false
         }

         object Logger {
           private var indent = 0

           inline def log[T](msg: String, indentMargin: =>Int)(op: => T): T =
             if (Config.logging) {
               println(s"${"  " * indent}start $msg")
               indent += indentMargin
               val result = op
               indent -= indentMargin
               println(s"${"  " * indent}$msg = $result")
               result
             }
             else op
         }
       #+end_src
       + The ~Config~ object contains a definition of the /inline value/ ~logging~.
         This means that ~logging~ is treated as a _constant value_, equivalent to
         its RHS ~false~. The RHS of such an ~inline val~ must itself be a
         /constant expression/.
         * Used in this way, ~inline~ is *equivalent to* Java and Scala 2's ~final~.
           =IMPORTANT=
           Note that ~final~, meaning /inlined constant/,
           - is still supported in Dotty,
           - but *will be Phased Out*.

       + The ~Logger~ object contains a definition of the /inline method/ ~log~.
         This method will always _be inlined at the point of call_.

       + Usage and re-write:
         #+begin_src scala
           var indentSetting = 2

           def factorial(n: BigInt): BigInt = {
             log(s"factorial($n)", indentSetting) {
               if (n == 0) 1
               else        n * factorial(n - 1)
             }
           }
         #+end_src

         - With current definition ~inline val logging = false~,
           The usage code will be re-written as
           #+begin_src scala
             def factorial(n: BigInt): BigInt = {
               if (n == 0) 1
               else        n * factorial(n - 1)
             }
           #+end_src

         - If the example code define ~inline val logging = true~, then the
           usage code will be re-written as
           #+begin_src scala
             def factorial(n: BigInt): BigInt = {
               val msg = s"factorial($n)"
               println(s"${"  " * indent}start $msg")
               Logger.inline$indent_=(indent.+(indentSetting))
               val result =
                 if (n == 0) 1
                 else        n * factorial(n - 1)
               Logger.inline$indent_=(indent.-(indentSetting))
               println(s"${"  " * indent}$msg = $result")
               result
             }
           #+end_src
           NOTE:
           + The /by-value parameter/ ~msg~ is _evaluated only once_,
             per the usual Scala semantics, by binding the value and reusing the
             ~msg~ through the body of ~factorial~.

           + The special handling of the assignment to the ~private var indent~.
             It is achieved by *generating* a /setter method/ ~def
             inline$indent_=~ and calling it instead.

**** DONE Recursive Inline Methods
     CLOSED: [2020-05-25 Mon 18:22]
     /Inline methods/ can be *recursive*.
     - For instance, when called with a *constant* exponent ~n~, the following method
       for ~power~ will be implemented by straight inline code *WITHOUT ANY
       /loop/ or /recursion/.*
       #+begin_src scala
         inline def power(x: Double, n: Int): Double = {
           if (n == 0) 1.0
           else if (n == 1) x
           else {
             val y = power(x, n / 2)
             if (n % 2 == 0) y * y else y * y * x
           }
         }

         power(expr, 10)
         // translates to
         //
         //    val x = expr
         //    val y1 = x * x   // ^2
         //    val y2 = y1 * y1 // ^4
         //    val y3 = y2 * x  // ^5
         //    y3 * y3          // ^10
       #+end_src

     - There also can be /inline parameters/.
       - =from Jian=
         The /by-name parameters/ in the context of this whole document is actually
         /by-need parameters/. This is the common implementation in all the practical
         programming languages.

       - =from Jian=
         /Inline parameters/ have call semantics *equivalent* to /naive by-name parameters/
         -- no caching.
           It is usually useful when /constant values/ need to be propagated to
         allow further optimizations/reductions. TODO ??? TODO

     - The difference in translation between /by-value/, /by-name/, and ~inline~
       parameters:
       #+begin_src scala
         inline def funkyAssertEquals(actual: Double,
                                      expected: =>Double,
                                      inline delta: Double): Unit =
           if ((actual - expected).abs > delta)
             throw new AssertionError(s"difference between ${expected} and ${actual} was larger than ${delta}")

         funkyAssertEquals(computeActual(), computeExpected(), computeDelta())
         // translates to
         //
         //    val actual = computeActual()
         //    def expected = computeExpected()
         //    if (actual - expected).abs > computeDelta() then
         //      throw new AssertionError(s"difference between ${expected} and ${actual} was larger than ${computeDelta()}")
       #+end_src
       + You can see the ~inline~ parameter here is actually a /NAIVE by-name
         parameter/. The /by-name parameter/ here is a /non-naive by-name parameter/,
         and people often call it /by-need parameter/.
         =from Jian= this manual doesn't mention the term /by-name parameter/.

**** DONE Rules for Overriding
     CLOSED: [2020-05-25 Mon 18:41]
     /Inline methods/ can *override* other /non-inline methods/.
     The rules are as follows:
     1. If an /inline method/ ~f~ implements or *overrides* another, /non-inline
        method/, the /inline method/ *can also be invoked at /runtime/.*
        For instance, consider the scenario:
        #+begin_src scala
          abstract class A {
            def f: Int
            def g: Int = f
          }

          class B extends A {
            inline def f = 22
            override inline def g = f + 11
          }

          val b = B
          val a: A = b

          // inlined invocations
          assert(b.f() == 22)
          assert(b.g() == 33)

          // dynamic invocations
          assert(a.f() == 22)
          assert(a.g() == 33)
        #+end_src
        The /inlined invocations/ and the /dynamically dispatched invocations/
        give the _SAME_ results.

     2. /Inline methods/ are effectively ~final~.

     3. /Inline methods/ can also be ~abstract~.
          An /abstract inline method/ can be _implemented_ ONLY by other /inline
        methods/. *It cannot be invoked directly*:
        #+begin_src scala
          abstract class A {
            inline def f: Int
          }

          object B extends A {
            inline def f: Int = 22
          }

          B.f  // OK
          val a: A = B
          a.f  // error: cannot inline `f` in `A`.
        #+end_src

**** DONE Relationship to ~@inline~
     CLOSED: [2020-05-25 Mon 19:11]
     - Scala also defines a ~@inline~ annotation which is used as _a *hint* for the
       BACKEND to inline._

     - The ~inline~ modifier is a _MORE POWERFUL_ than the ~@inline~ annotation.
       + ~@inline~ annotation ::
         * A _hint_
           - _Hint_ here means _try with *BEST EFFORT*, *but NOTHING guaranteed*!_

         * The _hint_ is for the *backend*


       + ~inline~ /modifier/ ::
         * A _command_
           - _Command_ here means _GUARANTEED!_

         * The _command_ is for the *frontend*

         * it also applies to /recursive methods/.

       + Cross compilation between Dotty and Scala 2:
         + Introduce ~@forceInline~ in Dotty.
           * For dotc it is the same as ~inline~.
           * For scalac it will be ignored.

         + Usage:
           Always use ~@forceInline @inline~ if cross compilation between Dotty
           and Scala 2 is required. This can make
           * Scala 2 ignores the ~@forceInline~ annotation, so one must use both
             annotations o guarantee inlining for Dotty and at the same time hint
             inline for Scala 2 (i.e. ~@forceInline @inline~)
             - =from Jian=
               My understanding: TODO ??? TODO =re-read= this summary
               When the ~@forceInline~ show up in Dotty code, it is considered as
               ~inline~. However, this is not enough if we want use this code from
               Scala 2 with _inline_ applied -- Scala 2 ignores ~@forceInline~.
                 This means it's better to use ~@forceInline @inline~ in Dotty, if
               you want to use this code can be used in Scala 2.
                 Still with ~@forceInline @inline~, it is possbile that the code
               at the Scala 2 call sites may not be _inlined_.

***** The definition of constant expression
      Scala Language Specification 2.13 - _6.24 Constant Expressions_
      - An /inline value/ *must* have a /literal type/ such as ~1~ or ~true~.
        #+begin_src scala
          inline val four = 4

          // Equivalent to

          inline val four: 4 = 4
        #+end_src

      - It is also possible to have _inline vals_ of /types/ that do not have a
        syntax, such as ~Short(4)~.
        #+begin_src scala
          trait InlineConstant {
            inline val myShort: Short
          }

          object Constants extends InlineConstants {
            inline val myShort/*: Short(4)*/ = 4
          }
        #+end_src

**** DONE Transparent Inline Methods
     CLOSED: [2020-05-25 Mon 19:21]
     /Inline methods/ can additionally be declared ~transparent~.
       This means that the /return type/ of the /inline method/ can be
     *SPECIALIZED to a more precise type* upon expansion.

     - Example:
       #+begin_src scala
         class A

         class B extends A {
           def m() = true
         }

         transparent inline def choose(b: Boolean): A =
           if b then new A() else new B()

         val obj1 = choose(true)  // static type is A
         val obj2 = choose(false) // static type is B

         // obj1.m() // compile-time error: `m` is not defined on `A`
         obj2.m()    // OK
       #+end_src

     - A *non-transparent* /inline method/ is a *"blackbox"* in the sense that
       details of its implementation do *not leak out.*

     - /Transparent inline methods/ are *"whitebox"* in the sense that the type
       of an application of such a method can be _more specialized than its
       DECLARED /return type/,_ depending on how the method expands.

     - Example:
       we see how the return type of zero is *specialized* to the /singleton
       type/ ~0~ permitting the addition to be ascribed with the correct
       /singleton type/ ~1~.
       #+begin_src scala
         transparent inline def Zero: Int = 0
         val one: 1 = zero() + 1
       #+end_src

**** DONE Inline Conditionals
     CLOSED: [2020-05-26 Tue 00:44]
     #+begin_src scala
       inline def update(delta: Int) =
         inline if (delta >= 0) increaseBy(delta)
                else            decreaseBy(-delta)
     #+end_src
     - If the /condition/ of an /if-then-else expressions/ is a /constant expression/
       then it _simplifies to the selected branch._ -- *NOT guaranteed*.

     - Prefix an /if-then-else expression/ with ~inline~ *enforces* that the
       /condition/ has to be a /constant expression/, and thus *guarantees* that
       the conditional _will *always* simplify_.

     - Use ~inline~ means, for legal code, in the call site ~delta~ _MUST be_ a
       /compile-time constant/.

     - A call ~update(22)~ would re-write to ~increaseBy(22)~.

     - A call with a value of *not* /compile-time constant/ will trigger a
       /compile error/:
       #+begin_src text
            |  inline if (delta >= 0) ???
            |  ^
            |  cannot reduce inline if
            |   its condition
            |     delta >= 0
            |   is not a constant value
            | This location is in code that was inlined at ...
       #+end_src

**** DONE Inline Matches
     CLOSED: [2020-05-26 Tue 02:06]
     - A /match expression/ _in the body_ of an /inline method definition/ _may be_
       prefixed by the ~inline~ modifier.
       + If there is *ENOUGH* _STATIC information_ to _unambiguously take a branch_,
         the expression is *reduced* to that branch and the type of the result
         is taken.

       + If not, a /compile-time error/ is raised that reports that the match cannot
         be reduced.

     - The example below defines an /inline method/ with a single /inline match
       expression/ that picks a case based on its /static type/:
       #+begin_src scala
         transparent inline def g(x: Any): Any = inline x match {
           case x: String => (x, x)  // Tuple2[String, String](x, x)
           case x: Double => x
         }

         g(1.0d)    // Has type `1.0d` which is a subtype of `Double`
         g("test")  // Has type `(String, String)`
       #+end_src

     - The scrutinee ~x~ is *examined statically* and the /inline match/ is
       *reduced* accordingly returning the corresponding value (with the /type
       specialized/ because ~g~ is declared ~transparent~).

     - The /type/ can have a _richer_ structure like the _simple_ /ADT/ below.
       ~toInt~
       1. _matches_ the structure of a number in /Church-encoding/
          AND
       2. _computes_ the corresponding integer.
       #+begin_src scala
         enum Nat {
           case Zero
           case Succ[N <: Nat](n: N)
         }

         transparent inline def toInt(n: Nat): Int = inline n match {
           case Nat.Zero     => 0
           case Nat.Succ(n1) => toInt(n1) + 1
         }

         inline val natTwo = toInt(Nat.Succ(Nat.Succ(Nat.Zero)))
         val intTwo: 2 = natTwo
       #+end_src
       ~natTwo~ is inferred to have the /singleton type/ ~2~
       (=from Jian= since here we have the ~transparent~)

**** DONE The ~scala.compiletime~ Package
     CLOSED: [2020-05-26 Tue 02:27]
     The ~scala.compiletime~ package contains _helper definitions_ that provide
     support for /compile time/ OPERATIONS over _values_.
     They are described in the following.
***** ~constValue~, ~constValueOpt~, and the ~S~ combinator
      - ~constValue[T]~ generate a _constant value_ of the /singleton type/ ~T~
        #+begin_src scala
          import scala.compiletime.{constValue, S}

          transparent inline def toIntC[N]: Int =
            inline constValue[N] match {
              case 0        => 0
              case _: S[n1] => 1 + toIntC[n1]
            }

          inline val ctwo = toIntC[2]
        #+end_src

      - ~constValueOpt[T]~ is similar to ~constValue[T]~, and it generates a
        _constant value_ of type ~Option[T]~.

      - ~S~ is the type of the *successor* of some /singleton type/.
        For example, ~S[1]~ is the /singleton type/ ~2~.
        + =from Jian=
          How can we make a type of values can have /successor/, and how do
          these successors generate???

***** ~erasedValue~
      - The ~erasedValue[T]~ function in ~scala.comiletime.erasedValue~ is not
        implemented -- it would always raise a ~NotImplementedError~ exception
        when called.
          _However, it can in fact never be called, since it is declared ~erased~ --
        it is *ONLY* used at /compile-time/ during type checking._

      - Example:
        #+begin_src scala
          import scala.comiletime.erasedValue
          // erased def erasedValue[T]: T = ???

          inline def defaultValue[T] = inline erasedValue[T] match {
            case _: Byte    => Some(0: Byte)
            case _: Char    => Some(0: Char)
            case _: Short   => Some(0: Short)
            case _: Int     => Some(0)
            case _: Long    => Some(0L)
            case _: Float   => Some(0.0f)
            case _: Double  => Some(0.0d)
            case _: Boolean => Some(false)
            case _: Unit    => Some(())
            case _          => None
          }

          val dInt:     Some[Int]     = defaultValue[Int]
          val dDouble:  Some[Double]  = defaultValue[Double]
          val dBoolean: Some[Boolean] = defaultValue[Boolean]
          val dAny:     Any.type      = defaultValue[Any]
        #+end_src

      - Another example:
        #+begin_src scala
          inline def toIntT[N <: Nat] <: Int = inline erasedValue[N] match {
            case _: Zero.type => 0
            case _: Succ[n]   => toIntT[n] + 1
          }

          final val two = toIntT[Succ[Succ[Zero.type]]]
        #+end_src
        + =from Jian= I think the ~final~ here is not the best practice!!!
          #+begin_quote
          Used in this way, inline is equivalent to Java and Scala 2's final.
          Note that ~final~, meaning /inlined constant/,
          is still supported in Dotty, BUT *will be phased out*.
                              -- from "Inline Definitions" subsection in this doc
          #+end_quote

        + =from Jian=
          Better implementation:
          #+begin_src scala
            enum Nat {
              case Zero
              case Succ[N <: Nat](n: N)
            }

            import Nat._

            transparent inline def toIntT[N <: Nat]: Int =
              inline scala.compiletime.erasedValue[N] match {
                case _: Zero.type => 0
                case _: Succ[n]   => toIntT[n] + 1
              }

            inline val two = toIntT[Succ[Succ[Zero.type]]]
          #+end_src

      - ~erasedValue~ is /an ~erased~ method/ so *it _CANNOT be used_ and _has NO_
        /runtime behavior/.*
          Since ~toIntT~ performs /static checks/ over the /static type/ of ~N~
        we can safely use it to scrutinize its return type (~S[S[Z]]~ in this case).

***** ~error~
      The ~error~ /method/ is used to produce _user-defined_ /compile errors/
      *DURING /inline expansion/.* It has the following signature:
      #+begin_src scala
        inline def error(inline msg: String): Nothing
      #+end_src

      - If an /inline expansion/ results in a call ~error(msgStr)~ the compiler
        produces an _error message_ containing the given ~msgStr~.
        + Example 1
          #+begin_src scala
            inline def fail() = {
              error("failed for a reason")
            }

            fail()  // error: failed for a reason
          #+end_src

          OR

        + Example 2
          #+begin_src scala
            inline def fail(p1: => Any) = {
              error(code"failed on: $p1")
            }

            fail(indentity("foo"))  // error: failed on: indentity("foo")
          #+end_src

***** The ~scala.compiletime.ops~ package
      - The ~scala.compiletime.ops~ package contains types that provide support for
        *primitive operations on /singleton types/.*

      - When all arguments to a type in ~scala.compiletime.ops~ are /singleton types/,
        the compiler can *evaluate* the result of the operation.

      - For example,
        #+begin_src scala
          import scala.compiletime.ops.int._
          import scala.compiletime.ops.boolean._

          val conjunction: true && true = true
          val multiplication: 3 * 5 = 15
        #+end_src
        + ~scala.compiletime.ops.int.*~ provides support for _multiplying TWO
          /singleton ~Int~ types/,_

        + ~scala.compiletime.ops.boolean.&&~ for the _conjunction of TWO ~Boolean~
          types._

      - Many of these /singleton operation types/ are meant to be used _infix_
        (as in SLS Â§ 3.2.8), and are annotated with ~@infix~ accordingly.

      - Since /type aliases/ have the *SAME* /precedence/ rules as their term-level
        equivalents, the operations _COMPOSE with the *EXPECTED* /precedence/ rules_:
        #+begin_src scala
          import scala.compiletime.ops.int._
          val x: 1 + 2 * 3 = 7
        #+end_src

      - The /operation types/ are *located in* /packages/ named after the /type/
        of the *left-hand side* parameter.
        + ~scala.compiletime.ops.int.+~ represents _addition of two numbers_,
        + ~scala.compiletime.ops.string.+~ represents _string concatenation_.

      - To _use both_ and _distinguish the two types from each other_,
        a /match type/ can dispatch to the correct implementation:
        #+begin_src scala
          import scala.compiletime.ops._
          import scala.annotation.infix

          @infix type +[X <: Int | String, Y <: Int | String] = (X, Y) match {
            case (Int, Int)       => int.+[X, Y]
            case (String, String) => string.+[X, Y]
          }

          val concat: "a" + "b" = "ab"
          val addition: 1 + 1 = 2
        #+end_src

**** DONE Summoning Implicits Selectively - =TODO= =RE-READ=
     CLOSED: [2020-05-26 Tue 04:23]
     - It is foreseen that *many areas* of /typelevel programming/ *can be* done
       with _REWRITE_ methods *instead of* /implicits/.
       *BUT* _sometimes /implicits/ are *UNAVOIDABLE*._

     - The problem so far was that the /Prolog-like programming style/ of /implicit
       search/ becomes _viral_:
       *Once some construct depends on implicit search it has to be written as a
       logic program itself.* TODO ??? TODO

     - Consider for instance the problem of creating a ~TreeSet[T]~ or a ~HashSet[T]~
       depending on whether ~T~ _has an ~Ordering~ or not._
       We can create a set of /given definitions/ like this:
       #+begin_src scala
         trait SetFor[T, S <: Set[T]]

         class LowPriority {
           implicit def hashSetFor[T]: SetFor[T, HashSet[T]] = { ... }
         }

         object SetFor extends LowPriority {
           implicit def treeSetFor[T: Ordering]: SetFor[T, TreeSet[T]] = { ... }
         }
       #+end_src
       + =from Jian=
         I don't know the complete form of this example.
         I try to complete it:
         #+begin_src scala
           trait SetFor[T, S <: Set[T]] {
             val set: S
           }

           class LowPriority {
             given hashSetFor[T] as SetFor[T, HashSet[T]] {
               val set =  new HashSet
             }
           }

           object SetFor extends LowPriority {
             given def treeSetFor[T: Ordering] as SetFor[T, TreeSet[T]] {
               val set =  new TreeSet
             }
           }

           case class Person(name: String)

           println(summon[SetFor[Person, _]].set)  //  HashSet()
           println(summon[SetFor[String, _]].set)  //  TreeSet()
         #+end_src
         * =from Jian=
           TODO Can I find a conciser way to eliminate the ~.set~ part???

       + *Clearly, this is _NOT_ pretty.*
         * Besides all the usual indirection of /implicit search/,

         * we face the problem of rule *prioritization* where we have to
           _ensure that ~treeSetFor~ takes /priority/ over ~hashSetFor~ if the
           element type has an ordering._
             This is solved (clumsily) by putting ~hashSetFor~ in a /superclass/
           ~LowPriority~ of the object ~SetsFor~ where ~treeSetFor~ is defined.

         * Maybe the boilerplate would still be acceptable if the crufty code
           could be contained.
           _However_, this is not the case.

           TODO ??? TODO
           Every user of the abstraction *has to be PARAMETERIZED itself with a
           ~SetFor~ implicit.* Considering the simple task "I want a ~TreeSet[T]~
           if ~T~ has an /ordering/ and a ~HashSet[T]~ otherwise", this seems
           like a lot of ceremony.

       + There are some proposals to improve the situation _in specific areas,_
         for instance by allowing _MORE ELABORATE schemes to SPECIFY /priorities/._
           But they all keep the viral nature of /implicit search/ programs based
         on logic programming. -- =from Jian= _and they are NOT adopted._

     - _By contrast_,
       the NEW ~scala.compiletime.summonFrom~ construct makes /implicit search/ available
       _in a functional context_ -- =from Jian= rather than a logic programming cotext. TODO ??? TODO
       #+begin_src scala
         import scala.compiletime.summonFrom

         inline def setFor[T]: Set[T] = summonFrom {
           case given ord: Ordering[T] => new TreeSet[T]
           case _                      => new HashSet[T]
         }
       #+end_src
       + A ~summonFrom~ /call/ takes a /pattern matching closure/ as argument.
           All patterns in the /closure/ are /type ascriptions/ of the form
         ~identifier : Type~.

       + Patterns are tried *in sequence* (=from Jian= This help us avoiding using
         inheritance to solve the _implicit search priority issue_).
         * The first case with a pattern ~x: T~ such that an /given value/ of type
           ~T~ can be *summoned* is chosen. TODO ??? TODO

         * If the pattern is _PREFIXED_ with ~given~, the variable ~x~ is bound to
           the /given value/ *for the remainder of the case.* -- =from Jian= natural
           and reasonable, just like the other existing pattern boundings.

       + ~summonFrom~ application *must be reduced at /compile time/.*

     - Example:
       #+begin_src scala
         summon[Ordering[String]]

         println(setFor[String].getClass)  // prints class scala.collection.immutable.TreeSet
       #+end_src

     - Of course, when there is /contextual abstractions/, /ambiguity errors/ can
       happen:
       #+begin_src scala
         class A
         implicit val a1: A = new A
         implicit val a2: A = new A

         inline def f: Any = summonFrom {
           case given _: A => ???  // error: ambiguous implicits
         }
       #+end_src

***** DONE ~summonInline~
      CLOSED: [2020-05-26 Tue 03:21]
      The shorthand ~summonInline~ provides a _simple way_ to write a ~summon~ that is
      *delayed* _until the call is inlined_.
      #+begin_src scala
        transparent inline def summonInline[T]: T = summonFrom {
          case t: T => t
        }
      #+end_src
      - =from Jian=
        Need example and use case.

**** DONE Reference
     CLOSED: [2020-05-26 Tue 02:28]
     For more info, see
     - =PR #4768=, which explains how ~summonFrom~'s predecessor (/implicit matches/)
       can be used for /typelevel programming/ and /code specialization/

     - =PR #7201= which explains the _NEW ~summonFrom~ syntax._

*** TODO Macros
**** DONE Macros: Quotes and Splices
     CLOSED: [2020-06-21 Sun 22:43]
     - Macros are built on _TWO_ well-known fundamental operations:
       + quotation :: ~'{...}~ for /expressions/;
                      ~'[...]~ for /types/.

       + splicing :: ~${ ... }~

     - Additionally, *within* a /quote/ or a /splice/
       we can /quote/ or /splice/ _identifiers_ *directly* (i.e. ~'e~ and ~$e~).

     - Readers may notice the _RESEMBLANCE_ of the two aforementioned syntactic
       schemes with the familiar /string interpolation syntax/. /Quotes/ and
       /splices/ in this section allow us to treat code in a similar way,
       effectively supporting /macros/.
       #+begin_src scala
         println(s"Hello, $name, here is the result of 1 + 1 = ${1 + 1}")
       #+end_src
       In string interpolation we /quoted/ a string and then we /spliced/ into it,
       two others.
       1. ~name~, is a reference to a value of type string,
       2. an _arithmetic expression_ that will be evaluated followed by the /splicing/
          of its string representation.

     - The *entry point* for /macros/: an /inline method/ with a *top-level* /splice/.
       + We call it a *top-level*
         because it is the *ONLY* OCCASION where we encounter a /splice/ *outside*
         a /quote/ (consider as a /quote/ the compilation-unit at the call-site).

     - For example, the code below presents an ~inline~ /method/ ~assert~ which
       calls at compile-time a method ~assertImpl~ with a /boolean expression tree/
       as argument. ~assertImpl~ evaluates the expression and prints it again in
       an error message if it evaluates to ~false~.
       #+begin_src scala
         import sala.quoted._

         inline def assert(inline expr: Boolean): Unit =
           ${ assertImpl('expr) }

         def assertImpl(expr: Expr[Boolean])(using QuoteContext) = '{
           if (!$expr)
             throw new AssertionError(s"failed assertion: ${${ showExpr(expr) }}")
         }

         def showExpr(expr: Expr[Boolean])(using QuoteContext): Expr[String] =
           '{ "<some source code>" }  // Better implementation later in this document
       #+end_src
       + =IMPORTANT= =IMPORTANT= =IMPORTANT=
         =from Jian=
         The ~inline~ method with /top level splice/
         can provide a /given ~QuoteContext~ instance/, which, in syntax, is NOT
         be written out explicitly! This is why the signature of the above ~asset~
         does *NOT* have a ~(using QuoteContext)~ parameter list.

     - /Quotations/ can have _spliced_ parts in them; in this case the embedded /splices/
       _are evaluated and embedded as part of_ the formation of the /quotation/.

       - /Quotes/ and /splices/ can also be applied *DIRECTLY* to _identifiers_.
         + An /identifier/ ~$x~ starting with a ~$~ that appears _INSIDE_ a /quoted
           expression or type/ is _treated as_ a /splice/ ~${x}~.

         + Analogously, an /quoted identifier/ ~'x~ that appears _INSIDE_ a /splice/
           is _treated as_ a /quote/ ~'{x}~.

     - /Quotes/ and /splices/ are *DUALS of each other*.
       For arbitrary /expressions/ ~e~ and /types/ ~T~ we have:
       #+begin_src scala
         ${'{e}} = e
         '{${e}} = e
         ${'[T]} = T
         '{$[T]} = T
       #+end_src

**** DONE Types for Quotations
     CLOSED: [2020-05-30 Sat 15:39]
     - The /type signatures/ of /quotes/ and /splices/ can be described using
       _TWO_ _FUNDAMENTAL /types/:_
       + ~Expr[T]~: /abstract syntax trees/ representing /expressions/ of /type/ ~T~

       + ~Type[T]~: /type structures/ representing /type/ ~T~.

     - /Quoting/ and /splicing/ are dual to each other
       + /Quoting/
         * /expressions/ of /type/ ~T~ ---> /expressions/ of /type/ ~Expr[T]~
         * /types/ ~T~ ---> /expressions/ of /type/ ~Type[T]~.

       + /Splicing/
         - expressions of /type/ ~Expr[T]~ ---> /expressions/ of /type/ ~T~
         - expressions of /type/ ~Type[T]~ ---> /types/ ~T~.

     - The two types can be defined in package ~scala.quoted~ as follows:
       #+begin_src scala
         package scala.quoted

         sealed abstract calss Expr[+T]
         sealed abstract calss Type[T]
       #+end_src
       Both ~Expr~ and ~Type~ are ~abstract~ and ~sealed~, so _ALL /constructors/
       for these types are PROVIDED BY THE SYSTEM._

     - *TWO* ways to construct values of type ~Expr[T]~ or ~Type[T]~:
       + by /quoting/,
       + =TODO= by /type-specific lifting operations/ that will be discussed later on.

**** DONE The Phase Consistency Principle
     CLOSED: [2020-05-30 Sat 15:40]
     - A fundamental /phase consistency principle (PCP)/ regulates accesses to
       /free variables/ in /quoted/ and /spliced/ code:
       #+begin_quote
       For any /free variable reference/ ~x~,
       the _number_ of /quoted scopes/ and the _number_ of /spliced scopes/
       between the reference to ~x~ and the definition of ~x~ *must be equal*.
       #+end_quote
       + ~this~-reference count as /free variables/.

       + We assume
         * ALL _imports_ are fully expanded
         * ~_root_~ is *NOT* a /free variable/.
         So /references/ to /global definitions/ are allowed everywhere.

     - The /phase consistency principle/ can _be motivated as follows_:
       1. Suppose the result of a program _P_ is some /quoted text/ ~'{ ... x ... }~
          that refers to a /free variable/ ~x~ in _P_. This can be represented only
          by referring to the original variable ~x~.

       2. Hence, the result of the program will need to persist the program state
          itself as one of its parts. We don't want to do this, hence this situation
          should be made illegal.

          Dually, suppose a top-level part of a program is a /spliced text/
          ~${ ... x ... }~ that refers to a /free variable/ ~x~ in _P_. This would
          mean that we refer during construction of _P_ to a value that is
          _available ONLY during execution of P._
          *This is of course impossible and therefore needs to be ruled out.*

       Now, the small-step evaluation of a program will reduce /quotes/ and
       /splices/ in equal measure using the cancellation rules above. But it will
       neither create nor remove /quotes/ or /splices/ individually. So the PCP
       ensures that program elaboration will lead to neither of the two unwanted
       situations described above.

     - In what concerns the range of features it covers, this form of macros (Scala 3
       macro) introduces a principled metaprogramming framework that is quite
       close to the /MetaML family of languages/.
       + One difference is that MetaML does NOT have an equivalent of the PCP.
           quoted code in MetaML can access variables in its immediately
         enclosing environment, with some restrictions and caveats since such
         accesses involve serialization. _However, this does not constitute a
         fundamental gain in expressiveness._

**** DONE From ~Expr~'s to Functions and Back
     CLOSED: [2020-06-22 Mon 01:49]
     It is possible to
     CONVERT _back and forth_ any ~Expr[T => R]~ into ~Expr[T] => Expr[R]~!

     - The conversions can be implemented as follows:
       #+begin_src scala
         def to[T: Type, R: Type](f: Expr[T] => Expr[R])(using QuoteContext): Expr[T => R] =
           '{ (x: T) => ${ f('x) } }

         def from[T: Type, R: Type](f: Expr[T => R])(using QuoteContext): Expr[T] => Expr[R] =
           (x: Expr[T]) => '{ $f($x) }
       #+end_src
       - This decorator gives ~Expr~ the ~apply~ operation of an /applicative functor/.

       - Note how the fundamental /phase consistency principle/ works in two different
         directions here for ~f~ and ~x~.
         + For example,
           in the method ~to~,
           * the reference to ~f~ is legal because it is /quoted/, then /spliced/,
           * the reference to ~x~ is legal because it is /spliced/, then /quoted/.

     - Example:
       =from Jian= =TODO= Need to create a /given instance/ of ~QuoteContext~
       #+begin_src scala
         // '{ (x: Int) => x.toString }
         val f1: Expr[Int => String] = to((x: Expr[Int]) => '{ $x.toString })

         // (x: Expr[Int]) => '{ ((x: Int) => x.toString)($x) }
         val f2: Expr[Int] => Expr[String] = from('{ (x: Int) => x.toString })
         f2('{2})  // '{ ((x: Int) => x.toString)(2) }
       #+end_src
       + =from Jian=
         To make the code above runnable, they must be used in a scope with a
         given ~QuoteContext~ instance. To satisfy this, there are two ways:
         * Compile Time Macro:
           #+begin_src scala
             import scala.quoted._

             inline def runf1: Int => String = ${ mcrImpl1 }
             def mcrImpl1(using QuoteContext): Expr[Int => String] =
               to((x: Expr[Int]) => '{ $x.toString })

             inline def runf2: String = ${ mcrImpl2 }
             def mcrImpl2(using QuoteContext): Expr[String] = {
               val f2: Expr[Int] => Expr[String] = from('{ (x: Int) => x.toString })
               f2('{2})
             }
           #+end_src

         * Multi-Stage Programming (inlucdes /runtime/ evaluation, see next section
           for details):
           1. Include dependency (this was inside dotty before, and they are now
              separated) ~"ch.epfl.lamp" %% "dotty-staging" % scalaVersion.value~.

           2. Then,
              #+begin_src scala
                import scala.quoted._
                import scala.quoted.staging._

                given Toolbox = Toolbox.make(getClass.getClassLoader)

                //// '{ (x: Int) => x.toString }
                val runf1: Int => String = run {
                  to((x: Expr[Int]) => '{ $x.toString })
                }

                //// (x: Expr[Int]) => '{ ((x: Int) => x.toString)($x) }
                val runf2: String = run {
                  val f2: Expr[Int] => Expr[String] = from('{ (x: Int) => x.toString })
                  f2('{2})
                }
              #+end_src

     - *LIMITATION* of ~from~ ::
       it does _NOT_ \beta{}-reduce when a lambda is called immediately
       #+begin_src scala
         inline def resultOfFrom: String = ${ resultOfFromImpl }
         def resultOfFromImpl(using QuoteContext): Expr[String] = {
           val f: Expr[Int] => Expr[String] = from('{ (x: Int) => x.toString })
           Expr(f('{2}).show)
         }
         // ((x: scala.Int) => x.toString()).apply(2)
       #+end_src

       + In some cases we _want to *REMOVE* the lambda from the code_,
         for this we provide the method ~Expr.betaReduce~ that turns a /TREE
         describing a function/ into a /FUNCTION mapping trees to trees/.
         #+begin_src scala
           inline def forBetaReduceExample: String = ${ forBetaReduceExampleImpl }

           def forBetaReduceExampleImpl(using QuoteContext): Expr[String] = {
             val afterBetaReduction = Expr.betaReduce('{ (x: Int) => x.toString })('{2})
             Expr(afterBetaReduction.show)
           }
           // 2.toString()
         #+end_src

       + ~Expr.betaReduce~ _IMPLEMENTATION_:
         The definition of ~Expr.betaReduce(f)(x)~ is *assumed* to be functionally the
         same as ~'{($f)($x)}~, however _it SHOULD *optimize* this call by returning
         the result of beta-reducing ~f(x)~ if ~f~ is a *KNOWN* lambda expression._
         ~Expr.betaReduce~ DISTRIBUTES applications of ~Expr~ over function arrows:
         #+begin_src scala
           Expr.betaReduce(_): Expr[(T1, ..., Tn) => R] => ((Expr[T1], ..., Expr[Tn]) => Expr[R])
         #+end_src

**** DONE Lifting Types
     CLOSED: [2020-06-09 Tue 03:04]
     - */Types/ are _NOT_ directly affected by the /phase consistency principle/.*
       + *It is possible to use /types/ defined at any level in any other level.*
         *But*, if a type is used in a SUBSEQUENT /stage/ it will need to be _lifted_
         to a ~Type~.

         * The resulting value of ~Type~ will be subject to /PCP/.
           Indeed, the definition of ~to~ above uses ~T~ in the NEXT /stage/,
           there is a /quote/ but NO /splice/ between the parameter binding of
           ~T~ and its usage. But the code can be rewritten by adding a binding
           of a ~Type[T]~ tag:
           #+begin_src scala
             def to[T, R: Type](f: Expr[T] => Expr[R])(using t: Type[T])(using QuoteContext): Expr[T => R] =
               '{ (x: $t) => ${ f('x) } }
           #+end_src
           - In this version of ~to~, the type of ~x~ is now the result of /splicing/
             the ~Type~ value ~t~.

           - This operation is /splice/ CORRECT -- there is one /quote/ and one
             /splice/ BETWEEN the _use_ of ~t~ AND its _definition_.

      - *To _AVOID_ clutter* (=MOTIVATION= of /lifting types/),
       the Scala implementation tries to _CONVERT *ANY* /type reference/ to a type
       ~T~ *in subsequent phases* to a /type-splice/,_ by *rewriting* ~T~ to
        (automatically) ~${summon[Type[T]] }~.
       + For instance, the user-level definition of ~to~:
         #+begin_src scala
           def to[T: Type, R: Type](f: Expr[T] => Expr[R])(using QuoteContext): Expr[T => R] =
             '{ (x: T) => ${ f('x) } }
         #+end_src

         would be *rewritten* to

         #+begin_src scala
           def to[T: Type, R: Type](f: Expr[T] => Expr[R])(using QuoteContext): Expr[T => R] =
             '{ (x: ${ summon[Type[T]] }) => ${ f('x) } }
         #+end_src
         * The summon query succeeds because
           - there is a /given instance/ of type ~Type[T]~ available
           - _the reference to that value is phase-correct._

         * If that was NOT the case,
           the phase inconsistency for ~T~ would be _reported_ as an error.

**** DONE Lifting Expressions
     CLOSED: [2020-06-24 Wed 00:52]
     - Consider the following implementation of a /staged interpreter/ that implements
       a compiler through staging.
       #+begin_src scala
         import scala.quoted._

         enum Exp {
           case Num(n: Int)
           case Plus(e1: Exp, e2: Exp)
           case Var(x: String)
           case Let(x: String, e: Exp, in: Exp)
         }
       #+end_src
       + The interpreted language consists of numbers ~Num~, addition ~Plus~, and
         variables ~Var~ which are bound by ~Let~.

     - Here are two sample expressions in the language:
       #+begin_src scala
         val exp    = Plus(Plus(Num(2), Var("x")), Num(4))
         val letExp = Let("x", Num(3), exp)
       #+end_src

     - Here's a compiler that maps an expression given in the interpreted language
       to /quoted/ Scala code of type ~Expr[Int]~.
       #+begin_src scala
         import scala.quoted._

         def compile(e: Exp, env: Map[String, Expr[Int]])(using QuoteContext): Expr[Int] = e match {
           case Num(n)          => Expr(n)
           case Plus(e1, e2)    => '{ ${ compile(e1, env) } + ${ compile(e2, env) } }
           case Var(x)          => env(x)
           case Let(x, e, body) => '{ val y = ${ compile(e, env) }; ${ compile(body, env + (x -> 'y)) } }
         }
       #+end_src

     - Running ~compile(letExp, Map.empty)~ would yield the following Scala code:
       #+begin_src scala
         '{ val y = 3; (2 + y) + 4 }
       #+end_src
       =from Jian=
       You can check this representation with the help of ~showExpr~ mentioned below.

     - The body of the first clause, ~case Num(n) => Expr(n)~, *looks SUSPICIOUS*.
       ~n~ is declared as an ~Int~, yet it is converted to an ~Expr[Int]~ with
       ~Expr.apply~.
       + Q :: Shouldn't ~n~ be /quoted/ (=from Jian= Use ~'n~ instead of ~Expr(n)~)?

       + A :: In fact this would _NOT_ work since replacing ~n~ by ~'n~ in the
               clause would *NOT* be /phase correct/ (=from Jian= violate PCP).

     - =from Jian=
       From the Q&A above we know it's better to find a easy way to _lift a value
       of ~T~ to ~Expr[T]~ with enough information hiding_. A good API for this is
       already hinted above -- use ~Expr.apply[T: Liftable](v: T)~ to do this.
       We'll talk about the knowledge about ~Expr.apply~ below.

     - The ~Expr.apply~ method is defined in package ~quoted~:
       #+begin_src scala
         package quoted

         object Expr {
           /** Lift a value into an expression containing the construction of that value */
           def apply[T: Liftable](x: T)(using QuoteContext): Expr[T] =
             summon[Liftable[T]].toExpr(x)

           // ...
         }
       #+end_src
       + This method says that values of types implementing the ~Liftable~ /type
         class/ can be converted ("lifted") to ~Expr~ values using ~Expr.apply~.

     - Dotty comes with /given instances/ of ~Liftable~ for several types including
       ~Boolean~, ~String~, and /ALL primitive number types/.
       + For example,
         ~Int~ values can be converted to ~Expr[Int]~ values by wrapping the
         value in a ~Literal~ /tree node/ (=from Jian= actually ~Literal(Constant(n))~).
           This makes use of the underlying /tree representation/ in the compiler
         *for efficiency*.
         * _BUT_ the ~Liftable~ instances are nevertheless NOT magic in the sense
           that they could all be defined in a user program *without knowing
           anything about the representation of ~Expr~ trees* (=from Jian= this
           can be done is because the fundamental types /given ~Liftable~ instances/
           are provided by Scala).
           - For instance, here is a possible instance of ~Liftable[Boolean]~:
             #+begin_src scala
               given Liftable[Boolean] {
                 def toExpr(b: Boolean) =
                   if (b) '{ true } else '{ false }
               }
             #+end_src

     - Once we can lift bits, we can work our way up.
       For instance, here is a possible implementation (=from Jian= of course,
       not the best way!) of ~Liftable[Int]~ that _does *NOT* use the underlying
       tree machinery_:
       #+begin_src scala
         given Liftable[Int] {
           def toExpr(n: Int): Expr[Int] = n match {
             case Int.MinValue    => '{ Int.MinValue }
             case _ if n < 0      => '{ - ${ toExpr(-n) } }
             case 0               => '{ 0 }
             case _ if n % 2 == 0 => '{ ${ toExpr(n / 2) } * 2 }
             case _               => '{ ${ toExpr(n / 2) } * 2 + 1 }
           }
         }
       #+end_src
       + =from Jian=
         Since the PCP is only for /free variables/, and ~'{ Int.MinValue }~ and
         ~'{ 0 }~ don't include any /free variables/, they are legal and no
         violation to PCP. We can see they other 3 branches includes /free
         variables/ and follow PCP!

     - Since ~Liftable~ is a /type class/, its instances can be conditional.
       For example, a ~List~ is /liftable/ _if its element type is_:
       #+begin_src scala
         given [T: Liftable : Type] as Liftable[List[T]] {
           def toExpr(xs: List[T]) = xs match {
             case head :: tail => '{ ${ Expr(head) } :: ${ Expr(tail) } }
             case Nil          => '{ Nil: List[T] }
           }
         }
       #+end_src

     - *In the end, ~Liftable~ _RESEMBLES_ very much a serialization framework.*
       Like the latter it can be derived systematically for all /collections/,
       /case classes/ and /enums/.

     - =TODO= =???= =TODO=
       Note also that the synthesis of type-tag values of type ~Type[T]~ is
       essentially the type-level analogue of /lifting/.

     - Using /lifting/, we can now give the missing definition of ~showExpr~ in
       the introductory example:
       #+begin_src scala
         def showExpr[T](expr: Expr[T])(using QuoteContext): Expr[String] = {
           val code: String = expr.show
           Expr(code)
         }
       #+end_src
       + That is, the ~showExpr~ /method/ _converts_ its ~Expr~ argument to a
         string (~code~), and *lifts* the result back to an ~Expr[String]~ using
         ~Expr.apply~.

**** TODO Lifting Types - =from Jian= _MECHANISM_
**** TODO Relationship with ~inline~ _Rationale of Design_
     - First Impression (=from Jian= I add this title):
       Seen by itself, principled metaprogramming in Dotty looks
       _MORE like_ a framework for /runtime metaprogramming/
       _THAN_ one for /compile-time metaprogramming/ with /macros/.

     - But combined with Dotty's ~inline~ feature it can be _turned into_ a
       compile-time system.
         The idea is that /macro elaboration/ can be understood as a *combination*
       of a /macro library/ and a /quoted program/.

     - Example used to illustrate the above discussion,
       here's the ~assert~ /macro/ again together with a program that CALLS
       ~assert~.
       #+begin_src scala
         object Macros {

           inline def assert(inline expr: Boolean): Unit =
             ${ assertImpl('expr) }

           def assertImpl(expr: Expr[Boolean])(using QuoteContext) =
             val failMsg: Expr[String] = Expr(s"failed assertion: ${expr.show}")
             '{ if !($expr) then throw new AssertionError($failMsg) }
         }

         object App {
           val program = {
             val x = 1
             Macros.assert(x != 0)
           }
         }
       #+end_src
       + The example is only /phase correct/ because ~Macros~ is a /global value/
         and as such *NOT subject to* _phase consistency checking_.
         * *Conceptually that's a bit unsatisfactory.*
           - If the PCP is so fundamental,
             it should be applicable without the /global value/ *exception*.

           - _BUT_ in the example as given this does not hold since both ~assert~
             and program call ~assertImpl~ *with a* /splice/ but *no* /quote/.

       + However, one could argue that the example is really missing an important
         aspect:
         The /macro library/ has to be compiled in a phase *prior to* the program
         using it, _BUT_ in the code above, /macro/ and /program/ are _defined
         together_.
           A more accurate view of /macros/ would be to have the _user program_ be
         in a /phase/ *after* the /macro definitions/, reflecting the fact that
         /macros/ have to be defined and compiled before they are used.
           Hence, conceptually the program part should be treated by the compiler
         *as if* (=from Jian= not real legal code?) it was quoted:
         #+begin_src scala
           val program = '{
             val x = 1
             ${ Macros.assertImpl('{ x != 0 }) }
           }
         #+end_src

     - If program is treated as a /quoted expression/, the call to ~Macro.assertImpl~
       becomes /phase correct/ even if _macro library_ and _program_ are
       conceptualized as local definitions.

     - Q :: But what about the call from ~assert~ to ~assertImpl~?
     - A :: Here, *we need a _tweak_ of the /typing rules/.*
            An ~inline~ function such as ~assert~ that contains a /splice/ operation
            *outside* an ENCLOSING /quote/ is called a /macro/.
              /Macros/ are supposed to be expanded in a SUBSEQUENT /phase/, i.e. in
            a /quoted context/. Therefore, they are also type checked *as if*
            they were in a /quoted context/.
       + For instance, the definition of ~assert~ is typechecked *as if* it
         appeared INSIDE /quotes/.
           _This makes the call from ~assert~ to ~assertImpl~ phase-correct,_
         even if we assume that both definitions are *local*.

     - The ~inline~ modifier is used to declare a ~val~ that is either a constant or
       is a parameter that will be a constant when instantiated.
         This aspect is also important for /macro expansion/.
         =from Jian= More details about this "important"?!?!?!

     - To get values out of expressions containing constants ~Expr~ provides the
       /methods/
       + ~unlift[T]~: return a value of ~Option[T]~
       + ~unliftOrError[T]~: return a value of ~T~

     - Wanted: avoid having incidental ~val~ bindings generated by the /inlining/
               of the ~def~
       Solution: it is *RECOMMENDED* to use /an ~inline~ parameter/.
       + To illustrate this, consider an implementation of the ~power~ function that
         makes use of a statically known exponent:
         #+begin_src scala
           inline def power(x: Double, inline n: Int) =
             ${ powerCode('x, 'n) }

           private def powerCode(x: Expr[Double], n: Expr[Int])(using QuoteContext): Expr[Double] =
             n.unlift match {
               case Some(m) => powerCode(x, m)
               case None    => '{ Math.pow($x, $y) }
             }

           private def powerCode(x: Expr[Double], n: Int)(using QuoteContext): Expr[Double] =
             if      (n == 0)     '{ 1.0 }
             else if (n == 1)     x
             else if (n % 2 == 0) '{ val y = $x * $x; ${ powerCode('y, n / 2) } }
             else                 '{ $x * ${ powerCode(x, n - 1) } }
         #+end_src

**** DONE Scope Extrusion
     CLOSED: [2020-06-24 Wed 02:43]
     - /Quotes/ and /splices/ are duals _as far as_ the PCP is concerned.

     - _But_ there is an *additional RESTRICTION* that needs to be imposed on
       /splices/ to guarantee /soundness/:
       code in /splices/ *must be _FREE_ of /side effects/.*
       + The restriction prevents code like this:
         #+begin_src scala
           var x: Expr[T] = ...
             '{ (y: T) => ${ x = 'y; 1 } }
         #+end_src
         This code, IF it was accepted, would *extrude* a reference to a /quoted
         variable/ ~y~ from its /scope/.
           This would subsequently allow access to a variable outside the scope
         where it is defined, which is LIKELY (=TODO= =???=) problematic.
         1. The code is clearly /phase consistent/, so we CANNOT use PCP to rule
           it out.

         2. Instead we postulate a FUTURE /effect system/ that can guarantee that
            /splices/ are pure.
            - In the absence of such a system we simply demand that /spliced
              expressions/ are pure *by convention*, and allow for *undefined
              compiler behavior* if they are not.

            - This is analogous to the status of pattern guards in Scala, which
              are also required, but NOT VERIFIED, to be pure.

     - /Multi-Stage Programming/ (=from Jian= next section) introduces one
       additional /method/ where you can *expand code at runtime* with a
       /method/ ~scala.quoted.staging.run~.
       + =from Jian=
         /Staging/ is *not* a part of the standard libary. Need to includes the
         dependency: ~"ch.epfl.lamp" %% "dotty-staging" % scalaVersion.value~

     - Call ~scala.quoted.staging.run~ in /splices/ is forbidden!
       This is a little bit tricky.
       + Use this example code to illustrate this trickiness:
         #+begin_src scala
           '{ (x: Int) => ${ run('x); 1 } }  // Not legal code
         #+end_src
         * This is /phase correct/, _BUT_ WILL lead us into *TROUBLE*.

         * Evaluate the /splice/ will reduce the expression ~run('x)~ to ~x~.
           But then the result is *no longer* /phase correct/.
           #+begin_src scala
             '{ (x: Int) => ${ x; 1 } }
           #+end_src

       + To *prevent* this /soundness hole/ it seems easiest to classify ~run~ as
         a /side-effecting operation/. It would thus be *prevented from appearing
         in /splices/.*
           In a base language with /side effects/ we would have to do this anyway:
         Since ~run~ runs _ARBITRARY_ code it can ALWAYS produce a /side effect/
         if the code it runs produces one.
         =TODO= =TODO= =TODO=

**** TODO Example Expansion
     - Assume we have two methods, one map that takes an ~Expr[Array[T]]~ and a
       function ~f~ and one ~sum~ that performs a sum by delegating to ~map~.
       #+begin_src scala
         object Macros {
           def map[T](arr: Expr[Array[T]], f: Expr[T] => Expr[Unit])
                     (using t: Type[T], qctx: QuoteContext): Expr[Unit] = '{
             var i: Int = 0
             while (i < ($arr).length) {
               val element: $t = ($arr)(i)
               ${f('element)}
               i += 1
             }
           }

           def sum(arr: Expr[Array[Int]])(using QuoteContext): Expr[Int] = '{
             var sum = 0
             ${ map(arr, x => '{sum += $x}) }
             sum
           }

           inline def sum_m(arr: Array[Int]): Int = ${sum('arr)}
         }
       #+end_src

**** DONE Find implicits within a macro
     CLOSED: [2020-06-24 Wed 02:45]
     Make implicit search available in a /quote context/ with
     ~scala.quoted.Expr.summon~:
     #+begin_src scala
       import scala.quoted._
       inline def setFor[T]: Set[T] = ${ setForExpr[T] }

       def setForExpr[T: Type](using QuoteContext): Expr[Set[T]] = {
         Expr.summon[Ordering[T]] match {
           case Some(ord) => '{ new TreeSet[T]()($ord) }
           case _         => '{ new HashSet[T] }
         }
       }
     #+end_src

**** TODO Relationship with Whitebox ~Inline~
     =from Jian= *I don't quite understand!!!*
     ~Inline~ documents inlining.
     The code below introduces a /whitebox inline method/ that can calculate
     either a value of /type/ ~Int~ or a value of /type/ ~String~.
     #+begin_src scala
       transparent inline def defaultOf(inline str: String) =
         ${ defaultOfImpl('str) }

       def defaultOfImpl(strExpr: Expr[String])(using QuoteContext): Expr[Any] =
         strExpr.unliftOrError match {
           case "int"    => '{1}
           case "string" => '{"a"}
         }

       // in a separate file
       val a: Int    = defaultOf("int")
       val b: String = defaultOf("string")
     #+end_src

**** DONE Defining a macro and using it in a single project
     CLOSED: [2020-06-24 Wed 03:50]
     - =from Jian=
       Of course, if the /macros/ project is *different* from the project use
       /macros/, the compilation is much simpler, and this doc does discuss.

     - It is possible to define /macros/ and use them in the *same project* as
       long as the implementation of the /macros/ does NOT have /runtime
       dependencies/ on code in the file where it is used.
       + It might still have /compile-time dependencies/ on
         /types/ and /quoted code/ that refers to the use-site file.

     - To provide the functionality of a project inlcudes both /macros/ and the
       code use them, Dotty provides a /transparent compilation mode/ where
       1. Files that try to expand a /macro/ but fail because the /macro/ has NOT
          been compiled yet are *suspended*.

       2. If there are any suspended files when the compilation ends,
          the compiler will *AUTOMATICALLY* RESTART compilation of the suspended
          files using the output of the previous (partial) compilation as /macro/
          classpath.

       3. In case *ALL* files are *suspended* due to /cyclic dependencies/ the
          compilation will *FAIL* with an error.

**** TODO Pattern matching on quoted expressions
***** TODO Quoted patterns
***** TODO Recovering precise types using patterns

**** TODO More details

*** TODO Staging
    # *Multi-Stage Programming*
**** API
**** Create a new Dotty project with staging enabled
**** Example

*** TODO TASTy Reflection
    # *TASTy Reflect*
**** API: From quotes and splices to TASTy reflect trees and back
***** Extractors
***** Obtaining and underlying argument
***** Positions
***** Tree Utilities
****** Let

**** More Examples

*** TODO TASTy Inspection
**** Inspecting TASTy files
**** Template project

** TODO OTHER NEW FEATURES
*** DONE Trait Parameters
    CLOSED: [2020-07-14 Tue 12:59]
    Dotty allows /traits/ to have /parameters/, just like /classes/ have /parameters/.

    - Example:
      #+begin_src scala
        trait Greeting(val name: String) {
          def msg = s"How are you, $name"
        }

        class C extends Greeting("Bob") {
          println(msg)
        }
      #+end_src

    - =IMPORTANT= Initialization Rule:
      /Arguments/ to a /trait/ are *evaluated immediately _BEFORE_ the /trait/ is
      initialized.*

    - One potential issue with /trait parameters/:
      *how to prevent ambiguities.*
      For instance, you might try to _extend ~Greeting~ TWICE, with DIFFERENT
      parameters._
      #+begin_src scala
        class D extends C with Greeting("Bill")  // error: parameter passed twice
      #+end_src
      Should this print ~"Bob"~ or ~"Bill"~? In fact this program is *ILLEGAL*,
      because it _VIOLATES the SECOND rule of the following for /trait parameters/:_
      1. If a /class/ ~C~ _extends_ a /parameterized trait/ ~T~, and
         IF its /superclass/ does NOT:
         THEN ~C~ must pass arguments to ~T~.
         ELSE ~C~ must not pass arguments to ~T~

      2. /Traits/ *must NEVER* PASS arguments to /parent traits/.

    - Here's /trait/ extending the /parameterized trait/ ~Greeting~.
      #+begin_src scala
        trait FormalGreeting extends Greeting {
          override def msg = s"How do you do, $name"
        }
      #+end_src
      + As is required, no arguments are passed to ~Greeting~.
        However, this poses an *ISSUE* when defining a /class/ that extends
        ~FormalGreeting~:
        #+begin_src scala
          class E extends FormalGreeting  // error: missing arguments for `Greeting`.
        #+end_src
        The correct way to write ~E~ is to extend both ~Greeting~ and ~FormalGreeting~
        (in either order):
        #+begin_src scala
          class E extends Greeting("Bob") with FormalGreeting
        #+end_src

**** TODO Reference
     For more info, see _Scala SIP 25_.

*** DONE Super Traits
    CLOSED: [2020-06-10 Wed 23:46]
    - /Traits/ are used in _TWO_ roles:
      + As *mixins* for other /classes/ and /traits/
      + As *types* of ~vals~, ~defs~, or ~parameters~

    - /Super traits/ is designed to address the topic:
      Some /traits/ are used primarily in the _first role_,
      and we usually do *NOT* want to see them in /inferred types/.
      + Example:
        #+begin_src scala
          trait Kind
          case object Var extends Kind
          case object Val extends Kind
          val x = Set(if condition then Val else Var)
        #+end_src
        The inferred type of ~x~ in Scala 2 is ~Set[Kind & Product & Serializable]~
        whereas one would have hoped it to be ~Set[Kind]~.
        * The reasoning for this particular type to be inferred is as follows:
          1. The /type/ of the conditional above is the /union type/ ~Val | Var~
             (in concept -- we don't actually have the union type syntax in Scala 2).

          2. A /union type/ is widened in /type inference/ to the *least supertype*
             that is _NOT_ a /union type/.
               In the example, this type is ~Kind & Product & Serializable~
             since all THREE /traits/ are /supertraits/ of both ~Val~ and ~Var~.
             So that type becomes the inferred element type of the set.

    - Scala 3 allows one to mark a /trait/ as a ~super~ /trait/, which means that
      it can be suppressed in /type inference/.
      #+begin_src scala
        // In Scala 3
        super trait S
        trait Kind
        case object Var extends Kind
        case object Val extends Kind
        val x = Set(if condition then Val else Var)
      #+end_src
      Now ~x~ has /inferred type/ ~Set[Kind]~ -- the common /super trait/ ~S~ does
      _NOT_ appear in the /inferred type/.

**** Super Traits
     - The traits ~scala.Product~, ~java.lang.Serializable~ and ~java.lang.Comparable~
       are *treated _AUTOMATICALLY_ as /super traits/.*
         Other /traits/ can _be turned into_ /super traits/, by adding the keyword
       ~super~ in front of /trait/, as shown above.

     - Every /trait/ *can be* declared as a /super trait/.

     - Typically /super traits/ are /traits/ that influence the implementation of
       inheriting /classes/ and /traits/ and that are _NOT usually used as types by
       themselves._
       + Two examples from the standard collection library:
         * ~IterableOps~,
           which provides method implementations for an ~Iterable~

         * ~StrictOptimizedSeqOps~,
           which optimises some of these implementations for sequences with
           efficient indexing.

     - =IMPORTANT=
       Generally, any /trait/ that is _extended recursively_ is a good candidate
       to be declared a /super trait/.

**** Retro-Fitting Scala 2 Libraries
     - To allow cross-building between Scala 2 and 3,
       ~super~ /traits/ can also be introduced by adding the ~@superTrait~
       annotation, which is _defined in package ~scala.annotation~._

     - Example:
       #+begin_src scala
         import scala.annotation.superTrait

         @superTrait trait StrictOptimizedSeqOps[+A, +CC[_], +C] ...
       #+end_src

     - The ~@superTrait~ annotation *will be deprecated and removed in some later
       version* of Scala _when cross-building with Scala 2 will NO LONGER be a
       concern._

**** TODO Rules for Inference
     - /Super traits/ *can be* given as /explicit types/ as usual.
       But they are often elided when types are inferred.
         Roughly, the rules for /type inference/ say that /super traits/ _are
       DROPPED from intersections WHERE POSSIBLE._

     - The precise rules are NOT follows:
       + When inferring a type of a type variable, or the type of a ~val~, or the
         /return type/ of a ~def~,

       + where that type is not higher-kinded,

       + and where ~B~ is its known /upper bound/ or ~Any~ if none exists:

       + If the /type inferred/ so far is of the form ~T1 & ... & Tn~ where n >= 1,
         replace the maximal number of ~Ti~'s by ~Any~, while ensuring that the
         resulting type is still a subtype of the bound ~B~.

       + However, do NOT perform this /widening/ if all types ~Ti~ can get replaced
         in that way.

     - The _last clause_ *ensures* that
       a single /super trait/ instance such as ~Product~ is not widened to
       ~Any~.
       + =IMPORTANT= =SUMMARY=
         /Super trait/ instances are _ONLY dropped_ when they appear _in conjunction
         with some other type._

**** Syntax

*** DONE Creator Applications
    CLOSED: [2020-07-14 Tue 13:33]
    - creator applications :: use simple _function call syntax_ to *create*
      /instances/ of a /class/, even if there is no ~apply~ /method/ implemented.

    - Example:
      #+begin_src scala
        class StringBuilder(s: String) {
          def this() = this("")
        }

        StringBuilder("abc")  // same as `new StringBuilder("abc")`
        StringBuilder()       // same as `new StringBuilder()`
      #+end_src

    - /Creator applications/ generalize a functionality provided _so far only for_
      /case classes/, but the mechanism how this is achieved is different.
        Instead of generating an ~apply~ /method/, the compiler adds a new possible
      interpretation to a function call ~f(args)~. The previous rules are:
      + Given a function call ~f(args)~,
        * if ~f~ is a /method/ applicable to ~args~,
          typecheck ~f(args)~ *UNCHANGED*,

        * otherwise, if ~f~ has an ~apply~ /method/ applicable to ~args~ as a member,
          continue with ~f.apply(args)~,

        * otherwise, if ~f~ is of the form ~p.m~ and there is an /implicit conversion/
          ~c~ applicable to ~p~ so that ~c(p).m~ is applicable to ~args~, continue with
          ~c(p).m(args)~

      + There's now a fourth rule following these rules:
        * if ~f~ is syntactically a /stable identifier/, and ~new f~ where ~f~ is
          interpreted as a /type identifier/ is applicable to ~args~, continue
          with ~new f(args)~.

    - Analogously, the possible interpretations of a /function call with type arguments
      ~f[targs]~ syntax/ are augmented with the following interpretation as a _FINAL
      fallback_:
      + if ~f~ is syntactically a /stable identifier/, and ~new f[targs]~ where ~f~
        is interpreted as a /type identifier/ is _well-typed_, continue with ~new
        f[targs]~.

**** Motivation
     - Leave out ~new~ *hides* an implementation detail _and_
       makes code more pleasant to *read*

     - Q :: What's the cost of this change?
     - A :: _Add a new rule_ (a fallback rule) to the interpretation of the
            /function call syntax/.

     - Q :: Why this cost is valuable?
     - A :: It increase the perceived regularity of the language, since /case classes/
            already provide /function call creation syntax/ (and are often defined for
            this reason alone).
            + =from Jian=
              though define a /case class/ only for its ~apply~ is not the right
              way to use /case classes/ -- /case classes/ are HEAVY because of
              tens of generated methods. However, people keep doing this.

**** Discussion
     An alternative design would auto-generate ~apply~ /methods/ for _non /case
     classes/._
     - =from Jian= From the first glance, this alternative design has one good
       point -- NO need to add new (fallback) rule for the interpretation of
       /function call syntax/.

     - However, this alternative design can *cause numerous problems*:
       + overloading ambiguities =TODO= Example???
       + overriding errors =TODO= Example???
       + shadowing of user-defined ~apply~ /methods/ by more specific auto-generated ones.
         =TODO= Example???

*** DONE Export Clauses -- =TODO= _Elaboration of Export Clauses_
    CLOSED: [2020-05-18 Mon 02:46]
    *An ~export~ clause defines aliases for selected members of an object.*
    - NOTE:
      Unless otherwise stated, _the term "class"_ in this discussion also
      _includes_ /object/ and /trait/ definitions.

    - Example:
      #+begin_src scala
        class BitMap
        class InkJet

        class Printer {
          type PrinterType
          def print(bits: BitMap): Unit = ???
          def status: List[String] = ???
        }

        class Scanner {
          def scan(): BitMap = ???
          def status: List[String] = ???
        }

        class Copier {
          private val printUnit = new Printer { type PrinterType = InkJet }
          private val scanUnit = new Scanner

          export scanUnit.scan
          export printUnit.{status => _, _}

          def status: List[String] = printUnit.status ++ scanUnit.status
        }
      #+end_src
      - Here the two ~export~ clauses define the following /export aliases/ in
        class ~Copier~:
        #+begin_src scala
          final def scan(): BitMap            = scanUnit.scan()
          final def print(bits: BitMap): Unit = printUnit.print(bits)
          final type PrinterType              = printUnit.PrinterType
        #+end_src

      - The exported members can be accessed inside ~Copier~ as well as from
        outside:
        #+begin_src scala
          val copier = new Copier
          copier.print(copier.scan())
        #+end_src

      - Syntax (similar to ~import~):
        #+begin_src scala
          export path . { sel_1, ..., sel_n }
          export given path . { sel_1, ..., sel_n }
        #+end_src
        + ~path~ here must be a /stable identifier/.

        + ~export~ is like ~import~.
          Synthetic members generated by compiler can't be exported.

      - A member is *eligible for being exported* if all of the following holds:
        + The _owner of the being exported member_ is *NOT* the /base class/ of
          /class/ (includes /object) that conttains the /export clause/.

        + The member does *NOT* /override a concrete definition/ that has as owner
          a /base class/ of the /class/ containing the /export clause/.

        + it is _accessible_ at the /export clause/ -- =from Jian= with proper
          modifier,

        + it is
          * *NOT* a /constructor/,
          * *NOT* the (*synthetic*) class part of an object,

        + it is a /given instance/ (or an old-style /implicit value/)
          iff the ~export~ is tagged with ~given~.

      - It is a /compile-time error/ if a simple or renaming selector does *not*
        identify any _eligible members_.

      - _Code generation_ triggered by ~export~'s:
        + /Type members/ are aliased by _type definitions_;
        + /Term members/ are aliased by _method definitions_;
        + Export aliases _copy_ the /type and value parameters/ of the members
          they refer to.
        + /Export aliases/ are always ~final~.
        + Aliases of /given instances/ are again defined as /givens/.
        + Aliases of /inline methods/ or values are again defined ~inline~.
        + There are *NO* OTHER /modifiers/ that can be given to an alias.

      - The _Code generation_ rule above has the following *CONSEQUENCES* for
        /overriding/:
        + Export aliases *cannot* be overridden, since they are ~final~.

        + Export aliases *cannot* override /concrete members/ in /base classes/,
          since they are not marked ~override~.

        + However, export aliases *can* _IMPLEMENT_ /deferred members/ (=from Jian=
          I think this term is the same as /abstract members/) of /base classes/.

      - TODO =RE-READ= TODO
        /Export aliases/ for /public value/ definitions that are accessed *WITHOUT
        referring to* /private values/ in the qualifier path are marked by the
        compiler as "stable" and their result types are the /singleton types/ of
        the aliased definitions.
          This means that they can be used as parts of /stable identifier paths/,
        even though they are _technically_ /methods/. For instance, the
        following is OK (_technically_, the consequence of ~export O.c~ generate
        a ~c~ /method/ -- when this is not a /export alias/, it can't be marked
        as stable):
        #+begin_src scala
          class C { type T }
          object O { val c: C = ... }
          export O.c
          def f: c.T = ...
        #+end_src

      - /Export clauses/ *can* appear in /classes/
        /Export clauses/ *can* appear _at the /top-level/._
        /Export clauses/ *CANNOT* appear _as a statement IN A BLOCK_.

      - =from Jian=
        A real world example:
        Sometimes we know something should be /static/, but make it /static/ can
        make the code looks WIERD! Before adding ~export~, we don't have a perfect
        solution. After adding the ~export~ feature, we can get a solution, though
        it is limited, can apply to most of the real world cases with this
        requirement.

        + In Scala 2, if we want to keep /static/ things /static/, we need to
          write like
          #+begin_src scala
            trait Calculator {
              val calculatorName: String
            }

            class AbcCalculator extends Calculator {
              override val calculatorName = AbcCalculator.calculatorName
            }

            object AbcCalculator {
              val calculatorName = "Abc"
            }
          #+end_src

        + In Scala 3, we still keep /static/ things /static/, but we can write
          #+begin_src scala
            trait Calculator {
              val calculatorName: String
            }

            final class AbcCalculator extends Calculator {
              export AbcCalculator.calculatorName
            }

            object AbcCalculator {
              val calculatorName = "Abc"
            }
          #+end_src
          * Of course, for providing a perfect solution like this with ~export~,
            One requirement is satisfied:
            - The being overridden field is /abstract/.

          * Another limitation is that we can't do similar thing to the /subclasses/
            of ~AbcCalculator~ -- the compiler will synthesize /final members/ for
            ~export~ clauses! However, this is not a real limitation -- a good design
            should avoid creating a long inheritance chain. Mostly, none level is
            enough.
            - However, if in future this limitation can be removed or not that strict,
              it is of course better for using. The only thing I doubt is that if this
              will be limited in theory and remove the limitation can create an unsound
              system.

**** DONE Motivation
     CLOSED: [2020-05-18 Mon 02:35]
     - It is a standard recommendation to *prefer composition over inheritance*.
       + This is really an application of /the principle of least power/:
         * /Composition/ treats components as _BLACKBOXES_
           _WHEREAS_
         * /Inheritance/ can _AFFECT the internal workings_ of components through
           /overriding/

       + Sometimes the _close coupling_ implied by /inheritance/ is the best solution
         for a problem, but where this is not necessary the looser coupling of composition
         is better.

     - So far, OO Language including Scala made it much easier to use /inheritance/
       than /composition/, which pushing programmers to a solution that is often
       too powerful (=from Jian= "too powerful" is not a good word in /the principle
       of least power/) as well as complicated (=from Jian= hard to verify in the
       concept of math).
       + For example, in Scala,
         * /inheritance/: Use ~extends~ clause

         * /composition/: Require a verbose elaboration of a sequence of forwarders.
           - =from Jian=
             Introduce ~export~ can mostly reduce one level of forwarders --
             + before: ~def mth = c.mth~
             + now: ~export c.mth~

       + ~export~ clauses redress the balance, and
         make /composition relationships/ *as CONCISE and EASY to* express as
         /inheritance relationships/.
         * Actually, ~export~ clauses is MORE FLEXIBLE than ~extends~ clauses --
           members can be _renamed_ or _ommited_.

         * =from Jian=
           ~export~ has some limitations.
           I'm not sure if the limitations must be there for soundness, but one of
           the reason of their existence that I can guess is with them, the system
           eliminate most of the potential interference between composition (~export~)
           and inheritance (~extends~).

     - /Export clauses/ also fill a gap opened by _the shift from /package objects/
       (DEPRECATED in Scala 3) to /toplevel definitions/._
       + In Scala 2, sometimes /package objects/ is created also with ~extends~ clauses.

       + /Toplevel definitions/ doesn't reside in semantics in a user-defined object,
         so they _can't inherit anyting_. However, ~export~ can be applied in
         toplevel, and make a similar result to the /package object/ _inheritance_ way.

**** DONE Syntax changes
     CLOSED: [2020-05-18 Mon 02:35]
**** TODO Elaboration of Export Clauses
     - Q :: How does the order of elaboration affect type checking?

     - Example:
       #+begin_src scala
         class B { val c: Int }
         object a { val b = new B }
         export a._
         export b._
       #+end_src
       + Q :: Is the ~export b._~ clause legal?
       + Q :: If yes, what does it export?
       + Q :: Is it equivalent to export ~a.b._~?
       + Q :: What about if we swap the last two clauses?
              #+begin_src scala
                export b._
                export a._
              #+end_src

     - A :: To avoid tricky questions like these,
            we _FIX the elaboration order of exports_ as follows.

     - Export clauses are processed when the type information of the enclosing
       object or class is completed. Completion *SO FAR* consisted of the
       following steps: TODO TODO TODO TODO TODO TODO
       1. Elaborate any annotations of the class.

       2. Elaborate the parameters of the class.

       3. Elaborate the self type of the class, if one is given.

       4. Enter all definitions of the class as class members, with types to be
          completed on demand.

       5. Determine the types of all parents of the class.

          *With export clauses, the following steps are added*

       6. Compute the types of all paths in export clauses in a context logically
          inside the class but not considering any imports or exports in that class.

       7. Enter export aliases for the eligible members of all paths in export clauses.

     - Conclusion :: a path of an /export clause/ *cannot* _refer to_ an alias
                     made available by _ANOTHER_ /export clause/ of the _SAME_
                     /class/.

*** DONE Opaque Type Alias
    CLOSED: [2019-09-13 Fri 02:50]
    /Opaque types aliases/ provide type abstraction without any runtime overhead.

    - Example:
      #+begin_src scala
        object Logarithms {

          opaque type Logarithm = Double

          object Logarithm {
            // These are the ways to lift to the Logarithm type

            def apply(d: Double): Logarithm = math.log(d)

            def safe(d: Double): Option[Logarithm] =
              if (d > 0.0) Some(math.log(d)) else None
          }

          // Extension methods define opaque type aliases' public APIs
          extension (x: Logarithm) {
            def toDouble: Double = math.exp(x)
            def + (y: Logarithm): Logarithm = Logarithm(math.exp(x) + math.exp(y))
            def * (y: Logarithm): Logarithm = Logarithm(x + y)
          }
        }
      #+end_src
      + ~Logarithm~ is the same as ~Double~ is *only known in the scope where
        ~Logarithm~ is defined* which in this case is object ~Logarithms~.
        * This in scope knowledge of their equivalence is very important!
            Without this knowledge, type-check will say functions ~apply~, ~safe~,
          ~toDouble~, ~+~, and ~*~ have wrong type signature, there there will
          be no simple way to override it.

      + Outside ~Logarithms~, ~Logarithm~ is treated as a _NEW abstract type_.
        * Legal operations example:
          #+begin_src scala
            import Logarithms.Logarithm

            val l = Logarithm(1.0)
            val l2 = Logarithm(2.0)
            val l3 = l1 * l2
            val l4 = l1 + l2
          #+end_src
          - =IMPORTANT=
            The ~import Predef.{any2stringadd => _}~ is necessary!!!
              Without this import clause, the universal ~+~ in ~Predef~ would
            take precedence over the ~+~ extension method in ~LogarithmOps~.
            + Solution: eliminate ~any2stringadd~ -- this is already in DEPRECATED
                        status.

        * Illegal operations example:
          #+begin_src scala
            val d: Double = l        // error: found: Logarithm, required: Double
            val l2: Logarithm = 1.0  // error: found: Double, required: Logarithm
            l * 2                    // error: found: Int(2), required: Logarithm
            l / l2                   // error: `/` is not a member fo Logarithm
          #+end_src

**** Bounds For Opaque Type Alias
     /Opaque type aliases/ can also come with /bounds/.
     Example:
     #+begin_src scala
       object Access {

         opaque type Permissions = Int
         opaque type PermissionChoice = Int
         opaque type Permission <: Permissions & PermissionChoice = Int

         extension (x: Permissions) def & (y: Permissions): Permissions = x | y
         extension (x: PermissionChoice) def | (y: PermissionChoice): PermissionChoice = x | y
         extension (granted: Permissions) def is(required: Permissions): Boolean = (granted & required) == required
         extension (granted: Permissions) def isOneOf(required: PermissionChoice): Boolean = (granted & required) != 0

         val NoPermission: Permission = 0
         val Read: Permission = 1
         val Write: Permission = 2
         val ReadWrite: Permissions = Read | Write
         val ReadOrWrite: PermissionChoice = Read | Write
       }
     #+end_src
     - The ~Access~ object defines THREE /opaque type aliases/:
       + ~Permission~,       representing a single permission,
       + ~Permissions~,      representing a conjunction (logical "and") of permissions,
       + ~PermissionChoice~, representing a disjunction (logical "or") of permissions.

     - /Type bound/ of ~Permission~ makes it known outside the ~Access~ object that
       ~Permission~ is a /subtype/ of the other two types. Hence, the following
       usage scenario type-checks:
       #+begin_src scala
         object User {
           import Access._

           case class Item(rights: Permissions)

           val roItem = Item(Read)  // OK, since Permission <: Permissions
           val rwItem = Item(ReadWrite)
           val noItem = Item(NoPermission)

           assert(!roItem.rights.is(ReadWrite))
           assert(roItem.rights.isOneOf(ReadOrWrite))

           assert(rwItem.rights.is(ReadWrite))
           assert(rwItem.rights.isOneOf(ReadOrWrite))

           assert(!noItem.rights.is(ReadWrite))
           assert(!noItem.rights.isOneOf(ReadOrWrite))
         }
       #+end_src
       + On the other hand, ~roItem.rights.isOneOf(ReadWrite)~ can't pass the type check.

**** TODO More details
***** Syntax
***** Type Checking
***** Realtionship to SIP 35

*** DONE Open Classes
    CLOSED: [2020-05-14 Thu 01:29]
    An ~open~ /modifier/ on a class signals that the class _is planned for
    extensions_.
    - Example:
      #+begin_src scala
        // File Writer.scala
        package p

        open class Writer[T] {

          /** Sends to stdout, can be overridden */
          def send(x: T) = println(x)

          /** Sends all arguments using `send` */
          def sendAll(xs: T*) = xs.foreach(send)
        }

        // File EncryptedWriter.scala
        package p

        class EncryptedWriter[T: Encryptable] extends Writer[T] {
          override def send(x: T) = super.send(encrypt(x))
        }
      #+end_src

    - An /open class/ typically comes with
      *some documentation that describes the internal calling patterns between
      methods of the class as well as hooks that can be overridden.*
      + We call this the /extension contract/ of the /class/.
        It is DIFFERENT FROM the /external contract/ between a /class/ and its
        users.

    - /Classes/ that are _not open_ *can still be extended*, *but only if* at least
      one of two alternative conditions is met:
      + The /extending class/ is in the *same source file as* the /extended class/.
        In this case, the extension is usually an _internal implementation matter_.

      + The language feature ~adhocExtensions~ is enabled for the extending class.
        If not enabled, the compiler will issue a "feature" warning when it see an
        extension with no ~open~ and not in the same source file.
        * ~import scala.language.adhocExtensions~
        * command line option ~-language:adhocExtensions~

**** DONE Motivation
     CLOSED: [2020-05-14 Thu 01:28]
     - When writing a class, there are _THREE possible expectations_ of
       /extensibility/:
       1. The class is intended to allow extensions.
          This means one should expect
          + a *carefully* worked out (=from Jian= this kind of class is like a public API)
          + *documented* /extension contract/ for the class. (=IMPORTANT=)

       2. Extensions of the class are _forbidden_,
          for instance to make correctness or security guarantees.

       3. There is no firm decision either way.
          The class is not a priori intended for extensions, but if others find
          it useful to extend on an ad-hoc basis, let them go ahead. However,
          they are on their own in this case.
          + Possible issue:
            There is _NO documented /extension contract/,_ and future versions of
            the class might break the extensions (by rearranging internal call
            patterns, for instance =from Jian= this happens in my everyday work).

     - The three cases are clearly distinguished by using
       + ~open~ for 1
       + ~final~ for 2
       + _no modifier_ for 3

     - _It is GOOD PRACTICE to *avoid* ad-hoc extensions in a code base,_
       since they tend to lead to fragile systems that are hard to evolve.

     - But there are _still some situations_ where these extensions are *USEFUL*.
         That's why /ad-hoc extensions/ are permitted, but only if there is an
       explicit opt-in via a language feature import.
       + for instance,
         * to _mock_ classes in tests,
         * to _apply temporary patches_ that add features or fix bugs in library classes.

**** DONE Details
     CLOSED: [2020-05-14 Thu 01:14]
     - ~open~ is a /soft modifier/.
       It is treated as a normal identifier _unless it is in modifier position._

     - An ~open~ /class/ *CANNOT BE* ~final~ or ~sealed~.

     - /Traits/ or /abstract classes/ are *always ~open~,* so ~open~ is redundant
       for them.

**** DONE Relationship with ~sealed~
     CLOSED: [2020-05-14 Thu 01:20]
     - A class that is _NEITHER abstract NOR open_ is SIMILAR TO a /sealed class/:
       it can still be extended, but ONLY _in the same compilation unit_.

     - The _DIFFERENCE_ is what happens
       if an extension of the class is attempted _in another compilation unit_.
       + For a /sealed class/, this is an *error*

       + for a /simple non-open class/, this is still permitted provided
         * the ~adhocExtensions~ feature is enabled
         * otherwise, it gives a *warning*.

**** DONE Migration
     CLOSED: [2020-05-14 Thu 01:16]
     - ~open~ is a NEW modifier in Scala 3.

     - _To allow /cross compilation/ between Scala 2.13 and Scala 3.0 WITHOUT /warnings/,_
       the /feature warning/ for /ad-hoc extensions/ is produced only under ~-strict~.
       + It will be produced by default from Scala 3.1 on.

*** DONE Parameter Untupling
    CLOSED: [2019-12-31 Tue 00:56]
    For data like ~val xs: List[(Int, Int)]~,
    - In Scala 2.x,
      use _EXPLICIT_ /pattern matching/ (partial function) decomposition:
      #+BEGIN_SRC scala
        xs map {
          case (x, y) => x + y
        }
      #+END_SRC

    - Dotty allows the syntax:
      #+BEGIN_SRC scala
        xs map {
          (x, y) => x + y
        }

        // OR, EQUIVALENTLY:
        xs.map(_ + _)
      #+END_SRC

    - Generally, a /function value/ with *n > 1 parameters* is _converted to_ a
      /pattern-matching closure/ using ~case~ if the expected type is a /unary
      function type/ of the form ~((T_1, ..., T_n)) => U~.

**** Reference

*** DONE Kind Polymorphism
    CLOSED: [2020-07-24 Fri 23:48]
    - Normally /type parameters/ in Scala are _partitioned into_ /kinds/.
      + /First-level types/ are /types/ of /values/.
      + /Higher-kinded types/ are /type constructors/ such as ~List~ or ~Map~.

    - The /kind/ of a /type/ is indicated by the /TOP type/ of which it is a
      /subtype/.
      + /First-level types/ are /subtypes/ of ~Any~.

      + /Higher-kinded types/:
        * /Covariant/ SINGLE argument /type constructors/, such as ~List~, are
          /subtypes/ of ~[+X] =>> Any~;

        * The ~Map~ /type constructor/ is a /subtype/ of ~[X, +Y] =>> Any~.

    - =from Jian=
      I remember when learning Haskell (long ago, need verify this memory by
      checking the book I read), we call types /kind-0 type/, /kind-1 type/,
      etc. When /kind/ is greater than 0, we call them /higher-kinded types/.

    - A /type/ can be used ONLY as prescribed by its /kind/.
      + /Subtypes/ of ~Any~ *cannot* be applied to type arguments
      + whereas /subtypes/ of ~[X] =>> Any~ *must* be applied to a /type argument/,
        unless they are passed to /type parameters/ of the SAME /kind/.

    - Sometimes we would like to have /type parameters/ that can have _more than one_
      /kind/, for instance to define an /implicit value/ that works for _parameters
      of *ANY* /kind/._
      + This is now possible through a form of (subtype) kind polymorphism. Kind
        polymorphism relies on the special type ~scala.AnyKind~ that can be used
        as an upper bound of a type. ~def f[T <: AnyKind] = ...~

    - The actual /type arguments/ of ~f~ can then be /types/ of ARBITRARY /kinds/.
      So the following would all be legal:
      #+begin_src scala
        f[Int]
        f[List]
        f[Map]
        f[[X] =>> String]
      #+end_src

    - We call /type parameters/ and /abstract types/ with an ~AnyKind~ /upper bound/
      /any-kinded types/.

    - Since the ACTUAL /kind/ of an /any-kinded type/ is unknown, its usage must
      be heavily restricted:
      An /any-kinded type/ can be neither the type of a value, nor can it be
      instantiated with /type parameters/. So about the only thing one can do with
      an /any-kinded type/ is to pass it to another /any-kinded type argument/.
      Nevertheless, this is enough to achieve some interesting generalizations
      that work across /kinds/, typically through advanced uses of /implicits/.

    - (todo: insert good concise example)
      =from Jian= Official =TODO= :-)

    - Some technical details:
      ~AnyKind~ is a /synthesized class/ just like ~Any~, but *WITHOUT* any /members/.
      + It _extends NO other /class/._

      + It is declared ~abstract~ and ~final~,
        so it can be *NEITHER /instantiated/ NOR /extended/.*

    - ~AnyKind~ plays a special role in /Scala's subtype system/:
      + It is a /supertype/ of *ALL other* /types/ no matter what their /kind/ is.

      + It is also assumed to be /kind-compatible/ with *ALL other* /types/.

      + Furthermore,
        * ~AnyKind~ is *treated as* a /higher-kinded type/ (so it CANNOT be used
          as a /type/ of values),

        * _BUT_ at the SAME TIME it has *NO /type parameters/* (so it cannot be
          instantiated).

    - *NOTE*:
      This feature is considered _EXPERIMENTAL but STABLE_
      AND
      it can be disabled under /compiler flag/ (i.e. =-Yno-kind-polymorphism=).

*** DONE Tupled Function - =TODO= =RE-READ=
    CLOSED: [2020-07-14 Tue 15:15]
    # Subtitle "Tupled Function" should be removed!!! =FIX-DOC=
    =from Jian= =Rephrase=
    - Requirement:
      GENERALIZE some operation on *ALL* /function types/

    - Solutions:
      + In Scala 2:
        Since /functions/ bounded to /arities/ *up to* 22, it was possible and
        the solution is straightforward.

      + In Scala 3:
        Since /functions/ and /tuples/ generalized to /arities/ *above* 22,
        /overloading/ is NOT an option anymore.
        * The /type class/ ~TupleFunction~ provides a way to abstract DIRECTLY
          over a /function/ of *ANY* /arity/ CONVERTING it to an EQUIVALENT
          /function/ that *receives ALL /arguments/ in a SINGLE /tuple/.*

    - XX
      #+begin_src scala
        /** Type class relating a `FunctionN[..., R]` with an equivalent tupled function `Function1[TupleN[...], R]` ,*
         \ast{}  @tparam F a function type
         \ast{}  @tparam G a tupled function type (function of arity 1 receiving a tuple as argument)
         */
        @implicitNotFound("${F} cannot be tupled as ${G}")
        sealed trait TupledFunction[F, G] {
          def tupled(f: F): G
          def untupled(g: G): F
        }
      #+end_src

    - The compiler will synthesize an /instance/ of ~TupledFunction[F, G]~ if:
      + ~F~ is a /function type/ of arity ~N~

      + ~G~ is a /function/ with a _single /tuple/ argument_ of size ~N~
        and
        its /types/ are equal to the /arguments/ of ~F~

      + The /return type/ of ~F~ is EQUAL TO the /return type/ of ~G~

      + ~F~ and ~G~ are the same sort of function
        (both are ~(...) => R~ or both are ~(...) ?=> R~)

      + If only one of ~F~ or ~G~ is /instantiated/ the second one is /inferred/.

**** Examples
     - ~TupledFunction~ can be used to GENERALIZE the ~Function1.tupled~, ...
       ~Function22.tupled~ /methods/ to /functions/ of _ANY_ /arities/.
       The following defines ~tupled~ as /extension method/ ([[https://github.com/lampepfl/dotty/blob/master/tests/run/tupled-function-tupled.scala][full example]]).
       #+begin_src scala
         /** Creates a tupled version of this function: instead of N arguments,
          \ast{}  it accepts a single [[scala.Tuple]] with N elements as argument.
          *
          \ast{}  @tparam F the function type
          \ast{}  @tparam Args the tuple type with the same types as the function arguments of F
          \ast{}  @tparam R the return type of F
          */
         extension [F, Args <: Tuple, R](f: F)
           def tupled(using tf: TupledFunction[F, Args => R]): Args => R = tf.tupled(f)
       #+end_src

     - ~TupledFunction~ can be used to GENERALIZE the ~Function.untupled~ to a
       /function/ of ANY /arities/ ([[https://github.com/lampepfl/dotty/blob/master/tests/run/tupled-function-untupled.scala][full example]])
       #+begin_src scala
         /** Creates an untupled version of this function: instead of a single argument of type [[scala.Tuple]] with N elements,
          \ast{}  it accepts N arguments.
          *
          \ast{}  This is a generalization of [[scala.Function.untupled]] that work on functions of any arity
          *
          \ast{}  @tparam F the function type
          \ast{}  @tparam Args the tuple type with the same types as the function arguments of F
          \ast{}  @tparam R the return type of F
          */
         extension [F, Args <: Tuple, R](f: Args => R)
           def untupled(using tf: TupledFunction[F, Args => R]): F = tf.untupled(f)
       #+end_src

     - ~TupledFunction~ can also be used to GENERALIZE the ~Tuple1.compose~ and
       ~Tuple1.andThen~ /methods/ to ~compose~ functions of *larger* /arities/ and
       with /functions/ that return /tuples/.
       #+begin_src scala
         /** Composes two instances of TupledFunction into a new TupledFunction, with this function applied last.
          *
          \ast{}  @tparam F a function type
          \ast{}  @tparam G a function type
          \ast{}  @tparam FArgs the tuple type with the same types as the function arguments of F and return type of G
          \ast{}  @tparam GArgs the tuple type with the same types as the function arguments of G
          \ast{}  @tparam R the return type of F
          */
         extension [F, G, FArgs <: Tuple, GArgs <: Tuple, R](f: F)
           def compose(g: G)(using tg: TupledFunction[G, GArgs => FArgs],
                                   tf: TupledFunction[F, FArgs => R]): GArgs => R =
             (x: GArgs) => tf.tupled(f)(tg.tupled(g)(x))
       #+end_src

*** DONE ~threadUnsafe~ Annotation
    CLOSED: [2019-12-31 Tue 04:24]
    When the compiler see a ~@threadUnsafe lazy val~, it can pick a faster
    mechanism to do the initialization.

    - =from Jian= TODO TODO TODO
      Does this mean before introducing the ~threadUnsafe~ annotation, we only
      have one mechanism that initialize all ~lazy val~ in a /thread safe/
      way???

**** Examples
     #+begin_src scala
       import scala.annotation.threadUnsafe

       class ThreadUnsafeExample {
         @threadUnsafe lazy val x: Int = 1
       }
     #+end_src

*** DONE New Control Syntax
    CLOSED: [2020-07-22 Wed 03:04]
    #+begin_src scala
      if x < 0 then
        "negative"
      else if x == 0
        "zero"
      else
        "positive"

      if x < 0 then -x else x

      while x >= 0 do x = f(x)

      for x <- xs if x > 0
      yield x * x

      for
        x <- xs
        y <- ys
      do
        println(x + y)
    #+end_src
    - =from Jian= The rules in details are listed in the docs.

    - The rules in details:
      + Two choices for new ~if~ syntax:
        * with a ~then~ that FOLLOWS the ~if~-condition
        * with proper INDENTATION

      + ~while~-loop with ~do~ following the ~while~-condition
        =from Jian=
        Remember:
        ~do ... while~ syntax is _removed_ from Scala 3.
        In Scala 3, ~do~ will only show up in the _new control syntax_.

      + For the enumerators of ~for~-expression,
        * /comprehensions/ still use ~yield~
        * /side effect loops/ use ~do~

**** Rewrites
     The Dotty compiler _can rewrite_ source code bidirectionally
     - old to new: option =-rewrite -new-syntax=
     - new to old: option =-rewrite -old-syntax=

*** TODO Optional Braces
    *As an /experimental feature/,*
    Scala 3 _enforces some rules on indentation_ and _allows *SOME* occurrences of
    braces {...} to be optional_. =from Jian= WHY NOT *ALL*???

    - It can be *turned off* with the /compiler flag/ =-noindent=.

    - Benefits:
      1. Some badly indented programs are *ruled out*,
         which means they are _flagged with WARNINGS._

      2. Some occurrences of braces ~{...}~ are *made optional*.
         Generally, the rule is that adding a pair of optional braces will NOT
         change the meaning of a well-indented program.

**** DONE Indentation Rules
     CLOSED: [2020-07-22 Wed 03:18]
     - The compiler
       + enforces *TWO* rules for well-indented programs,
       + flagging violations as _WARNINGS_. =from Jian= WHY *NOT* Error???

     - The two rules are:
       1. In a brace-delimited region, no statement is allowed to start to the left
          of the first statement after the opening brace that starts a new line.
          This rule is helpful for finding missing closing braces. It prevents
          errors like:
          #+begin_src scala
            if (x < 0) {
              println(1)
              println(2)

            println("done")  // error: indented too far to the left
          #+end_src

       2. If /significant indentation/ is _turned off_ (i.e. under Scala-2 mode OR
          under ~-noindent~) and we are at the start of an indented sub-part of an
          expression, and the indented part ends in a newline, the next statement
          must start at an indentation width less than the sub-part. This prevents
          errors where an opening brace was forgotten, as in
          #+begin_src scala
            if (x < 0)
              println(1)
              println(2)   // error: missing `{`
          #+end_src

     - These rules still leave _a lot of leeway_ how programs should be indented.
       For instance, they do *NOT impose* any restrictions on
       + indentation within expressions,

       + all statements of an indentation block line up exactly.
         =from Jian= I think this SHOULD be enforced

     - The rules are _generally helpful in pinpointing_ the root cause of errors
       related to _missing opening or closing braces_.

**** TODO Optinal Braces
     - The *compiler will insert* <indent> or <outdent> tokens at certain /line
       breaks/.
         Grammatically, pairs of <indent> and <outdent> tokens have the _SAME_
       effect as pairs of braces ~{~ and ~}~.

     - =FIX-DOC= (if -> is)
       If an <outdent> is inserted, the top element if popped from IW.

**** TODO Optinal Braces Around Template Bodies
**** DONE Spaces vs Tabs
     CLOSED: [2019-12-29 Sun 03:29]
     - _Mix SPACES and TABS is legal._
       However, there is no rule defined about how many SPACES equals to a TAB, or
       vice versa. This means
       + "2 tabs, fllowed by 4 spaces" is strictly less than "2 tabs, followed by
         5 spaces",

       + BUT "2 tabs, followed by 4 spaces" is *incomparable*
         * to "6 tabs"
           or
         * to "4 spaces, followed by 2 tabs".

     - *CAUTION*:
       NOT all the legal ways are recommended!!!
       *Do NOT MIX Spaces and Tabs!!!*

**** TODO Indentation and Braces
**** DONE Special Treatment of Case Clauses
     CLOSED: [2020-07-22 Wed 03:36]
     - The _INDENTATION RULES_ for
       ~match~ expressions
       AND
       ~catch~ clauses
       are refined as follows:
       + An _indentation region_ is *OPENED*
         after a ~match~ or ~catch~ ALSO if the following ~case~ appears at the
         _indentation width that's current_ for the ~match~ (or ~catch~) itself.

       + In that case, the _indentation region_ *CLOSES*
         at the first token at that *SAME* _indentation width_ that is
         *not* a ~case~, or at any token with a *SMALLER* _indentation width_,
         whichever comes first.

     - Legal form (the ~println~ in example do not belong to ~match~ block)
       + Next leval indentation:
         #+begin_src scala
           x match
             case 1 => print("I")
             case 2 => print("II")
             case 3 => print("III")
             case 4 => print("IV")
             case 5 => print("V")

           println(".")
         #+end_src

       + Same level indentation:
         #+begin_src scala
           x match
           case 1 => print("I")
           case 2 => print("II")
           case 3 => print("III")
           case 4 => print("IV")
           case 5 => print("V")

           println(".")
         #+end_src

**** TODO The End Marker
***** DONE When to Use End Markers
      CLOSED: [2020-07-22 Wed 03:43]
      - It is recommended that /end markers/ _are used for_ CODE where
        the extent of an indentation region is *NOT* immediately apparent "at a
        glance".

      - People will have different preferences what this means,
        BUT one can _nevertheless_ give some guidelines that stem from experience.

      - An /end marker/ makes sense if
        + the construct *contains blank lines*, or
        + the construct is *long*, say _15-20 lines or more_,
        + the construct ends *heavily indented*, say _4 indentation levels or more_.

      - If none of these above criteria apply,
        it's often better to *not* use an /end marker/
        since the code will be just as clear and more concise.

      - If there are _SEVERAL ending regions_ that _SATISFY one of the criteria
        above_, =FIX-DOC= (reason -> region)
        we usually need an /end marker/ *ONLY for the _outermost_ closed region*.
        + Usually *better avoid* /CASCADES of end markers/ like the example above.

***** TODO Syntax

**** TODO Example
**** TODO Settings and Rewrites
**** TODO Variant: Indentation Marker ~:~
     NOT STABLE -- Learn when this feature is stable!!!

*** TODO Explicit Nulls
    Explicit nulls is an /opt-in feature/ that _modifies the Scala type system_,
    which *makes /reference types/ (anything that extends ~AnyRef~) non-nullable.*

    - opt-in feature :: A feature need to enabled via a flag.
      + For this /explicit nulls/ feature, the flag is ~-Yexplicit-nulls~.

    - After introducing this feature, some old style code will no longer typecheck:
      #+begin_src scala
        val x: String = null  // error: found `Null`, but required `String`
      #+end_src

      Instead, if consider the code above is a piece of Scala 2 code which can
      typecheck, translate it into Scala 3 form:
      #+begin_src scala
        val x: String | Null = null
      #+end_src

**** DONE New Type Hierarchy
     CLOSED: [2019-12-31 Tue 04:05]
     - NEW - Without /explicit nulls/:
       ~Null~ is the subtype of all ~AnyRef~ subtypes.
       The only subtype of ~Null~ is the /bottom type/ ~Noting~.

     - OLD - With /explicit nulls/:
       ~Null~ is a subtype of ~Any~.
       Its only subtype doesn't change, still ~Noting~.

     - Of course, the /NEW type hierarchy/ descried above is the one for typechecker
       -- before /type erasure/.
         After /type erasure/, ~Null~, as JVM enforced, remains a /subtype/ of
       all /reference types/

**** DONE Unsoundness
     CLOSED: [2020-03-21 Sat 02:26]
     The new type system is unsound with respect to ~null~.
     Enforcing /sound initialization/, which is can be done, is a non-goal of
     this proposal.

     - The /unsoundness/ happens because uninitialized fields in a class start out
       as ~null~:
       #+begin_src scala
         // -Yexplicit-nulls
         class C with
           val f: String = foo(f)
           def foo(f2: String): String = f2

         val c = new C
         // c.f == "field is null"
       #+end_src

**** DONE Equality
     CLOSED: [2019-12-31 Tue 02:43]
     - NOT Allowed:
       Compare a value of ~AnyRef~ /subtypes/ with ~null~ is not allowed!!!
       The related operators are ~==~, ~!=~, ~eq~, and ~ne~.

     - ~null~ can _only_ be compared with values of type
       + ~Null~
       + nullable union ~(T | Null)~
       + ~Any~ type.

     - For some reason, if we really want to compare ~null~ with non-null values,
       we can use /cast/. For example,
       #+begin_src scala
         val x: String = ???
         val y: String | Null = ???

         x == null        // error: Values of types String and Null cannot be compared with == or !=
         x eq null        // error
         "hello" == null  // erro

         y == null  // ok
         y == x     // ok

         (x: String | Null) == null  // ok
         (x: Any) == null            // ok
       #+end_src

**** DONE Working with ~Null~
     CLOSED: [2019-12-31 Tue 04:01]
     To make working with nullable values easier, we *propose* adding a few
     utilities to the standard library. So far, we have found the following
     useful:
     - An extension method ~.nn~ to "cast away" nullability
       #+begin_src scala
         def[T] (x: T|Null) nn: x.type & T =
           if (x == null) throw new NullPointerException("tried to cast away nullability, but value is null")
           else           x.asInstanceOf[x.type & T]
       #+end_src
       This means that given ~x: String|Null~, ~x.nn~ has type ~String~, so we
       can call all the usual methods on it. Of course, ~x.nn~ will _throw a
       NPE_ if ~x~ is ~null~.
         *Don't use ~.nn~ on /mutable variables/ DIRECTLY*, which may introduce
       unknown value into the type. TODO TODO TODO ???

**** TODO Java Interop
***** ~UncheckedNull~

**** TODO Flow Typing
***** Logical Operators
***** Inside Conditions
***** Match Case
***** Mutable Variable
***** Unsupported Idioms

**** TODO Binary Compatibility

*** TODO Safe Initialization
    Dotty implements *experimental* /safe initialization check/, which can be
    enabled by the compiler option ~-Ycheck-init~.

**** DONE A Quick Glance
     CLOSED: [2020-07-25 Sat 00:58]
     To get a feel of how it works, we first show several examples below.

***** DONE Parent-Child Interaction
      CLOSED: [2020-07-25 Sat 00:52]
      #+begin_src scala
        abstract class AbstractFile {
          def name: String

          val extension: String = name.substring(4)
        }

        class RemoteFile(url: String) extends AbstractFile {
          val localFile: String = s"${url.##}.tmp"  // error: usage of `localFile` before it's initialized
          def name: String = localFile
        }
      #+end_src
      The checker will report:
      #+begin_src text
        -- Warning: tests/init/neg/AbstractFile.scala:7:4 ------------------------------
        7 |	    val localFile: String = url.hashCode + ".tmp"  // error
          |	        ^
          |    Access non-initialized field value localFile. Calling trace:
          |     -> val extension: String = name.substring(4)	[ AbstractFile.scala:3 ]
          |      -> def name: String = localFile            	[ AbstractFile.scala:8 ]
      #+end_src
      + _FIX_:
        =from Jian=
        * Make ~localFile~ a ~lazy val~.
        * I'm not sure this fix is a fundamental fix, OR just hide a possible defect?

***** DONE Inner-Outer Interaction
      CLOSED: [2020-07-25 Sat 00:54]
      #+begin_src scala
        object Trees {
          class ValDef { counter += 1 }
          class EmptyValDef extends ValDef
          val theEmptyValDef = new EmptyValDef
          private var counter = 0  // error
        }
      #+end_src
      The checker will report:
      #+begin_src text
        -- Warning: tests/init/neg/trees.scala:5:14 ------------------------------------
        5 |  private var counter = 0  // error
          |              ^
          |             Access non-initialized field variable counter. Calling trace:
          |              -> val theEmptyValDef = new EmptyValDef    [ trees.scala:4 ]
          |               -> class EmptyValDef extends ValDef       [ trees.scala:3 ]
          |                -> class ValDef { counter += 1 }	        [ trees.scala:2 ]
      #+end_src
      + _FIX_: =from Jian=
        Move the ~private var counter = 0~, and make it the first line in ~Trees~
        can

***** DONE Functions
      CLOSED: [2020-07-25 Sat 00:58]
      #+begin_src scala
        abstract class Parent {
          val f: () => String = () => this.message
          def message: String
        }

        class Child extends Parent {
          val a = f()
          val b = "hello"           // error
          def message: String = b
        }
      #+end_src
      The checker reports:
      #+begin_src text
        -- Warning: tests/init/neg/features-high-order.scala:7:6 -----------------------
        7 |  val b = "hello"           // error
          |      ^
          |Access non-initialized field value b. Calling trace:
          | -> val a = f()                              	[ features-high-order.scala:6 ]
          |   -> val f: () => String = () => this.message	[ features-high-order.scala:2 ]
          |    -> def message: String = b	                [ features-high-order.scala:8 ]
      #+end_src

**** DONE Design Goals
     CLOSED: [2020-07-25 Sat 03:25]
     - We establish the following design goals:
       + Sound ::
         checking _ALWAYS_ terminates,
         and is /sound/ for common and reasonable usage (over-approximation =???=)

       + Expressive ::
         support common and reasonable initialization patterns

       + Friendly ::
         * simple rules
         * minimal syntactic overhead
         * informative error messages

       + Modular ::
         modular checking, *NO* analysis *beyond* _project boundary_

       + Fast ::
         instant feedback

       + Simple ::
         no changes to core type system, explainable by a simple theory

     - By reasonable usage, we _INCLUDE_ the following use cases (but *not restricted
       to* them):
       *During initialization*
       + *Access* /fields/ on *this* and _outer_ *this*
       + *Call* /methods/ on *this* and _outer_ *this*
       + *Instantiate* /inner class/ and *call* /methods/ on such /instances/
       + *Capture* /fields/ in /functions/

**** TODO Principles and Rules
     - To achieve the goals, we uphold _THREE_ *fundamental principles*:
       + stackability
       + monotonicity
       + scopability

     - Stackability ::
       objects are initialized in /stack order/:
       if the /object/ ~b~ is created during the initialization of /object/ ~a~,
       then all /fields/ of ~b~ should become *initialized BEFORE* OR *at the same
       time* as ~a~.
       + Scala enforces this property _in syntax_ by demanding that all /fields/
         are *initialized at the end* of the /primary constructor/,
         *EXCEPT* for the language feature below: ~var x: T = _~

       + /Control effects/ such as /exceptions/ may *break* this property, as the
         following example shows:
         #+begin_src scala
           class B {
             throw new MyException(this)
             val a: Int = 1
           }

           class MyException(val b: B) extends Exception("")

           class A {
             val b = try { new B } catch { case myEx: MyException => myEx.b }
             println(b.a)
           }
         #+end_src
         * In the code above, the _control effect_ *teleport* the _uninitialized
           value_ wrapped in an /exception/.
             In the implementation, we avoid the problem by ensuring that the
           values that are thrown *MUST BE* /transitively initialized/.

           - transitively initialized :: =TODO= ??? =TODO= NO in the Docs

         * =from Jian= not in the official doc
           *WARNING* message:
           #+begin_src text
             [warn]  2 |  throw new MyException(this)
             [warn]    |                        ^^^^
             [warn]    |Promote the value under initialization to fully-initialized. Calling trace:
             [warn]    | -> throw new MyException(this)      [ SafeInitialization.scala:2 ]
             [warn] one warning found
           #+end_src

     - Monotonicity ::
       The _initialization status_ of an /object/ *should _NOT_ go backward*:
       /initialized fields/ continue to be initialized, a /field/ points to an
       /initialized object/ may *not* later point to an object _UNDER initialization_.
       As an example, the following code will be rejected:
       #+begin_src scala
         trait Reporter {
           def report(msg: String): Unit
         }

         class FileReporter(ctx: Context) extends Reporter {
           ctx.typer.reporter = this  // `ctx` now reaches an uninitialized object
           val file: File = new File("report.txt")
           def report(msg: String) = file.write(msg)
         }
       #+end_src
       + In the code above,
         *SUPPOSE* ~ctx~ points to a /transitively initialized object/.
         * Now the assignment at line 3 makes ~this~, which is *not* _fully
           initialized_, reachable from ~ctx~.
             This makes /field/ usage *DANGEROUS*, as it may INDIRECTLY reach
           /uninitialized fields/.

       + /Monotonicity/ is based on a well-known technique called /heap monotonic
         typestate/ to *ensure* /soundness/ in the presence of aliasing [1].
           Otherwise, either /soundness/ will be compromised or we have to *disallow*
         the usage of /ALREADY initialized fields/.

     - Scopability :: =TODO= =START=
       an expression may *ONLY* access existing objects via /formal parameters/ and
       ~this~. More precisely, given any environment ~Ï~ (which are the value
       bindings for method parameters and this) and heap ~Ïƒ~ for evaluating an
       expression ~e~, if the resulting value reaches an object ~o~ pre-existent in
       ~Ïƒ~, then ~o~ is reachable from ~Ï~ in ~Ïƒ~. Control effects like coroutines,
       delimited control, resumable exceptions may break the property, as they
       can transport a value upper in the stack (not in scope) to be reachable
       from the current scope. Static fields can also serve as a teleport thus
       breaks this property. In the implementation, we need to enforce that
       teleported values are transitively initialized.

     - With the established principles and design goals, following rules are imposed:
       1. In an assignment o.x = e, the expression e may only point to transitively initialized objects.

          This is how monotonicity is enforced in the system. Note that in an
          initialization val f: T = e, the expression e may point to an object
          under initialization. This requires a distinction between mutation and
          initialization in order to enforce different rules. Scala has
          different syntax for them, it thus is not an issue.

       2. References to objects under initialization may not be passed as arguments
          to method calls or constructors.

          Escape of this in the constructor is commonly regarded as an
          anti-pattern, and it's rarely used in practice. This rule is simple
          for the programmer to reason about initialization and it simplifies
          implementation. The theory supports safe escape of this with the help
          of annotations, we delay the extension until there is a strong need.

       3. Local definitions may only refer to transitively initialized objects.

          It means that in a local definition val x: T = e, the expression e may
          only evaluate to transitively initialized objects. The same goes for
          local lazy variables and methods. This rule is again motivated for
          simplicity in reasoning about initialization: programmers may safely
          assume that all local definitions only point to transitively
          initialized objects.

**** TODO Modularity
**** TODO Theory
**** TODO Back Doors
**** TODO Caveats
**** TODO References

** TODO OTHER CHANGED FEATURES
*** DONE Numeric Literals
    CLOSED: [2020-08-23 Sun 01:25]
    - *OLD*:
      In Scala 2, /numeric literals/ were *confined* to the /primitive numeric
      types/:
      + ~Int~
      + ~Long~
      + ~Float~
      + ~Double~

    - *NEW*:
      Scala 3 allows to write /numeric literals/ also for /user defined types/.
      Example:
      #+begin_src scala
        val x: Long = -10_000_000_000

        val y: BigInt = 0x123_abc_789_def_345_678_901
        val z: BigDecimal = 110_222_799_799.99

        (y: BigInt) match {
          case 123_456_789_012_345_678_901 =>
        }
      #+end_src

    - (In Scala 3)
      The _syntax_ of /numeric literals/ is the same as before,
      + *EXCEPT* there are *NO pre-set limits* _how large they can be_.

**** DONE Meaning of Numeric Literals
     CLOSED: [2020-08-23 Sun 00:10]
     - The meaning of a numeric literal is determined as follows:
       + If the literal ends with ~l~ or ~L~,
         it is a ~Long~ /integer/ (and *must fit* in its legal /range/).

       + If the literal ends with ~f~ or ~F~,
         it is a _single precision_ /floating point number/ of type ~Float~.

       + If the literal ends with ~d~ or ~D~,
         it is a _double precision_ /floating point number/ of type ~Double~.

     - In each of these cases the conversion to a number is exactly as in Scala 2
       or in Java.
         If a /numeric literal/ does NOT end in one of these suffixes, its meaning
       is _determined by the expected type_:
       1. If the expected type is ~Int~, ~Long~, ~Float~, or ~Double~,
          the literal is treated as a standard literal of that type.

       2. If the _expected type_ is a fully defined type ~T~ that has a /given
          instance/ of /type/ ~scala.util.FromDigits[T]~, the literal is
          converted to a value of type ~T~ by passing it as an argument to the
          ~fromDigits~ /method/ of that instance (more details below).

       3. Otherwise, the literal is treated as a ~Double~ /literal/ (if it has a
          decimal point or an exponent), or as an ~Int~ /literal/ (if not).
          (This last possibility is again as in Scala 2 or Java.)

     - With these rules, =FIX-DOC= =(wrong syntax highlight)=
       + The definition ~val x: Long = -10_000_000_000~ is legal by *rule 1*,
         since the _expected type_ is ~Long~.

       + The definitions
         #+begin_src scala
           val y: BigInt = 0x123_abc_789_def_345_678_901
           val z: BigDecimal = 111222333444.55
         #+end_src
         are legal by *rule 2*, since both ~BigInt~ and ~BigDecimal~ have ~FromDigits~
         instances (which implement the ~FromDigits~ /subclasses/ ~FromDigits.WithRadix~
         and ~FromDigits.Decimal~, respectively).


       + On the other hand,
         ~val x = -10_000_000_000~ gives a type error, since without an _expected
         type_ ~-10_000_000_000~ is treated by *rule 3* as an ~Int~ literal, but
         it is _too large_ for ~Int~.

**** DONE The ~FromDigits~ Trait
     CLOSED: [2020-08-23 Sun 00:10]
     To allow /numeric literals/, a /type/ simply has to define a /given instance/
     of the ~scala.util.FromDigits~ /type class/, or one of its /subclasses/.

     - ~FromDigits~:
       #+begin_src scala
         trait FromDigits[T] {
           def fromDigits(digits: String): T
         }
       #+end_src

     - Implementations of the ~fromDigits~
       _CONVERT_ strings of digits _TO_ the values of the implementation type ~T~.
       + The _digits string_ consists of
         * digits between 0 and 9,
         * possibly preceded by a sign ("+" or "-").

       + Number separator characters _ are *filtered out* _BEFORE_ the string is
         passed to ~fromDigits~.

       + =from Jian=
         Process _decimal point_ and _exponent_ is *NOT* a duty of ~FromDigits~.
         This is why they are not mentioned above. /Sub-traits/ of ~FromDigits~
         will handle _decimal point_ and _exponent_. See below!

     - The /companion object/ ~FromDigits~ also defines /subclasses/ of ~FromDigits~
       + for /whole numbers/ with a given _radix_,
       + for _numbers with a decimal point_,
       + for _numbers that can have both a decimal point and an exponent_:
       #+begin_src scala
         object FromDigits {

           /** A subclass of `FromDigits` that also allows to convert whole number literals
             *  with a radix other than 10
             */
           trait WithRadix[T] extends FromDigits[T] {
             def fromDigits(digits: String): T = fromDigits(digits, 10)
             def fromDigits(digits: String, radix: Int): T
           }

           /** A subclass of `FromDigits` that also allows to convert number
             *  literals containing a decimal point ".".
             */
           trait Decimal[T] extends FromDigits[T]

           /** A subclass of `FromDigits` that allows also to convert number
             *  literals containing a decimal point "." or an
             *  exponent `('e' | 'E')['+' | '-']digit digit*`.
             */
           trait Floating[T] extends Decimal[T]

           // ...
         }
       #+end_src
       A /user-defined number type/ can implement one of those, which signals to
       the compiler that hexadecimal numbers, decimal points, or exponents are
       also accepted in literals for this type.

**** DONE Error Handling
     CLOSED: [2020-08-23 Sun 00:34]
     ~FromDigits~ implementations can signal errors by throwing /exceptions/ of some
     /subtype/ of ~FromDigitsException~.
       ~FromDigitsException~ is defined with _THREE_ /subclasses/ in the ~FromDigits~
     /object/ as follows:
     #+begin_src scala
       abstract class FromDigitsException(msg: String) extends NumberFormatException(msg)

       class NumberTooLarge(msg: String = "number too large")          extends FromDigitsException(msg)
       class NumberTooSmall(msg: String = "number too small")          extends FromDigitsException(msg)
       class MalformedNumber(msg: String = "malformed number literal") extends FromDigitsException(msg)
     #+end_src

**** DONE Example
     CLOSED: [2020-08-23 Sun 00:52]
     As a fully worked out example, here is an implementation of a new numeric
     class, ~BigFloat~, that accepts /numeric literals/. ~BigFloat~ is defined in
     terms of a ~BigInt~ /mantissa/ and an ~Int~ /exponent/:
     #+begin_src scala
       case class BigFloat(mantissa: BigInt, exponent: Int) {
         override def toString = s"${mantissa}e${exponent}"
       }
     #+end_src
     - ~BigFloat~ /literals/ can have a /decimal point/ as well as an /exponent/.
       E.g. the following expression should produce the ~BigFloat~ number
       ~BigFloat(-123, 997)~: ~-0.123E+1000: BigFloat~

     - The /companion object/ of ~BigFloat~ defines an apply /constructor method/
       to construct a ~BigFloat~ from a _digits string_.
       Here is a possible implementation:
       #+begin_src scala
         object BigFloat {
           import scala.util.FromDigits

           def apply(digits: String): BigFloat = {
             val (mantissaDigits, givenExponent) = digits.toUpperCase.split('E') match {
               case Array(mantissaDigits, edigits) =>
                 val expo =
                   try FromDigits.intFromDigits(edigits)
                   catch {
                     case ex: FromDigits.NumberTooLarge =>
                       throw FromDigits.NumberTooLarge(s"exponent too large: $edigits")
                   }
                 (mantissaDigits, expo)

               case Array(mantissaDigits) =>
                 (mantissaDigits, 0)
             }

             val (intPart, exponent) = mantissaDigits.split('.') match {
               case Array(intPart, decimalPart) =>
                 (intPart ++ decimalPart, givenExponent - decimalPart.length)

               case Array(intPart) =>
                 (intPart, givenExponent)
             }

             BigFloat(BigInt(intPart), exponent)
           }

           given FromDigits as FromDigits.Floating[BigFloat] {
             def fromDigits(digits: String) = apply(digits)
           }
         }  // end BigFloat
       #+end_src
       + To accept ~BigFloat~ /literals/, all that's needed in addition is a
         /given instance/ of type ~FromDigits.Floating[BigFloat]~

       + Note that the ~apply~ method does *NOT check* the format of the digits
         argument. It is assumed that only VALID arguments are passed:
         For calls coming from the compiler that assumption is valid, since the
         compiler will FIRST check whether a /numeric literal/ has the correct
         format BEFORE it gets passed on to a conversion method.

**** DONE Compile-Time Errors
     CLOSED: [2020-08-23 Sun 01:25]
     - With the setup of the previous section, a literal like
       #+begin_src scala
         1e10_0000_000_000: BigFloat
       #+end_src
       would be expanded by the compiler to
       #+begin_src scala
         BigFloat.FromDigits.fromDigits("1e100000000000")
       #+end_src
       Evaluating this expression throws a ~NumberTooLarge~ /exception/ at /runtime/.

     - Required enhancement ::
       We would like it to produce a compile-time error instead.

     - Solution ::
       We can achieve this by tweaking the ~BigFloat~ /class/ with a small dose of
       metaprogramming. The idea is to turn the ~fromDigits~ /method/ into a /macro/,
       i.e. make it an /inline method/ with a /splice/ as right hand side.
         To do this, replace the ~FromDigits~ /instance/ in the ~BigFloat~ /object/
       by the following two definitions:
       #+begin_src scala
         object BigFloat {
           // ...

           class FromDigits extends FromDigits.Floating[BigFloat] {
             def fromDigits(digits: String) = apply(digits)
           }

           given FromDigits {
             override inline def fromDigits(digits: String) = ${
               fromDigitsImpl('digits)
             }
           }

           private def fromDigitsImpl(digits: Expr[String])
                                     (using ctx: QuoteContext): Expr[BigFloat] =
             digits match {
               case Const(ds) =>
                 try {
                   val BigFloat(m, e) = apply(ds)
                   '{BigFloat(${Expr(m)}, ${Expr(e)})}
                 }
                 catch {
                   case ex: FromDigits.FromDigitsException =>
                     ctx.error(ex.getMessage)
                     '{BigFloat(0, 0)}
                 }

               case digits =>
                 '{apply($digits)}
             }
         }  // end BigFloat
       #+end_src
       + The /macro/ implementation takes an argument of /type/ ~Expr[String]~ and
         yields a result of /type/ ~Expr[BigFloat]~. It tests whether its argument
         is a _constant string_. If that is the case, it converts the string using
         the ~apply~ /method/ and lifts the resulting ~BigFloat~ back to ~Expr~
         level. For _non-constant strings_ ~fromDigitsImpl(digits)~ is simply
         ~apply(digits)~, i.e. everything is evaluated at /runtime/ in this case.
         * =from Jian=
           WHY DO WE NEED THE ~'{BigFloat(0, 0)}~ in the ~catch~ part???
           I know it want to match the /type/, however, in non-macro definition, we
           can ignore the dummy value creation in the ~catch~ part. Does this mean
           /macro/ has this special requirement? Is there a way to drop it???

       + The interesting part is the catch part of the case where digits is constant.
         If the ~apply~ /method/ throws a ~FromDigitsException~, the exception's message
         is issued as a /compile time error/ in the ~ctx.error(ex.getMessage)~ call.

       + With this new implementation, a definitiion like
         ~val x: BigFloat = 1234.45e3333333333~
         would give a compile time error message:
         #+begin_src text
           3 |  val x: BigFloat = 1234.45e3333333333
             |                    ^^^^^^^^^^^^^^^^^^
             |                    exponent too large: 3333333333
         #+end_src

*** TODO Structural Types
    # *Programmatic Structural Types*
    - Some usecases are more awkward in statically typed languages than in
      dynamically typed languages

    - Example: modelling database access
      1. With dynamically typed languages, it's quite natural to _model a /row/ as
         a /record/ or /object/_, and to select entries with simple dot notation
         (e.g. ~row.columnName~).

      2. Achieving the same experience in /statically typed language/ requires
         + defining a class for every possible row arising from database manipulation
           (including rows arising from joins and projections)
         + setting up a scheme to map between a row and the class representing it.

      3. This requires a large amount of boilerplate, which leads developers
         to trade the advantages of static typing for simpler schemes where
         colum names are represented as strings and passed to other operators
         (e.g. ~row.select("columnName")~). _This approach is unatural in both
         sides_
         + forgoes the advantages of static typing,
         + is still not as natural as the dynamically typed version.

    - Structural types help in situations where we would like to support simple
      dot notation in dynamic contexts without losing the advantages of static
      typing.
        They allow developers to use dot notation and configure how fields and
      methods should be resolved.

**** Example
     #+begin_src scala
       object StructuralTypeExample {
         case class Record(elems: (String, Any)*) extends Selectable {
           def selectDynamic(name: String): Any = elems.find(_._1 == name).get._2
         }

         type Person = Record {
           val name: String
           val age: Int
         }

         def main(args: Array[String]): Unit = {
           val person = Record("name" -> "Emma", "age" -> 42).asInstanceOf[Person]
           println(s"${person.name} is ${person.age} years old.")
           // Prints: Emma is 42 years old.
         }
       }
     #+end_src

**** Extensibility
     New instances of ~Selectable~ can be defined to *support means of access*
     _othr than_ /Java reflection/, which would enable usages such as the
     database access example given at the beginning of this document.

**** TODO Local Selectable Instances
**** TODO Relation with ~scala.Dynamic~
     TODO =from Jian= I need to learn more about ~scala.Dynamic~

*** DONE Operators
    CLOSED: [2020-01-18 Sat 16:07]
    *Rules for Operators*
**** DONE The ~@alpha~ Annotation
     CLOSED: [2020-01-18 Sat 14:52]
     - ~@alpha~ annotation :: it is applied on a /method definition/ defines an
       _alternate name_ for the implementation of that method.

     - Example: =TODO= =FIX-DOC=
       #+begin_src scala
         object VecOps {
           @alpha("append") extension [T](xs: Vec[T])
             def ++= (ys: Vec[T]): Vec[T] = // ...
         }
       #+end_src
       + The ~++=~ operation is implemented (in bytecode or native code) under
         the name ~append~.

     - The /implementation name/
       + affects the code that is generated
       + is the name under which code _from OTHER languages_ can call the method.
         *ONLY from OTHER languages! You can't use the name in Scala.*
         For instance, ~++=~ could be invoked from Java like this:
         #+begin_src java
           VecOps.append(vec1, vec2)
         #+end_src

     - An ~@alpha~ /annotation/ will be *MANDATORY*
       _if the /method name/ is symbolic_!!!
       + Symbolic name methods without ~@alpha~ annotation are *DEPRECATED*.

***** Motivation
      The ~@alpha~ annotation serves a dual purpose:
      + It helps *interoperability* between Scala and other languages.
      + It serves _as a documentation tool_ by providing an alternative regular
        name as an alias of a symbolic operator.

***** Details
      1. Syntax:
         ~@scala.annotation.alpha(externalName)~.
         ~externalName~ is a string.

      2. An ~@alpha~ annotation can be given for all kinds of definitions.
         _NOT ONLY for symbolic method._

      3. ~externalName~ must be a legal name on the host platform.

      4. Lack of an ~@alpha~ annotation will raise a _deprecation warning_.

      5. Definitions with names in backticks that are not legal host platform
         names should have an ~@alpha~ annotation. Lack of such an annotation
         will raise a deprecation warning.
         =from Jian= TODO EXAMPLE???

      6. ~@alpha~ annotations must agree:
         *There must be a one-to-one relationship between external and internal
         names*

**** DONE The ~@infix~ Annotation
     CLOSED: [2020-01-18 Sat 15:23]
     - ~@infix~ annotation :: it is applied on a /method definition/ allows using
       the method as an _infix operation_.

     - Example:
       #+begin_src scala
         trait MultiSet[T] {
           @infix
           def union(other: MultiSet[T]): MultiSet[T]

           def difference(other: MultiSet[T]): MultiSet[T]

           @alpha("intersection")
           def *(other: MultiSet[T]): MultiSet[T]
         }

         val s1, s2: MultiSet[Int]

         s1 union s2   // OK
         s1.union(s2)  // also OK

         s1.difference(s2)   // OK
         s1 `difference` s2  // OK
         s1 difference s2    // gives a deprecation warning

         s1 * s2   // OK
         s1.*(s2)  // also OK, but unusual
       #+end_src
       + *Infix operations involving alphanumeric operators are deprecated*,
         unless one of the following conditions holds:
         * the operator definition carries an ~@infix~ annotation, or
         * the operator was compiled with Scala 2, or
         * the operator is followed by an opening brace. TODO ??? TODO

     - alphanumeric operator :: an operator *consisting ENTIRELY* of
       + letters
       + digits
       + ~$~
       + ~_~
       + any unicode character for which ~java.lang.Character.isIdentifierPart(c)~
         returns ~true~.

     - /Infix operations/ involving /symbolic operators/ are *ALWAYS* allowed, so
       ~@infix~ is redundant for methods with _symbolic names_.

     - The ~@infix~ annotation can also _be given to a /type/:_
       #+begin_src scala
         @infix type or[X, Y]
         val x: String or Int
       #+end_src

***** Motivation
      The purpose of the @infix annotation is to achieve consistency across a
      code base in how a method or type is applied. The idea is that the author
      of a method decides whether that method should be applied as an infix
      operator or in a regular application. Use sites then implement that
      decision consistently.

***** Details
      1. ~@scala.annotation.infix~

      2. ~@infix~ annotations must agree when overriding.

      3. The first non-receiver parameter list of an ~@infix~ method must define
         exactly one parameter. For example,
         #+begin_src scala
           @infix def op(x: S): R                  // OK
           @infix def op[T](x: T)(y: S): R         // OK
           @infix def op[T](x: T, y: S): R         // error: two parameters

           @infix extension (x: A) def op (y: B): R          // OK
           @infix extension (x: A) def op (y1: B, y2: B): R  // error: two parameters
         #+end_src

      4. ~@infix~ annotations can also be given to /type/, /trait/ or /class/
         definitions that have exactly _two type parameters_. An /infix type/
         like
         #+begin_src scala
           @infix type op[X, Y]
         #+end_src
         can be applied using infix syntax, i.e. ~A op B~

      5. To smooth migration to Scala 3.0, alphanumeric operations will only be
         deprecated from Scala 3.1 onwards, or if the ~-strict~ option is given
         in Dotty/Scala 3.

**** DONE Syntax Change
     CLOSED: [2020-01-18 Sat 16:07]
     Infix operators can now appear at the start of lines in a multi-line expression.
     Thanks to the change of semicolon inference.

     - Illustrate by examples:
       + The leading infix operator should be followed by at least one space
         character (=from Jian= and then another operand).
         #+begin_src scala
           freezing
           | boiling
         #+end_src

       + No space, no infix operation
         #+begin_src scala
           freezing
           !boiling
         #+end_src

       + No following legal operand
         #+begin_src scala
           println("hello")
           ???
           ??? match { case 0 => 1 }
         #+end_src
         * The second line ~???~ doesn't have a following operand.
         * The thrid line ~???~ doesn't have a legal following operand -- ~match~
           is not a token that can start an expression.

*** DONE Wildcard Types - =IMPORTANT= =Migration is COMPLICATED=
    CLOSED: [2020-11-07 Sat 00:44]
    # Wildcard Arguments in Types
    - The syntax of /wildcard arguments in types/ has *changed* FROM ~_~ TO ~?~.

    - Examples:
      + ~List[?]~
      + ~Map[? <: AnyRef, ? >: Null]~

**** Motivation
     =from Jian=
     Give ~_~ syntax another semantics, and make semantics more consistent and
     flexible. One of the reason that Scala want to do this is that there exists
     a better synbol can convey the current ~_~ semantics -- ~?~. After doing this
     ~?~ in both Java and Scala has the same semantics.

     - We would like to use the underscore syntax ~_~ to stand for an /anonymous
       type parameter/, *aligning* it with its meaning in /value parameter lists/.
       So, just as ~f(\under{})~ is a shorthand for the lambda ~x => f(x)~ (we current
       have this in Scala 2), _in the future_ ~C[_]~ will be _a shorthand for the
       /type lambda/ ~[X] =>> C[X]~._
       + This makes /higher-kinded types/ easier to use.

     - The new ~_~ semantics also *removes the wart* that,
       + used AS a /type parameter/, ~F[_]~ means ~F~ is a /type constructor/

       + WHEREAS used AS a /type/, ~F[_]~ means it is a /wildcard (i.e. existential)
         type/.

     - In the future, ~F[_]~ will always mean the one thing,
       no matter where it is used.

     - We pick ~?~ as a _REPLACEMENT syntax_ for /wildcard types/, since it
       _ALIGNS WITH Java's syntax._

**** Migration Strategy
     - The migration to the new scheme is *complicated*,
       in particular since the /kind projector compiler plugin/ still uses the
       _reverse convention_, with ~?~ meaning /parameter placeholder/ _INSTEAD
       OF_ /wildcard/.
       + Fortunately, kind projector has added ~*~ as an *ALTERNATIVE syntax* for
         ~?~.

     - A step-by-step migration is made possible with the following measures:
       * _In Scala 3.0_,
         both ~_~ and ~?~ are legal names for /wildcards/.

       * _In Scala 3.1_,
         ~_~ is deprecated in favor of ~?~ as a name for a /wildcard/.
         A ~-rewrite~ option is available to rewrite one to the other.

       * _In Scala 3.2_,
         the meaning of ~_~ changes _FROM_ /wildcard/ _TO_ /placeholder for type
         parameter/.

       * The _Scala 3.1_ behavior is already available today under the ~-strict~
         setting.

     - To smooth the transition for codebases that use *kind-projector*, we adopt
       the following measures under the command line option ~-Ykind-projector~:
       + In Scala 3.0,
         ~*~ is _available_ as a /type parameter placeholder/.

       + In Scala 3.2,
         ~*~ is *DEPRECATED* in favor of ~_~.
         A ~-rewrite~ option is available to rewrite one to the other.

       + In Scala 3.3,
         ~*~ is *REMOVED* again, and all /type parameter placeholders/ will be
         expressed with ~_~.

     - These rules make it possible to cross build between Scala 2 using the
       /kind projector/ plugin and Scala 3.0 - 3.2 using option ~-Ykind-projector~.

*** TODO Type Checking - =No Doc till now=
    *Type Checking*
    - [//]:# todo: fill in

*** TODO Type Inference - =TODO: WATCH=
    *Changes in Type Inference*
    See
    - https://www.youtube.com/watch?v=lMvOykNQ4zs
    - https://www.youtube.com/watch?v=VV9lPg3fNl8

*** TODO Implicit Resolution
    # Changes in Implicit Resolution

    This page describes *changes* to the /implicit resolution/ that apply both to
    the new ~given~'s and to the old-style ~implicit~'s in Dotty.
      /Implicit resolution/ uses a new algorithm which *caches* implicit results
    *more aggressively* for performance. There are also some changes that affect
    implicits on the language level.

    1. *EXPLICIT types* of /implicits/:
       #+begin_src scala
         class C {
           val ctx: Context = ...  // ok

           /*!*/ implicit val x = ...  // error: type must be given explicitly

           /*!*/ implicit def y = ...  // error: type must be given explicitly

           val y = {
             implicit val ctx = this.ctx  // ok
             // ...
           }
         }
       #+end_src
       + Scala 3:
         /Types/ of /implicit values/ and /implicit methods (_result types_)/
         *MUST be explicitly declared* -- no inferred types.
         * *EXCEPTION*:
           ONLY values in /local blocks/ where the /type/ may still be inferred.

       + Scala 2:
         *NO* /EXPLICIT types/ required!

    2. _Nesting_ is NOW _taken into account_ for selecting an /implicit/:
       #+begin_src scala
         def f(implicit i: C) = {
           def g(implicit j: C) = {
             implicitly[C]
           }
         }
       #+end_src
       + Scala 3:
         _Select ~j~, because ~j~ is in the immediate scope that is accessible by
         ~implicitly[C]~._
         * _Nesting_ is NOW _taken into account_ for selecting an /implicit/.
           This is different from Scala 2.

       + Scala 2:
         _This would have resulted in an *ambiguity error*._
         The previous possibility of an implicit search failure due to shadowing
         (where an implicit is hidden by a nested definition) no longer applies.

    3. /Package prefixes/ no longer contribute to the /implicit search scope/ of
       a /type/. Example:
       #+begin_src scala
         package p
         given a as A

         object o {
           given b as B
           type C
         }
       #+end_src
       + Both ~a~ and ~b~ are visible as /impicits/ at the point of the definition
         of /type/ ~C~.
           However, a reference to ~p.o.C~ _OUTSIDE of package_ ~p~ will have *ONLY*
         ~b~ in its /implicit search scope/ but NOT ~a~.

       + In more detail, here are the rules for what contributes the /implicit
         scope/ of a /type/:
         * Definition:
           A reference is an anchor if it refers to an object, a class, a trait,
           an abstract type, an opaque type alias, or a match type alias.
             References to packages and package objects are anchors only under
           =-source:3.0-migration=.

       + Definition:
         The anchors of a type ~T~ is a set of references defined as follows:
         1. If ~T~ is a reference to an anchor, ~T~ itself plus, if ~T~ is of the
            form ~P#A~, the anchors of ~P~.

         2. If ~T~ is an alias of ~U~, the anchors of ~U~.

         3. If ~T~ is a reference to a type parameter, the union of the anchors of
            both of its bounds.

         4. If ~T~ is a singleton reference, the anchors of its underlying type,
            plus, if ~T~ is of the form (~P#x~).type, the anchors of ~P~.

         5. If ~T~ is the this-type ~o.this~ of a /static object/ ~o~, the anchors
            of a term reference ~o.type~ to that object.

         6. If ~T~ is some other type, the union of the anchors of each constituent
            type of ~T~.

       + Definition:
         The implicit scope of a type T is the smallest set S of term references
         such that
         1. If T is a reference to a class, S includes a reference to the /companion
            object/ of the class, if it exists, as well as the implicit scopes of all
            of T's parent classes.

         2. If T is a reference to an object, S includes T itself as well as the
            implicit scopes of all of T's parent classes.

         3. If T is a reference to an opaque type alias named A, S includes a
            reference to an object A defined in the same scope as the type, if
            it exists, as well as the implicit scope of T's underlying type or
            bounds.

         4. If T is a reference to an an abstract type or match type alias named
            A, S includes a reference to an object A defined in the same scope
            as the type, if it exists, as well as the implicit scopes of T's
            given bounds.

         5. If T is a reference to an anchor of the form p.A then S also includes
            all term references on the path p.

         6. If T is some other type, S includes the implicit scopes of all anchors
            of T.

    4. The treatment of ambiguity errors has changed.
       If an ambiguity is encountered in some recursive step of an implicit
       search, the ambiguity is propagated to the caller.
       + Example:
         #+begin_src scala
           class A
           class B extends C
           class C
           implicit def a1: A
           implicit def a2: A
           implicit def b(implicit a: A): B
           implicit def c: C
         #+end_src
         Try query with ~implicitly[C]~.

       + The treatment of /divergence errors/ has also changed.
         A _divergent_ implicit is treated as a *normal* failure, after which
         alternatives are still tried. This also makes sense:
           Encountering a /divergent implicit/ means that we assume that no finite
         solution can be found on the corresponding path, but another path can still
         be tried. By contrast, *most (but not all)* /divergence errors/ in Scala 2
         would terminate the /implicit search/ as a whole.

    5. In Scala-2
       /implicit conversions/ with /call-by-name parameters/
       *has a lower level of priority* relative to
       /implicit conversions/ with /call-by-value parameters/.
         Dotty *drops* this distinction. So the following code snippet would be
       _ambiguous in Dotty_:
       #+begin_src scala
         implicit def conv1(x: Int): A = new A(x)
         implicit def conv2(x: => Int): A = new A(x)
         def buzz(y: A) = ???
         buzz(1)  // error: ambiguous
       #+end_src

    6. The rule for picking a *most specific* alternative among a set of /overloaded/
       or /implicit alternatives/ is refined to take /context parameters/ into account.
         All else being equal, an alternative that takes some /context parameters/
       is taken to be less specific than an alternative that takes none.
         If both alternatives take /context parameters/, we try to choose between
       them as if they were methods with _regular parameters_. The following
       paragraph in the SLS is affected by this change:
       + Original version:
         #+begin_quote
         An alternative ~A~ is more specific than an alternative ~B~ if the relative
         weight of ~A~ over ~B~ is greater than the relative weight of ~B~ over ~A~.
         #+end_quote

       + Modified version:
         An alternative ~A~ is more specific than an alternative ~B~ if
         * the relative weight of ~A~ over ~B~ is greater than the relative weight
           of ~B~ over ~A~, or

         * the relative weights are the same, and ~A~ takes NO /implicit parameters/
           but ~B~ does, or

         * the relative weights are the same, both ~A~ and ~B~ take /implicit
           parameters/, and ~A~ is more specific than ~B~ if all /implicit parameters/
           in either alternative are replaced by _regular parameters_.

    7. The previous disambiguation of _implicits based on inheritance depth_ is
       *refined* to make it /transitive/.
         /Transitivity/ is important to _GUARANTEE_ that *search outcomes are
       compilation-order independent*.
       + Here's a scenario where the *previous rules* VIOLATED /transitivity/:
         #+begin_src scala
           class A extends B
           object A { given a ... }

           class B
           object B extends C { given b ... }

           class C { given c }
         #+end_src
         1. ~a~ is *more specific* than ~b~
            SINCE the /companion class/ ~A~ is a /subclass/ of the /companion
            class/ ~B~.

         2. ~b~ is *more specific* than ~c~
            SINCE /object/ ~B~ /extends/ /class/ ~C~.

         3. But ~a~ is *NOT more specific* than ~c~.
            This means if ~a~, ~b~, ~c~ are all applicable /implicits/, *it makes
            a difference in what order they are compared.*
              If we compare ~b~ and ~c~ first, we keep ~b~ and drop ~c~. Then,
            comparing ~a~ with ~b~ we keep ~a~. But if we compare ~a~ with ~c~
            first, we fail with an ambiguity error.

       + The new rules are as follows:
         An _implicit_ ~a~ defined in ~A~
         is *more specific* than
         an _implicit_ ~b~ defined in ~B~ if
         * ~A~ /extends/ ~B~, or

         * ~A~ is an /object/ and the /companion class/ of ~A~ /extends/ ~B~, or

         * ~A~ and ~B~ are /objects/, =TODO= =???= =TODO=
           - ~B~ does *not inherit* any /implicit members/ from base /classes/ (*),
             AND
           - the /companion class/ of ~A~ /extends/ the /companion class/ of ~B~.
             + Condition (*) is new.
               It is necessary to ensure that the defined relation is transitive.

    8. [//]: #todo: expand with precise rules

*** DONE Implicit Conversions
    CLOSED: [2020-11-07 Sat 02:25]
    - An /implicit conversion/, also called /view/, is a conversion that is applied
      by the compiler in several situations:
      1. When an expression ~e~ of type ~T~ is encountered, but the compiler needs
         an expression of type ~S~.

      2. When an expression ~e.m~ where ~e~ has type ~T~ but ~T~ defines no member
         ~m~ is encountered.
      
    - In those cases, the compiler LOOKS in the /implicit scope/ FOR a conversion
      (declared as ~given~ in Scala 3 or ~implicit~ in Scala 2 or Scala 3.0)
      that can convert an expression of type ~T~
      * to an expression of /type/ ~S~ to the first case above
        OR
      * to a /type/ that defines a /member/ ~m~ in the second case above.
      
    - This conversion can be either:
      1. An ~implicit def~ of type ~T => S~ or ~(=> T) => S~
      2. An /implicit value/ of type ~scala.Conversion[T, S]~

    - Defining an implicit conversion will emit a warning unless
      * the ~import scala.language.implicitConversions~ is _in scope_, =FIX=
        OR 
      * the _flag_ ~-language:implicitConversions~ is given to the compiler.

    - =from Jian=
      Like other features that are considered should be restricted, and can be
      switched on by _flag_ or _import_, *use _import_ is always the preferred
      way* because of its *flexibility* -- limit the usage to a single file, or
      even a smaller scope.
      
**** Examples
     - The first example is taken from ~scala.Predef~. Thanks to this /implicit
       conversion/, it is possible to pass a ~scala.Int~ to a /Java method/ that
       expects a ~java.lang.Integer~
       #+begin_src scala
         import scala.language.implicitConversions

         implicit def int2Integer(x: Int): java.lang.Integer =
           x.asInstanceOf[java.lang.Integer]
       #+end_src
       * =from Jian=
         The above code use the legacy syntax because it tries to only emphasise
         the ~import scala.language.implicitConversions~ and the ~Conversion~.
         If re-write it in the new syntax, we have:
         #+begin_src scala
           import scala.language.implicitConversions

           given int2Integer as Conversion[Int, java.lang.Integer] =
             java.lang.Integer.valueOf(_)
         #+end_src

     - The second example shows how to use ~Conversion~ to define an ~Ordering~
       for an _ARBITRARY_ /type/, given existing ~Ordering~'s for other /types/:
       #+begin_src scala
         import scala.language.implicitConversions

         implicit def ordT[T, S](
           implicit conv: Conversion[T, S],
                    ordS: Ordering[S]
         ): Ordering[T] = {
           // `ordS` compares values of type `S`, but we can convert from `T` to `S`
           (x: T, y: T) => ordS.compare(x, y)
         }

         class A(val x: Int)  // The type for which wewant an `Ordering`

         // Convert `A` to a type for which an `Ordering` is available:
         implicit val AToInt: Conversion[A, Int] = _.x

         implicitly[Ordering[Int]]  // Ok, exists in the standard library
         implicitly[Ordering[A]]    // Ok, will use the implicit conversion from
                                    // `A` to `Int` and the `Ordering` for `Int`.
       #+end_src
       * =from Jian=
         The above code use the legacy syntax because it tries to only emphasise
         the ~import scala.language.implicitConversions~ and the ~Conversion~.
         If re-write it in the new syntax, we have:
         #+begin_src scala
           import scala.language.implicitConversions

           given ordT[T, S](
             using conv: Conversion[T, S],
                   ordS: Ordering[S]
           ): Ordering[T] = {
             // `ordS` compares values of type `S`, but we can convert from `T` to `S`
             (x: T, y: T) => ordS.compare(x, y)
           }

           class A(val x: Int)  // The type for which wewant an `Ordering`

           // Convert `A` to a type for which an `Ordering` is available:
           given AToInt as Conversion[A, Int] = _.x

           summon[Ordering[Int]]  // Ok, exists in the standard library
           summon[Ordering[A]]    // Ok, will use the implicit conversion from
                                  // `A` to `Int` and the `Ordering` for `Int`.
         #+end_src
         
*** TODO Overload Resolution - TODO =READING=
    # Changes in Overload Resolution
    /Overload resolution/ in Dotty *improves* on Scala 2 in _TWO_ ways.
    - it takes *ALL* /argument lists/ into account
      _instead of_
      just the *first* /argument list/.

   - it can *infer* /parameter types/ of /function values/ *even if they are in
      the _FIRST_ /argument list/.*

**** Looking Beyond the First Argument List
     - Example:
       Code legal in Dotty, while it results in an ambiguous overload error in Scala 2:
       + Example 1:
         #+begin_src scala
           def f(x: Int)(y: String): Int = 0
           def f(x: Int)(y: Int): Int = 0

           f(3)("")
         #+end_src

       + Example 2:
         #+begin_src scala
           def g(x: Int)(y: Int)(z: Int): Int = 0
           def g(x: Int)(y: Int)(z: String): Int = 0

           g(2)(3)(4)     // ok
           g(2)(3)("")    // ok
         #+end_src

     -

**** Parameter Types of Function Values

*** DONE Match Expressions
    CLOSED: [2020-05-24 Sun 22:53]
    The /syntactical precedence/ of /match/ expressions has been *changed*.

    - ~match~ is still a keyword,
      but it is used like an /alphabetical operator/.

    - This has several consequences:
      1. ~match~ expressions can be chained:
         #+begin_src scala
           xs match {
             case Nil    => "empty"
             case h :: t => "nonempty"
           } match {
             case "empty"    => 0
             case "nonempty" => 1
           }
         #+end_src

      2. ~match~ may follow a /period/: =TODO= check the concept of /period/
         #+begin_src scala
           if xs.match {
             case Nil => false
             case _   => true
           }
           then "nonempty"
           else "empty"
         #+end_src

      3. The scrutinee of a ~match~ expression must be an ~InfixExpr~.
         + Previously the scrutinee could be _followed by_ a type ascription ~: T~,
           but this is *no longer supported*.

         + So ~x : T match { ... }~ in Scala 3 now has to be written
           ~(x: T) match { ... }~.

**** Syntax

*** DONE Vararg Patterns
    CLOSED: [2020-05-24 Sun 23:01]
    The syntax of /vararg patterns/ has changed.
    - In the _NEW syntax_ one writes /varargs/ in patterns _EXACTLY LIKE_ one writes
      them in expressions, using ~a : _*~ type annotation:
      + NEW:
        #+begin_src scala
          xs match {
            case List(1, 2, xs: _*) => println(xs)  // binds xs
            case List(1, _: _*)     =>              // wildcard pattern
          }
        #+end_src

      + OLD:
        it is shorter but _less regular_, and it no longer supported.
        #+begin_src scala
          /*!*/  case List(1, 2, xs @ _*) =>  // syntax error
          /*!*/  case List(1, _ @ _*)     =>  // syntax error
        #+end_src

**** Compatibility considerations
     - To enable smooth cross compilation between Scala 2 and Scala 3, Dotty will
       accept both the old and the new syntax.

     - Under the ~-strict~ setting,
       an error will be emitted when the old syntax is encountered.

     - The ~-strict~ setting will be _enabled by default in version 3.1 of the
       language._

*** DONE Pattern Bindings
    CLOSED: [2020-05-25 Mon 11:03]
    - In Scala 2, /pattern bindings/ in ~val~ definitions and ~for~ expressions are
      *LOOSELY typed*.
      + Potentially failing matches are still _ACCEPTED at /compile-time/,_
        but may influence the program's /runtime/ behavior.

    - From Scala 3.1 on, /type checking/ rules will be tightened so that _errors
      are *REPORTED* at /compile-time/ instead._

**** Bindings in Pattern Definitions
     - Exmaple 1:
       _Code that can pass Scala 2 /type checking/ but fail at /runtime/_
       #+begin_src scala
         val xs: List[Any] = List(1, 2, 3)
         val (x: String) :: _ = xs
       #+end_src
       + This code gives a compile-time error in Scala 3.0 with the ~-strict~
         setting or Scala 3.1 by default.

       + It will fail at runtime with a ~ClassCastException~ in Scala 2.

     - In Scala 3.1, a /pattern binding/ is *ONLY allowed* if the pattern is
       /irrefutable/, that is, if the right-hand side's type CONFORMS TO the
       pattern's type.
       + For instance, the following is OK:
         #+begin_src scala
           val pair = (1, true)
           val (x, y) = pair
         #+end_src

     - Exmaple 2:
       Sometimes one wants to decompose data anyway, even though the pattern is
       /refutable/.

       For instance, if at some point one knows that a list elems is non-empty
       one might want to decompose it like this:
       #+begin_src scala
         val first :: rest = elems  // error in Scala 3.1; work in Scala 2
       #+end_src
       + Use ~@unchecked~ to avoid the error:
         ~val first :: rest : @unchecked = elems~

**** Pattern Bindings in For Expressions
     - In Scala 2, /pattern bindings/ in ~for~ expressions do _implicit filtering_,
       and filter out data that are not matched.
       + In Scala 3, apply this _implicit filtering_ *ONLY when* a ~case~ shows up
         before the pattern -- this is a new syntax! Scala 2 syntax no longer has
         the implicit filtering semantics.

     - Example (in Scala 3.1):
       + Usage in Scala 2 can fail becuase of the semantics change.
         #+begin_src scala
           val elems: List[Any] = List((1, 2), "hello", (3, 4))
           for ((x, y) <- elems) yield (y, x) // error: pattern's type (Any, Any) is more specialized
                                              // than the right hand side expression's type Any
         #+end_src

       + Make implicit filtering available by using the ~case~:
         #+begin_src scala
           for (case (x, y) <- elems) yield (y, x)  // returns List((2, 1), (4, 3))
         #+end_src

**** Syntax Changes
     There are *TWO syntax CHANGES* relative to Scala 2:
     - /pattern definitions/ can carry ascriptions such as ~: @unchecked~.
     - /generators/ in ~for~ expressions may be prefixed with ~case~.

**** Migration
     - The new syntax is supported in Dotty and Scala 3.0.

     - However, to enable smooth cross compilation between Scala 2 and Scala 3,
       + In Scala 3.0, the changed behavior and additional type checks are *only
         enabled under the ~-strict~ setting*.

       + In Scala 3.1+, they will be enabled by default.

*** DONE Pattern Matching - =RE-READ= - =Further simplification will come, no need to read now=
    CLOSED: [2020-09-25 Fri 00:37]
    # Option-less pattern matching
    - Dotty implementation of /pattern matching/ was GREATLY *simplified* compared
      to scalac.
        From a user perspective, this means that Dotty generated patterns are a
      lot easier to debug as
      + _variables_ all show up in debug modes
      + _positions_ are correctly preserved

    - Dotty supports a *superset* of scalac's /extractors/.

    - CAUTION:
      *There are plans for further simplification*,
      in particular to factor out /product match/ and /name-based match/ into a
      SINGLE type of /extractor/.

**** DONE Extractors
     CLOSED: [2020-09-25 Fri 00:37]
     /Extractors/ are objects that expose a /method/ ~unapply~ or ~unapplySeq~:
     - fixed-arity extractors :: /extractor/ for /patterns of fixed arity/:
       #+begin_src scala
         def unapply[A](x: T)(using x: B): U
       #+end_src

     - variadic extractors :: /extractor/ for /variadic patterns/:
       #+begin_src scala
         def unapplySeq[A](x: T)(using x: B): U
       #+end_src

***** Fixed-Arity Extractors
      /Fixed-arity extractors/ expose the following signature:
      #+begin_src scala
        def unapply[A](x: T)(using x: B): U
      #+end_src
      - The type ~U~ conforms to
        + one of the following matches:
          * /Boolean match/
          * /Product match/

          OR

        + the type ~R~:
          #+begin_src scala
            type R = {
              def isEmpty: Boolean

              def get: S
            }
          #+end_src
          Here ~S~ conforms to one of the following matches:
          * single match
          * name-based match

      - *Precedence* of these forms:
        ~unapply~ form > /single match/ > /name-based match/

      - A usage of a /fixed-arity extractor/ is irrefutable if one of the following
        condition holds:
        + ~U~ = ~true~
        + the /extractor/ is used as a /product match/
        + ~U~ = ~Some[T]~ *(for Scala2 compatibility)*
        + ~U <: R~ and ~U <: { def isEmpty: false }~

***** Variadic Extractors
      Variadic extractors expose the following signature:
      #+begin_src scala
        def unapplySeq[A](x: T)(using x: B): U
      #+end_src
      - The type U conforms to one of the following matches:
        + /sequence match/
        + /product-sequence match/

      - Or ~U~ conforms to the /type/ ~R~:
        #+begin_src scala
          type R = {
            def isEmpty: Boolean
            def get: S
          }
        #+end_src
        Here ~S~ conforms to one of the _TWO_ /matches/ above.

      - *Precedence* of these forms:
        ~unapplySeq~ form > /sequence match/ > /product-sequence match/

      - A usage of a /variadic extractor/ is irrefutable if one of the following
        condition holds:
        + the /extractor/ is used directly as a
          * /sequence match/
            OR
          * /product-sequence match/
        + ~U~ = ~Some[T]~ *(for Scala2 compatibility)*
        + ~U <: R~ and ~U <: { def isEmpty: false }~

**** DONE Boolean Match
     CLOSED: [2020-09-24 Thu 23:44]
     - Criterion:
       + ~U =:= Boolean~
       + Pattern-matching on exactly *0* patterns

     - Example:
       #+begin_src scala
         object Even {
           def unapply(s: String): Boolean = s.size % 2 == 0
         }

         "even" match {
           case s @ Even() => println(s"$s has an even number of characters")
           case s          => println(s"$s has an odd number of characters")
         }
         // even has an even number of characters
       #+end_src

**** DONE Product Match
     CLOSED: [2020-09-24 Thu 23:44]
     - Criterion:
       + ~U <: Product~

       + N > 0 is the maximum number of consecutive (parameterless ~def~ or ~val~)
          ~_1: P1~ ... ~_N: PN~ members in ~U~

       + Pattern-matching on EXACTLY *N* patterns with types P1, P2, ..., PN

     - Example:
       #+begin_src scala
         class FirstChars(s: String) extends Product {
           def _1 = s.charAt(0)
           def _2 = s.charAt(1)

           // Not used by pattern matching: Product is only used as a marker trait.
           def canEqual(that: Any): Boolean = ???
           def productArity: Int = ???
           def productElement(n: Int): Any = ???
         }

         object FirstChars {
           def unapply(s: String): FirstChars = new FirstChars(s)
         }

         "Hi!" match {
           case FirstChars(char1, char2) =>
             println(s"First: $char1; Second: $char2")
         }
         // First: H; Second: i
       #+end_src

**** DONE Single Match
     CLOSED: [2020-09-24 Thu 23:46]
     - Criterion:
       If there is exactly *1* pattern, pattern-matching on 1 pattern with type ~U~

     - Example:
       #+begin_src scala
         class Nat(val x: Int) {
           def get: Int = x
           def isEmpty = x < 0
         }

         object Nat {
           def unapply(x: Int): Nat = new Nat(x)
         }

         5 match {
           case Nat(n) => println(s"$n is a natural number")
           case _      => ()
         }
         // 5 is a natural number
       #+end_src

**** DONE Name-based Match
     CLOSED: [2020-09-24 Thu 23:51]
     - Criterion:
       + N > 1 is the maximum number of consecutive (parameterless ~def~ or ~val~)
          ~_1: P1~ ... ~_N: PN~ members in ~U~

       + Pattern-matching on EXACTLY ~N~ patterns with types P1, P2, ..., PN

     - Example:
       #+begin_src scala
         object ProdEmpty {
           def _1: Int = ???

           def _2: String = ???

           def isEmpty = true
           def get = this

           def unapply(s: String): this.type = this
         }

         "" match {
           case ProdEmpty(_, _) => ???
           case _               => ()
         }
       #+end_src

**** DONE Sequence Match
     CLOSED: [2020-09-25 Fri 00:30]
     - Criterion:
       + ~U <: X~, ~T2~ and ~T3~ conform to ~T1~
         #+begin_src scala
           type X = {
             def lengthCompare(len: Int): Int // or, `def length: Int`
             def apply(i: Int): T1
             def drop(n: Int): scala.Seq[T2]
             def toSeq: scala.Seq[T3]
           }
         #+end_src

       + Pattern-matching on EXACTLY *N* simple patterns with types ~T1~, ~T1~, ...,
          ~T1~, where *N* is the _runtime size_ of the sequence, or

       + Pattern-matching on >= N simple patterns and a /vararg pattern/ (e.g.,
          ~xs: _*~) with types ~T1~, ~T1~, ..., ~T1~, ~Seq[T1]~, where *N* is the
          _minimum size_ of the sequence.

     - Example:
       #+begin_src scala
         object CharList {
           def unapplySeq(s: String): Option[Seq[Char]] = Some(s.toList)
         }

         "example" match {
           case CharList(c1, c2, c3, c4, _, _, _) =>
             println(s"$c1,$c2,$c3,$c4")

           case _ =>
             println("Expected *exactly* 7 characters!")
         }
         // e,x,a,m
       #+end_src

**** DONE Product-Sequence Match
     CLOSED: [2020-09-25 Fri 00:36]
     - Criterion:
       + ~U <: Product~

       + N > 0 is the _maximum number_ of consecutive (parameterless def or val)
          ~_1: P1~ ... ~_N: PN~ members in ~U~

       + ~PN~ conforms to the signature ~X~ defined in /Seq Pattern/

       + Pattern-matching on EXACTLY *>= N* patterns, the first *N - 1* patterns
          have types P1, P2, ... P(N-1), the type of the remaining patterns are
         determined as in /Seq Pattern/.

     - Example:
       #+begin_src scala
         class Foo(val name: String, val children: Int*)
         object Foo {
           def unapplySeq(f: Foo): Option[(String, Seq[Int])] = Some((f.name, f.children))
         }

         def foo(f: Foo) = f match {
           case Foo(name, ns : _*)       => ...
           case Foo(name, x, y, ns : _*) => ...
         }
       #+end_src

*** DONE Eta Expansion
    CLOSED: [2020-05-25 Mon 10:20]
    # Automatic Eta Expansion
    The _conversion of METHODS into FUNCTIONS_ has been improved and happens
    *automatically* for /methods/ with one or more parameters.
    - Example:
      #+begin_src scala
        def m(x: Boolean, y: String)(z: Int): List[Int]
        // automatically create a function value of type
        // `(Boolean, String) => Int => List[Int]`
        // without explicit type annotation.
        val f1 = m

        // automatically create a function value of type
        // `Int => List[Int]`
        // without explicit type annotation.
        val f2 = m(true, "abc")
      #+end_src

    - The syntax ~m \under{}~ *is _NO LONGER_ needed* and *will be _DEPRECATED_ in the future.*

**** Automatic eta-expansion and nullary methods
     /Automatic eta expansion/ does *NOT apply* to /"nullary" methods/ that take
     an empty parameter list.
     - Given a simple reference to ~next~ does *NOT auto-convert* to a function.
       One has to write explicitly ~() => next()~ to achieve that
       + Once again since the ~_~ _is going to be DEPRECATED_ it's better to write
         it this way rather than ~next _~.

     - The _REASON_ for *excluding* /nullary methods/ from /automatic eta expansion/:
       that Scala implicitly inserts the ~()~ argument, which would *conflict* with
       /eta expansion/.
       + _Automatic ~()~ insertion_ is _limited_ (see "Dropped: Auto-Application)
         in Dotty, but the fundamental ambiguity remains.

*** TODO Compiler Plugins
    # Changes in Compiler Plugins

**** Using Compiler Plugins
**** Writing a Standard Compiler Plugin
**** Writing a Research Compiler Plugin

*** TODO Lazy Vals initialization
**** Motivation
**** Implementation
**** Note on recursive lazy vals
**** Reference
     - SIP-20

*** DONE Main Functions
    CLOSED: [2020-05-24 Sun 22:31]
    Scala 3 offers a new way to define programs that can be invoked from the command line:
    _A ~@main~ /annotation/ on a /method/ turns this /method/ into an executable program._

    - Example:
      #+begin_src scala
        @main def happyBirthday(age: Int, name: String, others: String*): Unit = {
          val suffix =
            (age % 100) match {
              case 11 | 12 | 13 => "th"
              case _ =>
                (age % 10) match {
                  case 1 => "st"
                  case 2 => "nd"
                  case 3 => "rd"
                  case _ => "th"
                }
            }

          val bldr = new StringBuilder(s"Happy $age$suffix birthday, $name")
          for other <- others do bldr.append(" and ").append(other)
          bldr.toString
        }
      #+end_src
      This would generate a main program ~happyBirthday~ that could be called like this
      #+begin_src bash
        scala happyBirthday 23 Lisa Peter
        # Happy 23rd Birthday, Lisa and Peter!
      #+end_src

    - A ~@main~ annotated method can be written either
      + at the top-level
        OR
      + in a statically accessible object.

    - Parameters of the ~@main~ method:
      + the ~@main~ method can have an _ARBITRARY number_ of parameters.

      + For each parameter type there *must be* an instance of the
        ~scala.util.FromString~ /type class/ that is used to
        _convert_ an *argument string* _to_ the *required parameter type*.

      + The _parameter list_ of a ~@main~ method can end in a repeated parameter
        that then takes all remaining arguments given on the command line.

    - The program implemented from a @main method checks that there are enough
      arguments on the command line to fill in all parameters, and that argument
      strings are convertible to the required types. If a check fails, the
      program is terminated with an error message.
      + Examples:
        #+begin_src bash
          scala happyBirthday 22
          # Illegal command line after first argument: more arguments expected

          scala happyBirthday sixty Fred
          # Illegal command line: java.lang.NumberFormatException: For input string: "sixty"
        #+end_src

    - Code generation:
      The Scala compiler generates a program from a ~@main~ method ~f~ as follows:
      1. It creates a /class/
         * name ~f~
         * in the package where the ~@main~ method was found

      2. The generated /class/ has a /static method/ ~main~ with the *usual*
         /signature/: it takes an ~Array[String]~ as argument and returns ~Unit~.

      3. The generated ~main~ /method/
         * calls /method/ ~f~ with arguments converted using methods in the
           ~scala.util.CommandLineParser~ object.

    - Code generation example:
      #+begin_src scala
        final class happyBirthday {
          import scala.util.{CommandLineParser => CLP}
          <static> def main(args: Array[String]): Unit =
            try
              happyBirthday(
                CLP.parseArgument[Int](args, 0),
                CLP.parseArgument[String](args, 1),
                CLP.parseRemainingArguments[String](args, 2))
            catch {
              case error: CLP.ParseError => CLP.showError(error)
            }
        }
      #+end_src
      NOTE:
      The ~<static>~ modifier above expresses that the main method is
      generated as a /static method/ of /class/ ~happyBirthDay~.
        It is *NOT* available for user programs in Scala. Regular "static"
      members are generated in Scala using objects instead.

    - ~@main~ methods are the recommended scheme to generate programs that can be
      invoked from the command line in Scala 3.

    - They replace the previous scheme to write program as objects with a special
      ~App~ parent class. In Scala 2, ~happyBirthday~ could be written also like this:
      #+begin_src scala
        object happyBirthday extends App {
          // needs by-hand parsing of arguments vector
          // ...
        }
      #+end_src
      + The previous functionality of ~App~, which relied on the "magic"
        ~DelayedInit~ trait, is *no longer available*.

      + ~App~ still exists in limited form for now,
        but
        * it does *not support* /command line arguments/
        * it will be *deprecated* in the future.

      + If programs need to *cross-build* between Scala 2 and Scala 3,
        it is *RECOMMENDED* to use an _EXPLICIT_ ~main~ method with an
        ~Array[String]~ argument instead.

** TODO DROPPED FEATURES
*** DelayedInit
    # *Dropped: DelayedInit*

*** Macros
    # *Dropped: Scala 2 Macros*

*** Existential Types
    # *Dropped: Existential Types*

*** Type Projection
    # *Dropped: General Type Projection*

*** Do-While
    # *Dropped: Do-While*

*** Procedure Syntax
    # *Dropped: Procedure Syntax*

*** Package Objects
    # *Dropped: Package Objects*

*** Early Initializers
    # *Dropped: Early Initializers*

*** Class Shadowing
    # *Dropped: Class Shadowing*

*** TODO Limit 22
    # *Dropped: Class Shadowing*

*** XML Literals
    # *Dropped: XML Literals*

*** TODO Symbol Literals
    # *Dropped: Symbol Literals*

*** Auto-Application
    # *Dropped: Auto-Application*
**** Migrating code
**** Reference

*** Weak Conformance
    # *Dropped: Weak Conformance*

*** Nonlocal Returns
    # *Deprecated: Nonlocal Returns*

*** DONE ~[this]~ Quanlifier
    CLOSED: [2020-04-30 Thu 12:49]
    # *Dropped: ~private[this]~ and ~protected[this]~*
    The ~private[this]~ and ~protected[this]~ /access modifiers/ are *deprecated*
    and will be phased out.

    - Previously, these /modifier/ were needed
      + for *avoiding the generation* of /getters/ and /setters/ (~private[this]~
        ONLY).

      + for *excluding from variance checks* (~private[this]~ ONLY).
        * Scala 2 also excludes ~protected[this]~ but this was found to be unsound
          and was therefore removed.
          TODO More details and examples about the unsoundness!!! TODO

    - _REASON_ of Dropped:
      + The compiler now *can infers for ~private~ members the fact that they are ONLY
        accessed via ~this~.* Such members are treated as if they had been declared
        ~private[this]~ -- the previous requirements can be satisfied without manually
        write ~[this]~ out.

      + ~protected[this]~ is dropped without a replacement.
        =from Jian=
        The only requirement for ~protected[this]~ is to tell the compiler NOT
        do /variance checks/, which is already been pointed out that it is unsound!
        Therefore, the requirement is not real.

    - TODO
      =from Jian=
      Learn more about this. There are more discussion about this topic.

* CONTRIBUTING
** Contribute Knowledge
*** Contribute Internals-related Knowledge

** Getting Started
*** Requirements
*** Compiling and Running
*** Starting a REPL
*** Generating Documentation

** Workflow
*** Compiling files with dotc
*** Inspecting Trees with Type Stealer
*** Pretty-printing
*** SBT Commands Cheat Sheet

** Testing
*** Unit tests
**** Testing with checkfiles

*** Integration tests
**** Bootstrapped-only tests
**** From TASTy tests

** Debugging
*** Setting up the playground
*** Show for human readable output
*** How to disable color
*** Reporting as a non-intrusive println
*** Printing out trees after phases
*** Printing out stack traces of compile time errors
*** Configuring the printer output
*** Figuring out an object creation site
**** Via ID
**** Via tracer

*** Built-in Logging Architecture
**** Printers
**** Tracing
**** Reporter

** IDEs and Tools
*** Mill
*** Scalafix

** Procedures
*** Release Model
**** Model
**** Example
***** At the Dotty Repo
***** At the CI
****** Canceling CI builds

***** Documentation
****** Release Procedure Checklist
****** GitHub Releases and Blog Post

***** Ecosystem

**** Procedure in Bash Scripts

*** Modifying the Test framework
    *Test Vulpix Framework*

* INTERNALS
** Backend
*** Data Flow
*** Architecture
**** (a) The queue subsystem
**** (b) Bytecode-level types, ~BType~
**** (c) Utilities offering a more "high-level" API to bytecode emission
**** (d) Mapping between type-checker types and ~BType~'s
**** (e) More "high-level" utilities for bytecode emission
**** (f) Building an ASM ~ClassNode~ given an AST ~TypeDef~

** Classpaths
** Core Data Structrues
*** Symbols and SymDenotations
*** Why is this important?
*** Are We Done Yet?
*** What Are the Next Steps?

** Contexts
*** Contexts in the typer
*** In other phases
*** Using contexts

** Dotc vs Scalac
   # Differences between Dotc and Scalac
*** Denotation
**** Denotation vs. SymDenotation
**** Implicit Conversion

*** Symbol
*** Flags
*** Tree
*** Type

** Higher-Kinded Types
   *This page is out of date and preserved for posterity. Please see
   Implementing Higher-Kinded Types in Dotty for a more up to date version*

*** Higher-Kinded Types in Dotty V2
**** The duality
**** Named type parameters
**** Wildcards
**** Type parameters in the encodings
**** Partial applications
**** Modelling polymorphic type declarations
**** Modelling polymorphic type aliases: simple case
**** Modelling polymorphic type aliases: general case
**** Modelling higher-kinded types
**** Full example
**** Status of ~#~

** Overall Structure
   # Dotty Overall Structure
*** Package Structure
*** Contexts
*** Compiler Phases

** Periods
   # Dotc's concept of time*

** Syntax
   # Scala Syntax Summary
*** Lexical Syntax
*** Keywords
**** Regular keywords
**** Soft keywords

*** Context-free Syntax
**** Literals and Paths
**** Types
**** Expressions
**** Type and Value Parameters
**** Bindings and Imports
**** Declarations and Definitions

** Type System
*** Class diagram
*** Proxy types and ground types
*** Representations of types
**** Representation of methods

*** Subtyping checks
**** Type rebasing

*** Type caching
    # TODO

*** Type inference via constraint solving
    # TODO

** Dotty Internals 1: Trees & Symbols (Meeting Notes)
*** Entry point
*** Phases
*** Trees
**** Untyped trees
**** Typed trees
**** Notes on some tree types
***** ThisTree

**** Creating trees
**** Meaning of trees
**** Errors
**** Assignment

*** Symbols
**** ~ClassSymbol~
**** ~SymDenotation~

** Debug Macros
*** position not set
*** unresolved symbols in pickling

* RESOURCES
*** Talks
**** Talks on Dotty
**** Deep Dive with Dotty
     :PROPERTIES:
     :ID:       b5b2ba1a-6e8d-4f0c-a3c4-14f0e17ee56a
     :END:
* TODO API
** dotty
*** (O) ~DottyPredef~

*** dotty.internal
**** (O) ~CompileTimeMacros~
**** (O) ~StringContextMacro~

*** dotty.runtime
**** (O) ~Arrays~
**** (O) ~LazyVals~

** scala
*** (C) ~*:~
*** (C) ~Conversion~
*** (O) ~EmptyTuple~
*** (T) ~Enum~
*** (T) ~Eql~
*** (T) ~FunctionXXL~
*** (T) ~NonEmptyTuple~
*** (T) ~PolyFunction~
*** (T) ~Product0~
*** (T) ~Selectable~
*** (T) ~Tuple~
*** (T) ~TupledFunction~
*** (O) ~deriving~
*** (C) ~main~
*** (O) ~opaques~

*** scala.annotation
**** (T) ~RefiningAnnotation~
**** (C) ~alpha~
**** (C) ~constructorOnly~
**** (C) ~infix~
**** (C) ~static~
**** (C) ~superTrait~
**** (C) ~threadUnsafe~

*** scala.annotation.internal
**** (C) ~Alias~
**** (C) ~AnnotationDefault~
**** (C) ~Body~
**** (C) ~Child~
**** (C) ~ContextResultCount~
**** (C) ~InlineParam~
**** (C) ~Repeated~
**** (C) ~SourceFile~
**** (C) ~WithBounds~
**** (C) ~sharable~
**** (C) ~unshared~

*** scala.compiletime.ops
**** (O) ~any~
**** (O) ~boolean~
**** (O) ~int~
**** (O) ~string~

*** scala.compiletime.testing
**** (C) ~Error~
**** (T) ~ErrorKind~

*** scala.implicits
**** (T) ~LowPriorityNot~
**** (C) ~Not~

*** scala.internal
**** (O) ~Chars~
**** (C) ~MatchCase~
**** (O) ~TupledFunction~
**** (C) ~TypeBox~
**** scala.internal.quoted
***** (O) ~CompileTime~
***** (C) ~Expr~
***** (O) ~Matcher~
***** (C) ~Type~
***** (O) ~Unpickler~
***** (C) ~showName~

*** scala.quoted
**** (O) ~Const~
**** (O) ~Consts~
**** (C) ~Expr~
**** (O) ~Lambda~
**** (T) ~Liftable~
**** (T) ~QuoteContext~
**** (O) ~Reporting~
**** (C) ~ScopeException~
**** (C) ~Type~
**** (T) ~Unliftable~
**** (O) ~Unlifted~
**** (O) ~Varargs~
**** scala.quoted.show
***** (T) ~SyntaxHighlight~

**** scala.quoted.unsafe
***** (O) UnsafeExpr

**** scala.quoted.util
***** ~ExprMap~
***** ~Var~

*** scala.reflect
**** ~Selectable~

*** scala.runtime
**** ~EnumValue~
**** ~EnumValues~
**** ~Tuple~
**** ~TupleXXL~

*** scala.tasty
**** (C) ~Reflection~
**** scala.tasty.reflect
***** (T) ~CompilerInterface~
***** (C) ~ExprCastError~
***** (C) ~ExtractorsPrinter~
***** (T) ~Printer~
***** (C) ~SourceCodePrinter~
***** (T) ~TreeAccumulator~
***** (T) ~TreeMap~
***** (T) ~TreeTraverser~

*** scala.util
**** (O) ~CommandLineParser~
**** (T) ~FromDigits~
**** (T) ~FromString~
**** scala.util.control
***** (O) ~NonLocalReturns~

** scalaShadowing
*** (O) ~language~
