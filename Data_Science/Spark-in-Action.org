#+TITLE: Spark in Action
#+VERSION: 2016
#+AUTHOR: Petar Zečević, Marko Bonači
#+STARTUP: entitiespretty

* Table of Contents                                      :TOC_4_org:noexport:
- [[PART 1 First Steps - 1][PART 1 First Steps - 1]]
- [[1 Introduction to Apache Spark - 3][1 Introduction to Apache Spark - 3]]
  - [[1.1 What is Spark? - 4][1.1 What is Spark? - 4]]
    - [[The Spark revolution - 5][The Spark revolution - 5]]
    - [[MapReduce's shortcomings - 5][MapReduce's shortcomings - 5]]
    - [[What Spark brings to  the table - 6][What Spark brings to  the table - 6]]
  - [[1.2 Spark components - 8][1.2 Spark components - 8]]
    - [[Spark Core - 8][Spark Core - 8]]
    - [[Spark SQL - 9][Spark SQL - 9]]
    - [[Spark Streaming - 10][Spark Streaming - 10]]
    - [[Spark MLlib - 10][Spark MLlib - 10]]
    - [[Spark GraphX - 10][Spark GraphX - 10]]
  - [[1.3 Spark program flow - 10][1.3 Spark program flow - 10]]
  - [[1.4 Spark ecosystem - 13][1.4 Spark ecosystem - 13]]
  - [[1.5 Setting up the spark-in-action VM - 14][1.5 Setting up the spark-in-action VM - 14]]
    - [[Downloading and starting the virtual machine - 15][Downloading and starting the virtual machine - 15]]
    - [[virtual machine - 16][virtual machine - 16]]
  - [[1.6 Summary - 16][1.6 Summary - 16]]
- [[2 Spark fundamentals - 18][2 Spark fundamentals - 18]]
  - [[2.1 Using the spark-in-action VM - 19][2.1 Using the spark-in-action VM - 19]]
    - [[Cloning the Spark in Action GitHub repository - 20][Cloning the Spark in Action GitHub repository - 20]]
    - [[Finding Java - 20][Finding Java - 20]]
    - [[Using the VM's Hadoop installation - 21][Using the VM's Hadoop installation - 21]]
    - [[Examining the VM's Spark installation - 22][Examining the VM's Spark installation - 22]]
  - [[2.2 Using Spark shell and writing your first Spark program - 23][2.2 Using Spark shell and writing your first Spark program - 23]]
    - [[Starting the Spark shell - 23][Starting the Spark shell - 23]]
    - [[The first Spark code example - 25][The first Spark code example - 25]]
    - [[The notion of a resilient distributed dataset - 27][The notion of a resilient distributed dataset - 27]]
  - [[2.3 Basic RDD actions and transformations - 28][2.3 Basic RDD actions and transformations - 28]]
    - [[Using the map transformation - 28][Using the map transformation - 28]]
    - [[Using the distinct and flatMap transformations - 30][Using the distinct and flatMap transformations - 30]]
    - [[Obtaining RDD's elements with the sample, take, and takeSample operations - 34][Obtaining RDD's elements with the sample, take, and takeSample operations - 34]]
  - [[2.4 Double RDD functions - 36][2.4 Double RDD functions - 36]]
    - [[Basic statistics with double RDD functions - 37][Basic statistics with double RDD functions - 37]]
    - [[Visualizing data distribution with histograms - 38][Visualizing data distribution with histograms - 38]]
    - [[Approximate sum and mean - 38][Approximate sum and mean - 38]]
  - [[2.5 Summary - 39][2.5 Summary - 39]]
- [[3 Writing Spark applications - 40][3 Writing Spark applications - 40]]
  - [[3.1 Generating a new Spark project in Eclipse - 41][3.1 Generating a new Spark project in Eclipse - 41]]
  - [[3.2 Developing the application - 46][3.2 Developing the application - 46]]
    - [[Preparing the GitHub archive dataset - 46][Preparing the GitHub archive dataset - 46]]
    - [[Loading JSON - 48][Loading JSON - 48]]
    - [[Running the application from Eclipse - 50][Running the application from Eclipse - 50]]
    - [[Aggregating the data - 52][Aggregating the data - 52]]
    - [[Excluding non-employees - 53][Excluding non-employees - 53]]
    - [[Broadcast variables - 55][Broadcast variables - 55]]
    - [[Using the entire dataset - 57][Using the entire dataset - 57]]
  - [[3.3 Submitting the application - 58][3.3 Submitting the application - 58]]
    - [[Building the uberjar - 59][Building the uberjar - 59]]
    - [[Adapting the application - 60][Adapting the application - 60]]
    - [[Using spark-submit - 62][Using spark-submit - 62]]
  - [[3.4 Summary - 64][3.4 Summary - 64]]
- [[4 The Spark API in depth - 66][4 The Spark API in depth - 66]]
  - [[4.1 Working with pair RDDs - 67][4.1 Working with pair RDDs - 67]]
    - [[Creating pair RDDs - 67][Creating pair RDDs - 67]]
    - [[The Spark API in depth - 68][The Spark API in depth - 68]]
  - [[4.2 Understanding data partitioning and reducing data shuffling - 74][4.2 Understanding data partitioning and reducing data shuffling - 74]]
    - [[Using Spark's data partitioners - 74][Using Spark's data partitioners - 74]]
    - [[Understanding and avoiding unnecessary shuffling - 76][Understanding and avoiding unnecessary shuffling - 76]]
    - [[Repartitioning RDDs - 80][Repartitioning RDDs - 80]]
    - [[Mapping data in partitions - 81][Mapping data in partitions - 81]]
  - [[4.3 Joining, sorting, and grouping data - 82][4.3 Joining, sorting, and grouping data - 82]]
    - [[Joining data - 82][Joining data - 82]]
    - [[Sorting data - 88][Sorting data - 88]]
    - [[Grouping data - 91][Grouping data - 91]]
  - [[4.4 Understanding RDD dependencies - 94][4.4 Understanding RDD dependencies - 94]]
    - [[RDD dependencies and Spark execution - 94][RDD dependencies and Spark execution - 94]]
    - [[Spark stages and tasks - 96][Spark stages and tasks - 96]]
    - [[Saving the RDD lineage with checkpointing - 97][Saving the RDD lineage with checkpointing - 97]]
  - [[4.5 Using accumulators and broadcast variables to communicate with Spark executors - 97][4.5 Using accumulators and broadcast variables to communicate with Spark executors - 97]]
    - [[Obtaining data from executors with accumulators - 97][Obtaining data from executors with accumulators - 97]]
    - [[Sending data to executors using broadcast variables - 99][Sending data to executors using broadcast variables - 99]]
  - [[4.6 Summary - 101][4.6 Summary - 101]]
- [[PART 2 Meet the Spark Family - 103][PART 2 Meet the Spark Family - 103]]
- [[5 Sparkling queries with Spark SQL - 105][5 Sparkling queries with Spark SQL - 105]]
  - [[5.1 Working with DataFrames - 106][5.1 Working with DataFrames - 106]]
    - [[Creating DataFrames from RDDs - 108][Creating DataFrames from RDDs - 108]]
    - [[DataFrame API basics - 115][DataFrame API basics - 115]]
    - [[Using SQL functions to perform calculations on data - 118][Using SQL functions to perform calculations on data - 118]]
    - [[Working with missing values - 123][Working with missing values - 123]]
    - [[Converting DataFrames to RDDs - 124][Converting DataFrames to RDDs - 124]]
    - [[Grouping and joining data - 125][Grouping and joining data - 125]]
    - [[Performing joins - 128][Performing joins - 128]]
  - [[5.2 Beyond DataFrames: introducing DataSets - 129][5.2 Beyond DataFrames: introducing DataSets - 129]]
  - [[5.3 Using SQL commands - 130][5.3 Using SQL commands - 130]]
    - [[Table catalog and Hive metastore - 131][Table catalog and Hive metastore - 131]]
    - [[Executing SQL queries - 133][Executing SQL queries - 133]]
    - [[Connecting to Spark SQL through the Thrift server - 134][Connecting to Spark SQL through the Thrift server - 134]]
  - [[5.4 Saving and loading DataFrame data - 137][5.4 Saving and loading DataFrame data - 137]]
    - [[Built-in data sources - 138][Built-in data sources - 138]]
    - [[Saving data - 139][Saving data - 139]]
    - [[Loading data 141][Loading data 141]]
  - [[5.5 Catalyst optimizer - 142][5.5 Catalyst optimizer - 142]]
  - [[5.6 Performance improvements with Tungsten - 144][5.6 Performance improvements with Tungsten - 144]]
  - [[5.7 Beyond DataFrames: introducing DataSets - 145][5.7 Beyond DataFrames: introducing DataSets - 145]]
  - [[5.8 Summary - 145][5.8 Summary - 145]]
- [[6 Ingesting data with Spark Streaming - 147][6 Ingesting data with Spark Streaming - 147]]
  - [[6.1 Writing Spark Streaming applications - 148][6.1 Writing Spark Streaming applications - 148]]
    - [[Introducing the example application - 148][Introducing the example application - 148]]
    - [[Creating a streaming context - 150][Creating a streaming context - 150]]
    - [[Creating a discretized stream - 150][Creating a discretized stream - 150]]
    - [[Using discretized streams - 152][Using discretized streams - 152]]
    - [[Saving the results to a file - 153][Saving the results to a file - 153]]
    - [[Starting and stopping the streaming computation - 154][Starting and stopping the streaming computation - 154]]
    - [[Saving the computation state over time - 155][Saving the computation state over time - 155]]
    - [[Using window operations for time-limited calculations - 162][Using window operations for time-limited calculations - 162]]
    - [[Examining the other built-in input streams - 164][Examining the other built-in input streams - 164]]
  - [[6.2 Using external data sources - 165][6.2 Using external data sources - 165]]
    - [[Setting up Kafka - 166][Setting up Kafka - 166]]
    - [[Changing the streaming application to use Kafka - 167][Changing the streaming application to use Kafka - 167]]
  - [[6.3 Performance of Spark Streaming jobs - 172][6.3 Performance of Spark Streaming jobs - 172]]
    - [[Obtaining good performance - 173][Obtaining good performance - 173]]
    - [[Achieving fault-tolerance - 175][Achieving fault-tolerance - 175]]
  - [[6.4 Structured Streaming - 176][6.4 Structured Streaming - 176]]
    - [[Creating a streaming DataFrame - 176][Creating a streaming DataFrame - 176]]
    - [[Outputting streaming data - 177][Outputting streaming data - 177]]
    - [[Examining streaming executions - 178][Examining streaming executions - 178]]
    - [[Future direction of structured streaming - 178][Future direction of structured streaming - 178]]
  - [[6.5 Summary - 178][6.5 Summary - 178]]
- [[7 Getting smart with MLlib - 180][7 Getting smart with MLlib - 180]]
  - [[7.1 Introduction to machine learning - 181][7.1 Introduction to machine learning - 181]]
    - [[Definition of machine learning - 184][Definition of machine learning - 184]]
    - [[Classification of machine-learning algorithm - 184][Classification of machine-learning algorithm - 184]]
    - [[Machine learning with Spark - 187][Machine learning with Spark - 187]]
  - [[7.2 Linear algebra in Spark - 187][7.2 Linear algebra in Spark - 187]]
    - [[Local vector and matrix implementations - 188][Local vector and matrix implementations - 188]]
    - [[Distributed matrices - 192][Distributed matrices - 192]]
  - [[7.3 Linear regression - 193][7.3 Linear regression - 193]]
    - [[About linear regression - 193][About linear regression - 193]]
    - [[Simple linear regression - 194][Simple linear regression - 194]]
    - [[Expanding the model to multiple linear regression - 196][Expanding the model to multiple linear regression - 196]]
  - [[7.4 Analyzing and preparing the data - 198][7.4 Analyzing and preparing the data - 198]]
    - [[Analyzing data distribution - 199][Analyzing data distribution - 199]]
    - [[Analyzing column cosine similarities - 200][Analyzing column cosine similarities - 200]]
    - [[Computing the covariance matrix - 200][Computing the covariance matrix - 200]]
    - [[Transforming to labeled points - 201][Transforming to labeled points - 201]]
    - [[Splitting the data - 201][Splitting the data - 201]]
    - [[Feature scaling and mean normalization - 202][Feature scaling and mean normalization - 202]]
  - [[7.5 Fitting and using a linear regression model - 202][7.5 Fitting and using a linear regression model - 202]]
    - [[Predicting the target values - 203][Predicting the target values - 203]]
    - [[Evaluating the model's performance - 204][Evaluating the model's performance - 204]]
    - [[Interpreting the model parameters - 204][Interpreting the model parameters - 204]]
    - [[Loading and saving the model - 205][Loading and saving the model - 205]]
  - [[7.6 Tweaking the algorithm - 205][7.6 Tweaking the algorithm - 205]]
    - [[Finding the right step size and number of iterations - 206][Finding the right step size and number of iterations - 206]]
    - [[Adding higher-order polynomials - 207][Adding higher-order polynomials - 207]]
    - [[Bias-variance tradeoff and model complexity - 209][Bias-variance tradeoff and model complexity - 209]]
    - [[Plotting residual plots - 211][Plotting residual plots - 211]]
    - [[Avoiding overfitting by using regularization - 212][Avoiding overfitting by using regularization - 212]]
    - [[K-fold cross-validation - 213][K-fold cross-validation - 213]]
  - [[7.7 Optimizing linear regression - 214][7.7 Optimizing linear regression - 214]]
    - [[Mini-batch stochastic gradient descent - 214][Mini-batch stochastic gradient descent - 214]]
    - [[LBFGS optimizer - 216][LBFGS optimizer - 216]]
  - [[7.8 Summary - 217][7.8 Summary - 217]]
- [[8 ML: classification and clustering - 218][8 ML: classification and clustering - 218]]
  - [[8.1 Spark ML library - 219][8.1 Spark ML library - 219]]
    - [[Estimators, transformers, and evaluators - 220][Estimators, transformers, and evaluators - 220]]
    - [[ML parameters - 220][ML parameters - 220]]
    - [[ML pipelines - 221][ML pipelines - 221]]
  - [[8.2 Logistic regression - 221][8.2 Logistic regression - 221]]
    - [[Binary logistic regression model - 222][Binary logistic regression model - 222]]
    - [[Preparing data to use logistic regression in Spark - 224][Preparing data to use logistic regression in Spark - 224]]
    - [[Training the model - 229][Training the model - 229]]
    - [[Evaluating classification models - 231][Evaluating classification models - 231]]
    - [[Performing k-fold cross-validation - 234][Performing k-fold cross-validation - 234]]
    - [[Multiclass logistic regression - 236][Multiclass logistic regression - 236]]
  - [[8.3 Decision trees and random forests - 238][8.3 Decision trees and random forests - 238]]
    - [[Decision trees - 239][Decision trees - 239]]
    - [[Random forests - 244][Random forests - 244]]
  - [[8.4 Using k-means clustering - 246][8.4 Using k-means clustering - 246]]
    - [[K-means clustering - 247][K-means clustering - 247]]
  - [[8.5 Summary][8.5 Summary]]
- [[9 Connecting the dots with GraphX - 254][9 Connecting the dots with GraphX - 254]]
  - [[9.1 Graph processing with Spark - 255][9.1 Graph processing with Spark - 255]]
    - [[Constructing graphs using GraphX API - 256][Constructing graphs using GraphX API - 256]]
    - [[Transforming graphs - 257][Transforming graphs - 257]]
  - [[9.2 Graph algorithms- 256][9.2 Graph algorithms- 256]]
    - [[Presentation of the dataset - 263][Presentation of the dataset - 263]]
    - [[Shortest-paths algorithm - 264][Shortest-paths algorithm - 264]]
    - [[Page rank - 265][Page rank - 265]]
    - [[Connected components - 266][Connected components - 266]]
    - [[Strongly connected components - 268][Strongly connected components - 268]]
  - [[9.3 Implementing the A* search algorithm - 269][9.3 Implementing the A* search algorithm - 269]]
    - [[Understanding the A* algorithm - 270][Understanding the A* algorithm - 270]]
    - [[Implementing the A* algorithm - 272][Implementing the A* algorithm - 272]]
    - [[Testing the implementation - 279][Testing the implementation - 279]]
  - [[9.4 Summary - 280][9.4 Summary - 280]]
- [[PART 3 Spark OPS - 283][PART 3 Spark OPS - 283]]
- [[10 Running Spark - 285][10 Running Spark - 285]]
  - [[10.1 An overview of Spark's runtime architecture - 286][10.1 An overview of Spark's runtime architecture - 286]]
    - [[Spark runtime components - 286][Spark runtime components - 286]]
    - [[Spark cluster types - 288][Spark cluster types - 288]]
  - [[10.2 Job and resource scheduling - 289][10.2 Job and resource scheduling - 289]]
    - [[Cluster resource scheduling - 290][Cluster resource scheduling - 290]]
    - [[Spark job scheduling Data-locality considerations - 293][Spark job scheduling Data-locality considerations - 293]]
    - [[Spark memory scheduling - 294][Spark memory scheduling - 294]]
  - [[10.3 Configuring Spark - 295][10.3 Configuring Spark - 295]]
    - [[Spark configuration file - 295][Spark configuration file - 295]]
    - [[Command-line parameters - 295][Command-line parameters - 295]]
    - [[System environment variables - 296][System environment variables - 296]]
    - [[Setting configuration programmatically - 296][Setting configuration programmatically - 296]]
    - [[The master parameter - 297][The master parameter - 297]]
    - [[Viewing all configured parameters - 297][Viewing all configured parameters - 297]]
  - [[10.4 Spark web UI - 297][10.4 Spark web UI - 297]]
    - [[Jobs page - 298][Jobs page - 298]]
    - [[Stages page - 300][Stages page - 300]]
    - [[Storage page - 302][Storage page - 302]]
    - [[Environment page 302][Environment page 302]]
    - [[Executors page - 303][Executors page - 303]]
  - [[10.5 Running Spark on the local machine - 303][10.5 Running Spark on the local machine - 303]]
    - [[Local mode - 304][Local mode - 304]]
    - [[Local cluster mode - 305][Local cluster mode - 305]]
  - [[10.6 Summary - 305][10.6 Summary - 305]]
- [[11 Running on a Spark standalone cluster - 306][11 Running on a Spark standalone cluster - 306]]
  - [[11.1 Spark standalone cluster components - 307][11.1 Spark standalone cluster components - 307]]
  - [[11.2 Starting the standalone cluster - 308][11.2 Starting the standalone cluster - 308]]
    - [[Starting the cluster with shell scripts - 309][Starting the cluster with shell scripts - 309]]
    - [[Starting the cluster manually - 312][Starting the cluster manually - 312]]
    - [[Viewing Spark processes - 313][Viewing Spark processes - 313]]
    - [[Standalone master high availability and recovery - 313][Standalone master high availability and recovery - 313]]
  - [[11.3 Standalone cluster web UI - 315][11.3 Standalone cluster web UI - 315]]
  - [[11.4 Running applications in a standalone cluster - 317][11.4 Running applications in a standalone cluster - 317]]
    - [[Location of the driver - 317][Location of the driver - 317]]
    - [[Specifying the number of executors - 318][Specifying the number of executors - 318]]
    - [[Specifying extra classpath entries and files - 319][Specifying extra classpath entries and files - 319]]
    - [[Killing applications - 320][Killing applications - 320]]
    - [[Application automatic restart - 320][Application automatic restart - 320]]
  - [[11.5 Spark History Server and event logging - 321][11.5 Spark History Server and event logging - 321]]
  - [[11.6 Running on Amazon EC2 - 322][11.6 Running on Amazon EC2 - 322]]
    - [[Prerequisites - 323][Prerequisites - 323]]
    - [[Creating an EC2 standalone cluster - 324][Creating an EC2 standalone cluster - 324]]
    - [[Using the EC2 cluster - 327][Using the EC2 cluster - 327]]
    - [[Destroying the cluster - 329][Destroying the cluster - 329]]
- [[12 Running on YARN and Mesos - 331][12 Running on YARN and Mesos - 331]]
  - [[12.1 Running Spark on YARN - 332][12.1 Running Spark on YARN - 332]]
    - [[YARN architecture - 332][YARN architecture - 332]]
    - [[Installing, configuring, and starting YARN - 334][Installing, configuring, and starting YARN - 334]]
    - [[Resource scheduling in YARN - 335][Resource scheduling in YARN - 335]]
    - [[Submitting Spark applications to YARN - 336][Submitting Spark applications to YARN - 336]]
    - [[Configuring Spark on YARN - 338][Configuring Spark on YARN - 338]]
    - [[Configuring resources for Spark jobs - 339][Configuring resources for Spark jobs - 339]]
    - [[YARN UI - 341][YARN UI - 341]]
    - [[Finding logs on YARN - 343][Finding logs on YARN - 343]]
    - [[Security considerations - 344][Security considerations - 344]]
    - [[Dynamic resource allocation - 344][Dynamic resource allocation - 344]]
  - [[12.2 Running Spark on Mesos - 345][12.2 Running Spark on Mesos - 345]]
    - [[Mesos architecture - 346][Mesos architecture - 346]]
    - [[Installing and configuring Mesos - 349][Installing and configuring Mesos - 349]]
    - [[Mesos web UI - 351][Mesos web UI - 351]]
    - [[Mesos resource scheduling - 353][Mesos resource scheduling - 353]]
    - [[Submitting Spark applications to Mesos - 354][Submitting Spark applications to Mesos - 354]]
    - [[Running Spark with Docker - 356][Running Spark with Docker - 356]]
  - [[12.3 Summary - 359][12.3 Summary - 359]]
- [[PART 4 Bringing It Together - 361][PART 4 Bringing It Together - 361]]
- [[13 Case study: real-time dashboard - 363][13 Case study: real-time dashboard - 363]]
- [[14 Deep learning on Spark with H2O - 383][14 Deep learning on Spark with H2O - 383]]
  - [[14.1 What is deep learning? - 384][14.1 What is deep learning? - 384]]
  - [[14.2 Using H2O with Spark - 385][14.2 Using H2O with Spark - 385]]
    - [[What is H2O? - 386][What is H2O? - 386]]
    - [[Starting Sparkling Water on Spark - 386][Starting Sparkling Water on Spark - 386]]
    - [[Starting the H2O cluster - 389][Starting the H2O cluster - 389]]
    - [[Accessing the Flow UI - 390][Accessing the Flow UI - 390]]
  - [[14.3 Performing regression with H2O’s deep learning - 391][14.3 Performing regression with H2O’s deep learning - 391]]
    - [[Loading data into an H2O frame - 391][Loading data into an H2O frame - 391]]
    - [[Building and evaluating a deep-learning model using the Flow UI - 394][Building and evaluating a deep-learning model using the Flow UI - 394]]
    - [[Building and evaluating a deep-learning model using the Sparkling Water API - 398][Building and evaluating a deep-learning model using the Sparkling Water API - 398]]
  - [[14.4 Performing classification with H2O’s deep learning - 402][14.4 Performing classification with H2O’s deep learning - 402]]
    - [[Loading and splitting the data - 402][Loading and splitting the data - 402]]
    - [[Building the model through the Flow UI - 403][Building the model through the Flow UI - 403]]
    - [[Building the model with the Sparkling Water API - 406][Building the model with the Sparkling Water API - 406]]
    - [[Stopping the H2O cluster - 406][Stopping the H2O cluster - 406]]
  - [[14.5 Summary - 407][14.5 Summary - 407]]
- [[appendix A Installing Apache Spark - 409][appendix A Installing Apache Spark - 409]]
- [[appendix B Understanding MapReduce - 415][appendix B Understanding MapReduce - 415]]
- [[appendix C A primer on linear algebra - 418][appendix C A primer on linear algebra - 418]]
- [[index - 423][index - 423]]

* PART 1 First Steps - 1
* TODO 1 Introduction to Apache Spark - 3
** 1.1 What is Spark? - 4
*** The Spark revolution - 5
*** MapReduce's shortcomings - 5
*** What Spark brings to  the table - 6

** 1.2 Spark components - 8
*** Spark Core - 8
*** Spark SQL - 9
*** Spark Streaming - 10
*** Spark MLlib - 10
*** Spark GraphX - 10

** 1.3 Spark program flow - 10
** 1.4 Spark ecosystem - 13
** 1.5 Setting up the spark-in-action VM - 14
*** Downloading and starting the virtual machine - 15
*** virtual machine - 16

** 1.6 Summary - 16

* TODO 2 Spark fundamentals - 18
** 2.1 Using the spark-in-action VM - 19
*** Cloning the Spark in Action GitHub repository - 20
*** Finding Java - 20
*** Using the VM's Hadoop installation - 21
*** Examining the VM's Spark installation - 22

** 2.2 Using Spark shell and writing your first Spark program - 23
*** Starting the Spark shell - 23
*** The first Spark code example - 25
*** The notion of a resilient distributed dataset - 27

** 2.3 Basic RDD actions and transformations - 28
*** Using the map transformation - 28
*** Using the distinct and flatMap transformations - 30
*** Obtaining RDD's elements with the sample, take, and takeSample operations - 34

** 2.4 Double RDD functions - 36
*** Basic statistics with double RDD functions - 37
*** Visualizing data distribution with histograms - 38
*** Approximate sum and mean - 38

** 2.5 Summary - 39

* TODO 3 Writing Spark applications - 40
** 3.1 Generating a new Spark project in Eclipse - 41
** 3.2 Developing the application - 46
*** Preparing the GitHub archive dataset - 46
*** Loading JSON - 48
*** Running the application from Eclipse - 50
*** Aggregating the data - 52
*** Excluding non-employees - 53
*** Broadcast variables - 55
*** Using the entire dataset - 57

** 3.3 Submitting the application - 58
*** Building the uberjar - 59
*** Adapting the application - 60
*** Using spark-submit - 62

** 3.4 Summary - 64

* TODO 4 The Spark API in depth - 66
** 4.1 Working with pair RDDs - 67
*** Creating pair RDDs - 67
*** The Spark API in depth - 68

** 4.2 Understanding data partitioning and reducing data shuffling - 74
*** Using Spark's data partitioners - 74
*** Understanding and avoiding unnecessary shuffling - 76
*** Repartitioning RDDs - 80
*** Mapping data in partitions - 81

** 4.3 Joining, sorting, and grouping data - 82
*** Joining data - 82
*** Sorting data - 88
*** Grouping data - 91

** 4.4 Understanding RDD dependencies - 94
*** RDD dependencies and Spark execution - 94
*** Spark stages and tasks - 96
*** Saving the RDD lineage with checkpointing - 97

** 4.5 Using accumulators and broadcast variables to communicate with Spark executors - 97
*** Obtaining data from executors with accumulators - 97
*** Sending data to executors using broadcast variables - 99

** 4.6 Summary - 101

* PART 2 Meet the Spark Family - 103
* TODO 5 Sparkling queries with Spark SQL - 105
** 5.1 Working with DataFrames - 106
*** Creating DataFrames from RDDs - 108
*** DataFrame API basics - 115
*** Using SQL functions to perform calculations on data - 118
*** Working with missing values - 123
*** Converting DataFrames to RDDs - 124
*** Grouping and joining data - 125
*** Performing joins - 128

** 5.2 Beyond DataFrames: introducing DataSets - 129
** 5.3 Using SQL commands - 130
*** Table catalog and Hive metastore - 131
*** Executing SQL queries - 133
*** Connecting to Spark SQL through the Thrift server - 134

** 5.4 Saving and loading DataFrame data - 137
*** Built-in data sources - 138
*** Saving data - 139
*** Loading data 141

** 5.5 Catalyst optimizer - 142
** 5.6 Performance improvements with Tungsten - 144
** 5.7 Beyond DataFrames: introducing DataSets - 145
** 5.8 Summary - 145

* TODO 6 Ingesting data with Spark Streaming - 147
** 6.1 Writing Spark Streaming applications - 148
*** Introducing the example application - 148
*** Creating a streaming context - 150
*** Creating a discretized stream - 150
*** Using discretized streams - 152
*** Saving the results to a file - 153
*** Starting and stopping the streaming computation - 154
*** Saving the computation state over time - 155
*** Using window operations for time-limited calculations - 162
*** Examining the other built-in input streams - 164

** 6.2 Using external data sources - 165
*** Setting up Kafka - 166
*** Changing the streaming application to use Kafka - 167

** 6.3 Performance of Spark Streaming jobs - 172
*** Obtaining good performance - 173
*** Achieving fault-tolerance - 175

** 6.4 Structured Streaming - 176
*** Creating a streaming DataFrame - 176
*** Outputting streaming data - 177
*** Examining streaming executions - 178
*** Future direction of structured streaming - 178

** 6.5 Summary - 178

* TODO 7 Getting smart with MLlib - 180
** 7.1 Introduction to machine learning - 181
*** Definition of machine learning - 184
*** Classification of machine-learning algorithm - 184
*** Machine learning with Spark - 187

** 7.2 Linear algebra in Spark - 187
*** Local vector and matrix implementations - 188
*** Distributed matrices - 192

** 7.3 Linear regression - 193
*** About linear regression - 193
*** Simple linear regression - 194
*** Expanding the model to multiple linear regression - 196

** 7.4 Analyzing and preparing the data - 198
*** Analyzing data distribution - 199
*** Analyzing column cosine similarities - 200
*** Computing the covariance matrix - 200
*** Transforming to labeled points - 201
*** Splitting the data - 201
*** Feature scaling and mean normalization - 202

** 7.5 Fitting and using a linear regression model - 202
*** Predicting the target values - 203
*** Evaluating the model's performance - 204
*** Interpreting the model parameters - 204
*** Loading and saving the model - 205

** 7.6 Tweaking the algorithm - 205
*** Finding the right step size and number of iterations - 206
*** Adding higher-order polynomials - 207
*** Bias-variance tradeoff and model complexity - 209
*** Plotting residual plots - 211
*** Avoiding overfitting by using regularization - 212
*** K-fold cross-validation - 213

** 7.7 Optimizing linear regression - 214
*** Mini-batch stochastic gradient descent - 214
*** LBFGS optimizer - 216

** 7.8 Summary - 217

* TODO 8 ML: classification and clustering - 218
** TODO 8.1 Spark ML library - 219
*** Estimators, transformers, and evaluators - 220
*** ML parameters - 220
*** ML pipelines - 221

** TODO 8.2 Logistic regression - 221
*** Binary logistic regression model - 222
*** Preparing data to use logistic regression in Spark - 224
*** Training the model - 229
*** Evaluating classification models - 231
*** Performing k-fold cross-validation - 234
*** Multiclass logistic regression - 236

** TODO 8.3 Decision trees and random forests - 238
*** Decision trees - 239
*** Random forests - 244

** TODO 8.4 Using k-means clustering - 246
*** K-means clustering - 247

** TODO 8.5 Summary

* TODO 9 Connecting the dots with GraphX - 254
** 9.1 Graph processing with Spark - 255
*** Constructing graphs using GraphX API - 256
*** Transforming graphs - 257

** 9.2 Graph algorithms- 256
*** Presentation of the dataset - 263
*** Shortest-paths algorithm - 264
*** Page rank - 265
*** Connected components - 266
*** Strongly connected components - 268

** 9.3 Implementing the A* search algorithm - 269
*** Understanding the A* algorithm - 270
*** Implementing the A* algorithm - 272
*** Testing the implementation - 279

** 9.4 Summary - 280

* PART 3 Spark OPS - 283
* TODO 10 Running Spark - 285
** 10.1 An overview of Spark's runtime architecture - 286
*** Spark runtime components - 286
*** Spark cluster types - 288

** 10.2 Job and resource scheduling - 289
*** Cluster resource scheduling - 290
*** Spark job scheduling Data-locality considerations - 293
*** Spark memory scheduling - 294

** 10.3 Configuring Spark - 295
*** Spark configuration file - 295
*** Command-line parameters - 295
*** System environment variables - 296
*** Setting configuration programmatically - 296
*** The master parameter - 297
*** Viewing all configured parameters - 297

** 10.4 Spark web UI - 297
*** Jobs page - 298
*** Stages page - 300
*** Storage page - 302
*** Environment page 302
*** Executors page - 303

** 10.5 Running Spark on the local machine - 303
*** Local mode - 304
*** Local cluster mode - 305

** 10.6 Summary - 305

* TODO 11 Running on a Spark standalone cluster - 306
** 11.1 Spark standalone cluster components - 307
** 11.2 Starting the standalone cluster - 308
*** Starting the cluster with shell scripts - 309
*** Starting the cluster manually - 312
*** Viewing Spark processes - 313
*** Standalone master high availability and recovery - 313

** 11.3 Standalone cluster web UI - 315
** 11.4 Running applications in a standalone cluster - 317
*** Location of the driver - 317
*** Specifying the number of executors - 318
*** Specifying extra classpath entries and files - 319
*** Killing applications - 320
*** Application automatic restart - 320

** 11.5 Spark History Server and event logging - 321
** 11.6 Running on Amazon EC2 - 322
*** Prerequisites - 323
*** Creating an EC2 standalone cluster - 324
*** Using the EC2 cluster - 327
*** Destroying the cluster - 329

* TODO 12 Running on YARN and Mesos - 331
** 12.1 Running Spark on YARN - 332
*** YARN architecture - 332
*** Installing, configuring, and starting YARN - 334
*** Resource scheduling in YARN - 335
*** Submitting Spark applications to YARN - 336
*** Configuring Spark on YARN - 338
*** Configuring resources for Spark jobs - 339
*** YARN UI - 341
*** Finding logs on YARN - 343
*** Security considerations - 344
*** Dynamic resource allocation - 344

** 12.2 Running Spark on Mesos - 345
*** Mesos architecture - 346
*** Installing and configuring Mesos - 349
*** Mesos web UI - 351
*** Mesos resource scheduling - 353
*** Submitting Spark applications to Mesos - 354
*** Running Spark with Docker - 356

** 12.3 Summary - 359

* PART 4 Bringing It Together - 361
* TODO 13 Case study: real-time dashboard - 363
* TODO 14 Deep learning on Spark with H2O - 383
** 14.1 What is deep learning? - 384
** 14.2 Using H2O with Spark - 385
*** What is H2O? - 386
*** Starting Sparkling Water on Spark - 386
*** Starting the H2O cluster - 389
*** Accessing the Flow UI - 390

** 14.3 Performing regression with H2O’s deep learning - 391
*** Loading data into an H2O frame - 391
*** Building and evaluating a deep-learning model using the Flow UI - 394
*** Building and evaluating a deep-learning model using the Sparkling Water API - 398

** 14.4 Performing classification with H2O’s deep learning - 402
*** Loading and splitting the data - 402
*** Building the model through the Flow UI - 403
*** Building the model with the Sparkling Water API - 406
*** Stopping the H2O cluster - 406

** 14.5 Summary - 407

* appendix A Installing Apache Spark - 409
* appendix B Understanding MapReduce - 415
* appendix C A primer on linear algebra - 418
* index - 423
